{"ast":null,"code":"var _jsxFileName = \"/home/mukeshs/Projects/edurights/client/src/components/ml/deepMl/adaboost.js\";\nimport React, { Component } from 'react';\nimport Prism from \"prismjs\";\nimport { Grid, Paper, withStyles, List } from \"@material-ui/core\";\nimport '../../ReactJs/styles.css';\nimport Sidebar from '../sidebar';\nimport PrismCode from '../../ReactJs/prismCode';\nconst titles = {\n  backgroundColor: '#F0F8FF',\n  padding: '1px',\n  fontSize: '16px'\n};\n\nconst styles = theme => ({\n  paper: {\n    margin: theme.spacing(1),\n    padding: theme.spacing(1)\n  },\n  smMargin: {\n    margin: theme.spacing(1)\n  },\n  actionDiv: {\n    textAlign: \"center\"\n  }\n});\n\nconst stack = `\nimport numpy as np\n\n# Decision stump used as weak classifier\nclass DecisionStump:\n    def __init__(self):\n        self.polarity = 1\n        self.feature_idx = None\n        self.threshold = None\n        self.alpha = None\n\n    def predict(self, X):\n        n_samples = X.shape[0]\n        X_column = X[:, self.feature_idx]\n        predictions = np.ones(n_samples)\n        if self.polarity == 1:\n            predictions[X_column < self.threshold] = -1\n        else:\n            predictions[X_column > self.threshold] = -1\n\n        return predictions\n\n\nclass Adaboost:\n    def __init__(self, n_clf=5):\n        self.n_clf = n_clf\n        self.clfs = []\n\n    def fit(self, X, y):\n        n_samples, n_features = X.shape\n\n        w = np.full(n_samples, (1 / n_samples))                          # Initialize weights to 1/N\n\n        self.clfs = []\n        \n        for _ in range(self.n_clf):                                      # Iterate through classifiers\n            clf = DecisionStump()\n            min_error = float(\"inf\")\n\n            for feature_i in range(n_features):                          # greedy search to find best threshold and feature\n                X_column = X[:, feature_i]\n                thresholds = np.unique(X_column)\n\n                for threshold in thresholds:\n                    p = 1                                                # predict with polarity 1\n                    predictions = np.ones(n_samples)\n                    predictions[X_column < threshold] = -1\n\n                    misclassified = w[y != predictions]                  # Error = sum of weights of misclassified samples\n                    error = sum(misclassified)\n\n                    if error > 0.5:\n                        error = 1 - error\n                        p = -1\n\n                    if error < min_error:                               # store the best configuration\n                        clf.polarity = p\n                        clf.threshold = threshold\n                        clf.feature_idx = feature_i\n                        min_error = error\n\n            # calculate alpha\n            EPS = 1e-10\n            clf.alpha = 0.5 * np.log((1.0 - min_error + EPS) / (min_error + EPS))\n\n            predictions = clf.predict(X)                                         # calculate predictions and update weights\n\n            w *= np.exp(-clf.alpha * y * predictions)\n            # Normalize to one\n            w /= np.sum(w)\n\n            self.clfs.append(clf)                                               # Save classifier\n\n    def predict(self, X):\n        clf_preds = [clf.alpha * clf.predict(X) for clf in self.clfs]\n        y_pred = np.sum(clf_preds, axis=0)\n        y_pred = np.sign(y_pred)\n\n        return y_pred\n`.trim();\nconst testings = `\nif __name__ == \"__main__\":\n    from sklearn import datasets\n    from sklearn.model_selection import train_test_split\n\n    def accuracy(y_true, y_pred):\n        accuracy = np.sum(y_true == y_pred) / len(y_true)   \n        return accuracy\n\n    data = datasets.load_breast_cancer()\n    X, y = data.data, data.target\n\n    y[y == 0] = -1\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=5)\n\n    # Adaboost classification with 5 weak classifiers\n    clf = Adaboost(n_clf=5)\n    clf.fit(X_train, y_train)\n    y_pred = clf.predict(X_test)\n\n    acc = accuracy(y_test, y_pred)\n    print(\"Accuracy:\", acc)\n    `.trim();\nconst performance = `\nPerformance of stump = 1/2 * log(1-Total Error) / Total Error\n\nNew weights = old weight * e(+-Performanse)\n    where, + for Misclassification\n           - for Right classification\n`.trim(); // const stack = ``.trim();\n\nclass Adaboots extends Component {\n  componentDidMount() {\n    setTimeout(() => Prism.highlightAll(), 0);\n  }\n\n  render() {\n    const {\n      classes\n    } = this.props;\n    return React.createElement(Grid, {\n      container: true,\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 150\n      },\n      __self: this\n    }, React.createElement(Grid, {\n      item: true,\n      xs: 2,\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 151\n      },\n      __self: this\n    }, React.createElement(Paper, {\n      className: classes.paper,\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 152\n      },\n      __self: this\n    }, React.createElement(\"h4\", {\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 153\n      },\n      __self: this\n    }, React.createElement(Sidebar, {\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 153\n      },\n      __self: this\n    })))), React.createElement(Grid, {\n      item: true,\n      xs: 10,\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 156\n      },\n      __self: this\n    }, React.createElement(Paper, {\n      className: classes.paper,\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 157\n      },\n      __self: this\n    }, React.createElement(List, {\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 158\n      },\n      __self: this\n    }, React.createElement(\"h3\", {\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 159\n      },\n      __self: this\n    }, \"Boosting Types\"), React.createElement(\"ul\", {\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 160\n      },\n      __self: this\n    }, React.createElement(\"li\", {\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 161\n      },\n      __self: this\n    }, React.createElement(\"b\", {\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 161\n      },\n      __self: this\n    }, \"1. Adaboost: \")), React.createElement(\"li\", {\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 162\n      },\n      __self: this\n    }, React.createElement(\"b\", {\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 162\n      },\n      __self: this\n    }, \"2. Gradient Boosting: \"), \"Instead of Weights updation, here gradient (residuals, loss) is passed in next model.\"), React.createElement(\"br\", {\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 163\n      },\n      __self: this\n    }), React.createElement(\"li\", {\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 164\n      },\n      __self: this\n    }, React.createElement(\"b\", {\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 164\n      },\n      __self: this\n    }, \"3. Extream Gradient Boosting: \")), React.createElement(\"ul\", {\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 165\n      },\n      __self: this\n    }, React.createElement(\"li\", {\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 166\n      },\n      __self: this\n    }, \"Much similar to GB.\"), React.createElement(\"li\", {\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 167\n      },\n      __self: this\n    }, \"2nd order Derivatives of Loss function.\"), React.createElement(\"li\", {\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 168\n      },\n      __self: this\n    }, \"High Performance.\"), React.createElement(\"li\", {\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 169\n      },\n      __self: this\n    }, \"Fast training.\"), React.createElement(\"li\", {\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 170\n      },\n      __self: this\n    }, \"Advanced L1 and L2 Loass Regularization.\"), React.createElement(\"li\", {\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 171\n      },\n      __self: this\n    }, \"Parallel and Distributed computing (DMLC).\"), React.createElement(\"li\", {\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 172\n      },\n      __self: this\n    }, \"It handle missing values.\"), React.createElement(\"li\", {\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 173\n      },\n      __self: this\n    }, \"Cache Optimisation\"), React.createElement(\"li\", {\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 174\n      },\n      __self: this\n    }, \"It has many hyperparameters. reg_alpha, reg_lambda.\"))), React.createElement(\"br\", {\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 178\n      },\n      __self: this\n    }), React.createElement(\"h3\", {\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 180\n      },\n      __self: this\n    }, \"Adaboost (Adaptive boosting)\"), React.createElement(\"ul\", {\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 181\n      },\n      __self: this\n    }, React.createElement(\"li\", {\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 182\n      },\n      __self: this\n    }, \"Used for Classification and Regression.\"), React.createElement(\"li\", {\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 183\n      },\n      __self: this\n    }, \"Sequencial boosting.\"), React.createElement(\"li\", {\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 184\n      },\n      __self: this\n    }, \"AdaBoost is one of the first boosting algorithms to be adapted in solving practices. Adaboost helps you combine multiple \\u201Cweak classifiers\\u201D into a single \\u201Cstrong classifier\\u201D.\"), React.createElement(\"li\", {\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 186\n      },\n      __self: this\n    }, \"The weak learners in AdaBoost are decision trees with a single split, called \", React.createElement(\"b\", {\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 186\n      },\n      __self: this\n    }, \"decision stumps\"), \".\"), React.createElement(\"li\", {\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 187\n      },\n      __self: this\n    }, \"AdaBoost works by putting more weight on difficult to classify instances and less on those already handled well.\"), React.createElement(\"li\", {\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 188\n      },\n      __self: this\n    }, \"Weight increase for misclassification and weight decreses for right classifications.\"), React.createElement(\"li\", {\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 189\n      },\n      __self: this\n    }, \"Used to exploit dependency between models.\"), React.createElement(\"li\", {\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 190\n      },\n      __self: this\n    }, \"Stagewise additive MultiModeling using Multiclass Exponential Loss Function.\"), React.createElement(\"li\", {\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 191\n      },\n      __self: this\n    }, \"Can handle missing values and outliner.\"), React.createElement(\"li\", {\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 192\n      },\n      __self: this\n    }, \"Can handles mixed predictors as well (Quantitive and Qualitative).\")), React.createElement(\"br\", {\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 194\n      },\n      __self: this\n    }), React.createElement(\"br\", {\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 195\n      },\n      __self: this\n    }), React.createElement(\"b\", {\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 197\n      },\n      __self: this\n    }, \"Steps for Adaboost Algoritham:\"), React.createElement(\"ul\", {\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 198\n      },\n      __self: this\n    }, React.createElement(\"li\", {\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 199\n      },\n      __self: this\n    }, \"1. Initialize the weights as 1/n to every n observations.\"), React.createElement(\"li\", {\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 200\n      },\n      __self: this\n    }, \"2. Select the 1 Feature according to Lowest Gini/Highest information Gain and calculate the total error.\"), React.createElement(\"li\", {\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 201\n      },\n      __self: this\n    }, \"3. Calculate the Performance of the stump.\"), React.createElement(\"li\", {\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 202\n      },\n      __self: this\n    }, \"4. Calculate the new weights for each misclassification(increase) and right classification(decrease).\"), React.createElement(\"li\", {\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 203\n      },\n      __self: this\n    }, \"5. Normalize the new weights so that sum of weight is 1.\"), React.createElement(\"li\", {\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 204\n      },\n      __self: this\n    }, \"6. Repeat from step 2 to till configured number of estimators reacfied the accuracy achieved.\")), React.createElement(\"br\", {\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 206\n      },\n      __self: this\n    }), React.createElement(\"div\", {\n      style: titles,\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 208\n      },\n      __self: this\n    }, React.createElement(PrismCode, {\n      code: performance,\n      language: \"js\",\n      plugins: [\"line-numbers\"],\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 209\n      },\n      __self: this\n    })), React.createElement(\"br\", {\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 215\n      },\n      __self: this\n    }), React.createElement(\"div\", {\n      style: titles,\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 217\n      },\n      __self: this\n    }, React.createElement(PrismCode, {\n      code: stack,\n      language: \"js\",\n      plugins: [\"line-numbers\"],\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 218\n      },\n      __self: this\n    })), React.createElement(\"br\", {\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 224\n      },\n      __self: this\n    }), React.createElement(\"h3\", {\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 226\n      },\n      __self: this\n    }, \"Testing\"), React.createElement(\"div\", {\n      style: titles,\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 227\n      },\n      __self: this\n    }, React.createElement(PrismCode, {\n      code: testings,\n      language: \"js\",\n      plugins: [\"line-numbers\"],\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 228\n      },\n      __self: this\n    }))))));\n  }\n\n}\n\nexport default withStyles(styles)(Adaboots);","map":{"version":3,"sources":["/home/mukeshs/Projects/edurights/client/src/components/ml/deepMl/adaboost.js"],"names":["React","Component","Prism","Grid","Paper","withStyles","List","Sidebar","PrismCode","titles","backgroundColor","padding","fontSize","styles","theme","paper","margin","spacing","smMargin","actionDiv","textAlign","stack","trim","testings","performance","Adaboots","componentDidMount","setTimeout","highlightAll","render","classes","props"],"mappings":";AAAA,OAAOA,KAAP,IAAgBC,SAAhB,QAAiC,OAAjC;AACA,OAAOC,KAAP,MAAkB,SAAlB;AACA,SAASC,IAAT,EAAeC,KAAf,EAAsBC,UAAtB,EAAkCC,IAAlC,QAA8C,mBAA9C;AAEA,OAAO,0BAAP;AACA,OAAOC,OAAP,MAAoB,YAApB;AACA,OAAOC,SAAP,MAAsB,yBAAtB;AAGA,MAAMC,MAAM,GAAG;AAAEC,EAAAA,eAAe,EAAE,SAAnB;AAA8BC,EAAAA,OAAO,EAAE,KAAvC;AAA8CC,EAAAA,QAAQ,EAAE;AAAxD,CAAf;;AAEA,MAAMC,MAAM,GAAGC,KAAK,KAAK;AACvBC,EAAAA,KAAK,EAAE;AACLC,IAAAA,MAAM,EAAEF,KAAK,CAACG,OAAN,CAAc,CAAd,CADH;AAELN,IAAAA,OAAO,EAAEG,KAAK,CAACG,OAAN,CAAc,CAAd;AAFJ,GADgB;AAKvBC,EAAAA,QAAQ,EAAE;AACRF,IAAAA,MAAM,EAAEF,KAAK,CAACG,OAAN,CAAc,CAAd;AADA,GALa;AAQvBE,EAAAA,SAAS,EAAE;AACTC,IAAAA,SAAS,EAAE;AADF;AARY,CAAL,CAApB;;AAcA,MAAMC,KAAK,GAAI;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;CAAD,CA+EZC,IA/EY,EAAd;AAiFA,MAAMC,QAAQ,GAAI;;;;;;;;;;;;;;;;;;;;;;;KAAD,CAuBXD,IAvBW,EAAjB;AAyBA,MAAME,WAAW,GAAI;;;;;;CAAD,CAMlBF,IANkB,EAApB,C,CAQA;;AAGA,MAAMG,QAAN,SAAuBxB,SAAvB,CAAiC;AAC/ByB,EAAAA,iBAAiB,GAAG;AAClBC,IAAAA,UAAU,CAAC,MAAMzB,KAAK,CAAC0B,YAAN,EAAP,EAA6B,CAA7B,CAAV;AACD;;AACDC,EAAAA,MAAM,GAAG;AACP,UAAM;AAAEC,MAAAA;AAAF,QAAc,KAAKC,KAAzB;AACA,WACE,oBAAC,IAAD;AAAM,MAAA,SAAS,MAAf;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,OACE,oBAAC,IAAD;AAAM,MAAA,IAAI,MAAV;AAAW,MAAA,EAAE,EAAE,CAAf;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,OACE,oBAAC,KAAD;AAAO,MAAA,SAAS,EAAED,OAAO,CAACf,KAA1B;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,OACE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,OAAI,oBAAC,OAAD;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,MAAJ,CADF,CADF,CADF,EAME,oBAAC,IAAD;AAAM,MAAA,IAAI,MAAV;AAAW,MAAA,EAAE,EAAE,EAAf;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,OACE,oBAAC,KAAD;AAAO,MAAA,SAAS,EAAEe,OAAO,CAACf,KAA1B;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,OACE,oBAAC,IAAD;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,OACE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,wBADF,EAEE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,OACE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,OAAI;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,uBAAJ,CADF,EAEE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,OAAI;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,gCAAJ,0FAFF,EAGE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,MAHF,EAIE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,OAAI;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,wCAAJ,CAJF,EAKE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,OACE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,6BADF,EAEE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,iDAFF,EAGE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,2BAHF,EAIE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,wBAJF,EAKE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,kDALF,EAME;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,oDANF,EAOE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,mCAPF,EAQE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,4BARF,EASE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,6DATF,CALF,CAFF,EAoBE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,MApBF,EAsBE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,sCAtBF,EAuBE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,OACE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,iDADF,EAEE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,8BAFF,EAGE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,4MAHF,EAKE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,wFAAiF;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,yBAAjF,MALF,EAME;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,0HANF,EAOE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,8FAPF,EAQE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,oDARF,EASE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,sFATF,EAUE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,iDAVF,EAWE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,4EAXF,CAvBF,EAoCE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,MApCF,EAqCE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,MArCF,EAuCE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,wCAvCF,EAwCE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,OACE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,mEADF,EAEE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,kHAFF,EAGE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,oDAHF,EAIE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,+GAJF,EAKE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,kEALF,EAME;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,uGANF,CAxCF,EAgDE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,MAhDF,EAkDE;AAAK,MAAA,KAAK,EAAEN,MAAZ;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,OACE,oBAAC,SAAD;AACE,MAAA,IAAI,EAAEe,WADR;AAEE,MAAA,QAAQ,EAAC,IAFX;AAGE,MAAA,OAAO,EAAE,CAAC,cAAD,CAHX;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,MADF,CAlDF,EAyDE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,MAzDF,EA2DE;AAAK,MAAA,KAAK,EAAEf,MAAZ;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,OACE,oBAAC,SAAD;AACE,MAAA,IAAI,EAAEY,KADR;AAEE,MAAA,QAAQ,EAAC,IAFX;AAGE,MAAA,OAAO,EAAE,CAAC,cAAD,CAHX;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,MADF,CA3DF,EAkEE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,MAlEF,EAoEE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,iBApEF,EAqEE;AAAK,MAAA,KAAK,EAAEZ,MAAZ;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,OACE,oBAAC,SAAD;AACE,MAAA,IAAI,EAAEc,QADR;AAEE,MAAA,QAAQ,EAAC,IAFX;AAGE,MAAA,OAAO,EAAE,CAAC,cAAD,CAHX;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,MADF,CArEF,CADF,CADF,CANF,CADF;AA8GD;;AApH8B;;AAuHjC,eAAgBlB,UAAU,CAACQ,MAAD,CAAV,CAAmBY,QAAnB,CAAhB","sourcesContent":["import React, { Component } from 'react';\nimport Prism from \"prismjs\"\nimport { Grid, Paper, withStyles, List } from \"@material-ui/core\";\n\nimport '../../ReactJs/styles.css'\nimport Sidebar from '../sidebar';\nimport PrismCode from '../../ReactJs/prismCode';\n\n\nconst titles = { backgroundColor: '#F0F8FF', padding: '1px', fontSize: '16px' }\n\nconst styles = theme => ({\n  paper: {\n    margin: theme.spacing(1),\n    padding: theme.spacing(1)\n  },\n  smMargin: {\n    margin: theme.spacing(1)\n  },\n  actionDiv: {\n    textAlign: \"center\"\n  }\n})\n\n\nconst stack = `\nimport numpy as np\n\n# Decision stump used as weak classifier\nclass DecisionStump:\n    def __init__(self):\n        self.polarity = 1\n        self.feature_idx = None\n        self.threshold = None\n        self.alpha = None\n\n    def predict(self, X):\n        n_samples = X.shape[0]\n        X_column = X[:, self.feature_idx]\n        predictions = np.ones(n_samples)\n        if self.polarity == 1:\n            predictions[X_column < self.threshold] = -1\n        else:\n            predictions[X_column > self.threshold] = -1\n\n        return predictions\n\n\nclass Adaboost:\n    def __init__(self, n_clf=5):\n        self.n_clf = n_clf\n        self.clfs = []\n\n    def fit(self, X, y):\n        n_samples, n_features = X.shape\n\n        w = np.full(n_samples, (1 / n_samples))                          # Initialize weights to 1/N\n\n        self.clfs = []\n        \n        for _ in range(self.n_clf):                                      # Iterate through classifiers\n            clf = DecisionStump()\n            min_error = float(\"inf\")\n\n            for feature_i in range(n_features):                          # greedy search to find best threshold and feature\n                X_column = X[:, feature_i]\n                thresholds = np.unique(X_column)\n\n                for threshold in thresholds:\n                    p = 1                                                # predict with polarity 1\n                    predictions = np.ones(n_samples)\n                    predictions[X_column < threshold] = -1\n\n                    misclassified = w[y != predictions]                  # Error = sum of weights of misclassified samples\n                    error = sum(misclassified)\n\n                    if error > 0.5:\n                        error = 1 - error\n                        p = -1\n\n                    if error < min_error:                               # store the best configuration\n                        clf.polarity = p\n                        clf.threshold = threshold\n                        clf.feature_idx = feature_i\n                        min_error = error\n\n            # calculate alpha\n            EPS = 1e-10\n            clf.alpha = 0.5 * np.log((1.0 - min_error + EPS) / (min_error + EPS))\n\n            predictions = clf.predict(X)                                         # calculate predictions and update weights\n\n            w *= np.exp(-clf.alpha * y * predictions)\n            # Normalize to one\n            w /= np.sum(w)\n\n            self.clfs.append(clf)                                               # Save classifier\n\n    def predict(self, X):\n        clf_preds = [clf.alpha * clf.predict(X) for clf in self.clfs]\n        y_pred = np.sum(clf_preds, axis=0)\n        y_pred = np.sign(y_pred)\n\n        return y_pred\n`.trim();\n\nconst testings = `\nif __name__ == \"__main__\":\n    from sklearn import datasets\n    from sklearn.model_selection import train_test_split\n\n    def accuracy(y_true, y_pred):\n        accuracy = np.sum(y_true == y_pred) / len(y_true)   \n        return accuracy\n\n    data = datasets.load_breast_cancer()\n    X, y = data.data, data.target\n\n    y[y == 0] = -1\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=5)\n\n    # Adaboost classification with 5 weak classifiers\n    clf = Adaboost(n_clf=5)\n    clf.fit(X_train, y_train)\n    y_pred = clf.predict(X_test)\n\n    acc = accuracy(y_test, y_pred)\n    print(\"Accuracy:\", acc)\n    `.trim();\n\nconst performance = `\nPerformance of stump = 1/2 * log(1-Total Error) / Total Error\n\nNew weights = old weight * e(+-Performanse)\n    where, + for Misclassification\n           - for Right classification\n`.trim();\n\n// const stack = ``.trim();\n\n\nclass Adaboots extends Component {\n  componentDidMount() {\n    setTimeout(() => Prism.highlightAll(), 0)\n  }\n  render() {\n    const { classes } = this.props;\n    return (\n      <Grid container>\n        <Grid item xs={2}>\n          <Paper className={classes.paper}>\n            <h4><Sidebar /></h4>\n          </Paper>\n        </Grid>\n        <Grid item xs={10}>\n          <Paper className={classes.paper}>\n            <List>\n              <h3>Boosting Types</h3>\n              <ul>\n                <li><b>1. Adaboost: </b></li>\n                <li><b>2. Gradient Boosting: </b>Instead of Weights updation, here gradient (residuals, loss) is passed in next model.</li>\n                <br />\n                <li><b>3. Extream Gradient Boosting: </b></li>\n                <ul>\n                  <li>Much similar to GB.</li>\n                  <li>2nd order Derivatives of Loss function.</li>\n                  <li>High Performance.</li>\n                  <li>Fast training.</li>\n                  <li>Advanced L1 and L2 Loass Regularization.</li>\n                  <li>Parallel and Distributed computing (DMLC).</li>\n                  <li>It handle missing values.</li>\n                  <li>Cache Optimisation</li>\n                  <li>It has many hyperparameters. reg_alpha, reg_lambda.</li>\n                </ul>\n              </ul>\n\n              <br />\n\n              <h3>Adaboost (Adaptive boosting)</h3>\n              <ul>\n                <li>Used for Classification and Regression.</li>\n                <li>Sequencial boosting.</li>\n                <li>AdaBoost is one of the first boosting algorithms to be adapted in solving practices. Adaboost helps you combine\n                  multiple “weak classifiers” into a single “strong classifier”.</li>\n                <li>The weak learners in AdaBoost are decision trees with a single split, called <b>decision stumps</b>.</li>\n                <li>AdaBoost works by putting more weight on difficult to classify instances and less on those already handled well.</li>\n                <li>Weight increase for misclassification and weight decreses for right classifications.</li>\n                <li>Used to exploit dependency between models.</li>\n                <li>Stagewise additive MultiModeling using Multiclass Exponential Loss Function.</li>\n                <li>Can handle missing values and outliner.</li>\n                <li>Can handles mixed predictors as well (Quantitive and Qualitative).</li>\n              </ul>\n              <br />\n              <br />\n\n              <b>Steps for Adaboost Algoritham:</b>\n              <ul>\n                <li>1. Initialize the weights as 1/n to every n observations.</li>\n                <li>2. Select the 1 Feature according to Lowest Gini/Highest information Gain and calculate the total error.</li>\n                <li>3. Calculate the Performance of the stump.</li>\n                <li>4. Calculate the new weights for each misclassification(increase) and right classification(decrease).</li>\n                <li>5. Normalize the new weights so that sum of weight is 1.</li>\n                <li>6. Repeat from step 2 to till configured number of estimators reacfied the accuracy achieved.</li>\n              </ul>\n              <br />\n\n              <div style={titles}>\n                <PrismCode\n                  code={performance}\n                  language=\"js\"\n                  plugins={[\"line-numbers\"]}\n                />\n              </div>\n              <br />\n\n              <div style={titles}>\n                <PrismCode\n                  code={stack}\n                  language=\"js\"\n                  plugins={[\"line-numbers\"]}\n                />\n              </div>\n              <br />\n\n              <h3>Testing</h3>\n              <div style={titles}>\n                <PrismCode\n                  code={testings}\n                  language=\"js\"\n                  plugins={[\"line-numbers\"]}\n                />\n              </div>\n              {/* <br />\n\n              <h3></h3>\n              <div style={titles}>\n                <PrismCode\n                  code={stack}\n                  language=\"js\"\n                  plugins={[\"line-numbers\"]}\n                />\n              </div>\n              <br />\n\n              <h3></h3>\n              <div style={titles}>\n                <PrismCode\n                  code={stack}\n                  language=\"js\"\n                  plugins={[\"line-numbers\"]}\n                />\n              </div> */}\n            </List>\n          </Paper>\n        </Grid>\n      </Grid>\n    )\n  }\n}\n\nexport default (withStyles(styles)(Adaboots));\n"]},"metadata":{},"sourceType":"module"}