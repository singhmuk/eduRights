{"ast":null,"code":"import _classCallCheck from\"/home/mukeshs/Projects/edurights/node_modules/@babel/runtime/helpers/esm/classCallCheck\";import _createClass from\"/home/mukeshs/Projects/edurights/node_modules/@babel/runtime/helpers/esm/createClass\";import _possibleConstructorReturn from\"/home/mukeshs/Projects/edurights/node_modules/@babel/runtime/helpers/esm/possibleConstructorReturn\";import _getPrototypeOf from\"/home/mukeshs/Projects/edurights/node_modules/@babel/runtime/helpers/esm/getPrototypeOf\";import _inherits from\"/home/mukeshs/Projects/edurights/node_modules/@babel/runtime/helpers/esm/inherits\";import React,{Component}from'react';import Prism from\"prismjs\";import{Grid,Paper,withStyles,List}from\"@material-ui/core\";import'../../ReactJs/styles.css';import Sidebar from'../sidebar';import PrismCode from'../../ReactJs/prismCode';import Equations from'../../../assets/ML/linear_regression.png';import Thersolds from'../../../assets/ML/thersold.png';import Outliner from'../../../assets/ML/regression_with_outlier.png';import Sigmoid from'../../../assets/ML/sigmoid_function.png';import LinearVsLogistic from'../../../assets/ML/LinearVsLogistic.jpg';import Logistic from'../../../assets/ML/logistic.jpg';var titles={backgroundColor:'#F0F8FF',padding:'1px',fontSize:'16px'};var redesign={height:350,width:600};var styles=function styles(theme){return{paper:{margin:theme.spacing(1),padding:theme.spacing(1)},smMargin:{margin:theme.spacing(1)},actionDiv:{textAlign:\"center\"}};};var cluster=\"\\nfrom matplotlib import pyplot as plt\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.linear_model import LogisticRegression\\n%matplotlib inline\\n\\ndf = pd.read_csv(\\\"insurance_data.csv\\\")\\n\\nplt.scatter(df.age,df.bought_insurance,marker='+',color='red')\\nX_train, X_test, y_train, y_test = train_test_split(df[['age']],df.bought_insurance,train_size=0.8)\\nmodel = LogisticRegression()\\nmodel.fit(X_train, y_train)\\n\\ny_predicted = model.predict(X_test)\\nmodel.predict_proba(X_test)\\nmodel.score(X_test,y_test)\\n\\ny_predicted\\nmodel.coef_                                                       #model.coef_ indicates value of m in y=m*x + b.\\nmodel.intercept_                                                  #model.intercept_ indicates value of b in y=m*x + b.\\n\\n\\n#Lets defined sigmoid function and do the math.\\nimport math\\ndef sigmoid(x):\\n  return 1 / (1 + math.exp(-x))\\n  \\ndef prediction_function(age):\\n  z = 0.042 * age - 1.53 # 0.04150133 ~ 0.042 and -1.52726963 ~ -1.53\\n  y = sigmoid(z)\\n  return y\\n  \\nage = 35\\nprediction_function(age)\\n\\nage = 43\\nprediction_function(age)\\n\".trim();var multiClass=\"\\nfrom matplotlib import pyplot as plt\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.linear_model import LogisticRegression\\n%matplotlib inline\\n\\ndf = pd.read_csv(\\\"insurance_data.csv\\\")\\n\\nplt.scatter(df.age,df.bought_insurance,marker='+',color='red')\\nX_train, X_test, y_train, y_test = train_test_split(df[['age']],df.bought_insurance,train_size=0.8)\\nmodel = LogisticRegression()\\nmodel.fit(X_train, y_train)\\n\\ny_predicted = model.predict(X_test)\\nmodel.predict_proba(X_test)\\nmodel.score(X_test,y_test)\\n\\ny_predicted\\n\".trim();var sigmoid=\"\\nimport math\\ndef sigmoid(x):\\n  return 1 / (1 + math.exp(-x))\\n  \\ndef prediction_function(age):\\n  z = 0.042 * age - 1.53 # 0.04150133 ~ 0.042 and -1.52726963 ~ -1.53\\n  y = sigmoid(z)\\n  return y\\n  \\nage = 35\\nprediction_function(age)\\n\\nage = 43\\nprediction_function(age)\\n  \".trim();var stack=\"\\n  import numpy as np\\n  \\n  \\n  class LogisticRegression:\\n      def __init__(self, learning_rate=0.001, n_iters=1000):\\n          self.lr = learning_rate\\n          self.n_iters = n_iters\\n          self.weights = None\\n          self.bias = None\\n  \\n      def fit(self, X, y):\\n          n_samples, n_features = X.shape\\n  \\n          self.weights = np.zeros(n_features)\\n          self.bias = 0\\n  \\n          for _ in range(self.n_iters):\\n              # approximate y with linear combination of weights and x, plus bias\\n              linear_model = np.dot(X, self.weights) + self.bias\\n              y_predicted = self._sigmoid(linear_model)                                 # apply sigmoid function\\n  \\n              dw = (1 / n_samples) * np.dot(X.T, (y_predicted - y))\\n              db = (1 / n_samples) * np.sum(y_predicted - y)\\n              \\n              self.weights -= self.lr * dw\\n              self.bias -= self.lr * db\\n  \\n      def predict(self, X):\\n          linear_model = np.dot(X, self.weights) + self.bias\\n          y_predicted = self._sigmoid(linear_model)\\n          y_predicted_cls = [1 if i > 0.5 else 0 for i in y_predicted]\\n          return np.array(y_predicted_cls)\\n  \\n      def _sigmoid(self, x):\\n          return 1 / (1 + np.exp(-x))\\n  \".trim();var testings=\"\\nif __name__ == \\\"__main__\\\":\\n      # Imports\\n      from sklearn.model_selection import train_test_split\\n      from sklearn import datasets\\n  \\n      def accuracy(y_true, y_pred):\\n          accuracy = np.sum(y_true == y_pred) / len(y_true)\\n          return accuracy\\n  \\n      bc = datasets.load_breast_cancer()\\n      X, y = bc.data, bc.target\\n  \\n      X_train, X_test, y_train, y_test = train_test_split(\\n          X, y, test_size=0.2, random_state=1234\\n      )\\n  \\n      regressor = LogisticRegression(learning_rate=0.0001, n_iters=1000)\\n      regressor.fit(X_train, y_train)\\n      predictions = regressor.predict(X_test)\\n  \\n      print(\\\"LR classification accuracy:\\\", accuracy(y_test, predictions))\\n\".trim();var poly=\"\\nimport matplotlib.pyplot as plt\\n\\nx = [1,2,3,5,6,7,8,9,10,12,13,14,15,16,18,19,21,22]\\ny = [100,90,80,60,60,55,60,65,70,70,75,76,78,79,90,99,99,100]\\n\\nplt.scatter(x, y)\\nplt.show()\\n\".trim();var poly_2=\"\\nx = [1,2,3,5,6,7,8,9,10,12,13,14,15,16,18,19,21,22]\\ny = [100,90,80,60,60,55,60,65,70,70,75,76,78,79,90,99,99,100]\\n\\nmymodel = np.poly1d(np.polyfit(x, y, 3))\\n\\nmyline = np.linspace(1, 22, 100)\\n\\nplt.scatter(x, y)\\nplt.plot(myline, mymodel(myline))\\nplt.show()\\n\".trim();var LogisticRegs=/*#__PURE__*/function(_Component){_inherits(LogisticRegs,_Component);function LogisticRegs(){_classCallCheck(this,LogisticRegs);return _possibleConstructorReturn(this,_getPrototypeOf(LogisticRegs).apply(this,arguments));}_createClass(LogisticRegs,[{key:\"componentDidMount\",value:function componentDidMount(){setTimeout(function(){return Prism.highlightAll();},0);}},{key:\"render\",value:function render(){var classes=this.props.classes;return React.createElement(Grid,{container:true},React.createElement(Grid,{item:true,xs:2},React.createElement(Paper,{className:classes.paper},React.createElement(\"h4\",null,React.createElement(Sidebar,null)))),React.createElement(Grid,{item:true,xs:10},React.createElement(Paper,{className:classes.paper},React.createElement(List,null,React.createElement(\"h3\",null,\"Logistic Regression (Supervised ML)\"),\"Used to model the probability of a certain class/ event. It is used when the data is linearly separable and the outcome is binary/ dichotomous in nature.\",React.createElement(\"br\",null),React.createElement(\"br\",null),React.createElement(\"i\",null,React.createElement(\"b\",null,\"Ex. of Binary classification\"),\" Yes/No, Pass/Fail, Win/Lose, Cancerous/Non-cancerous, etc.\"),React.createElement(\"br\",null),React.createElement(\"br\",null),React.createElement(\"b\",null,\"Types of Logistic Regression\"),React.createElement(\"ul\",null,React.createElement(\"li\",null,React.createElement(\"b\",null,\"Simple Logistic Regression: \"),\"A single independent is used to predict the output.\"),React.createElement(\"li\",null,React.createElement(\"b\",null,\"Multiple logistic regression: \"),\"Multiple independent variables are used to predict the output.\")),React.createElement(\"br\",null),React.createElement(\"b\",null,\"Extensions of Logistic Regression\"),React.createElement(\"br\",null),\"Although it is said Logistic regression is used for Binary Classification, it can be extended to solve multiclass classification problems.\",React.createElement(\"br\",null),React.createElement(\"br\",null),React.createElement(\"ul\",null,React.createElement(\"li\",null,React.createElement(\"b\",null,\"Multinomial Logistic Regression: \"),\"The o/p variable is discrete in three/ more classes with no natural ordering.\"),React.createElement(\"ul\",null,React.createElement(\"li\",null,React.createElement(\"b\",null,\"Food texture: \"),\"Crunchy, Mushy, Crispy.\"),React.createElement(\"li\",null,React.createElement(\"b\",null,\"Hair colour: \"),\"Blonde, Brown, Brunette, Red\\u200B.\")),React.createElement(\"br\",null),React.createElement(\"li\",null,React.createElement(\"b\",null,\"Ordered Logistic Regression: \"),\"The o/p variable is discrete in three/ more classes with the ordering of the levels.\"),React.createElement(\"ul\",null,React.createElement(\"li\",null,React.createElement(\"b\",null,\"Customer Rating: \"),\"extremely dislike, dislike, neutral, like, extremely like.\"),React.createElement(\"li\",null,React.createElement(\"b\",null,\"Income level: \"),\"low income, middle income, high income.\"))),React.createElement(\"br\",null),\"Now, let us try if we can use linear regression to solve a binary class classification problem. Assume we have a dataset that is linearly separable and has the o/p that is discrete in two classes (0, 1).\",React.createElement(\"br\",null),React.createElement(\"img\",{src:Equations,alt:\"Equations\",className:\"responsive\",style:redesign}),React.createElement(\"br\",null),\"we draw a straight line L1 such that the sum of distances of all the data points to the line is minimal.\",React.createElement(\"br\",null),React.createElement(\"br\",null),\"We define a threshold T = 0.5, above which the o/p belongs to class 1 otherwise class 0.\",React.createElement(\"br\",null),React.createElement(\"img\",{src:Thersolds,alt:\"Equations\",className:\"responsive\",style:redesign}),React.createElement(\"ul\",null,React.createElement(\"li\",null,React.createElement(\"b\",null,\"Case 1: \"),\"The predicted value for x1 is \\u22480.2 which is less than the threshold, so x1 belongs to class 0.\"),React.createElement(\"li\",null,React.createElement(\"b\",null,\"Case 2: \"),\"Predicted value for the point x2 is \\u22480.6 which is greater than the threshold, so x2 belongs to class 1.\"),React.createElement(\"li\",null,React.createElement(\"b\",null,\"Case 3: \"),\"Predicted value for the point x3 is beyond 1.\"),React.createElement(\"li\",null,React.createElement(\"b\",null,\"Case 4: \"),\"Predicted value for the point x4 is below 0.\")),\"The predicted values for the points x3, x4 exceed the range (0,1) which doesn\\u2019t make sense because the probability values always lie between 0 and 1. And our output can have only two values either 0 or 1. Hence, this is a problem with the linear regression model.\",React.createElement(\"br\",null),React.createElement(\"br\",null),\"Now, introduce an outlier and see what happens. The regression line gets deviated to keep the distance of all the data points to the line to be minimal.\",React.createElement(\"br\",null),React.createElement(\"img\",{src:Outliner,alt:\"Equations\",className:\"responsive\",style:redesign}),React.createElement(\"br\",null),\"L2 is the new best-fit line after the addition of an outlier. Seems good till now. But the problem is, if we closely observe, some of the data points are wrongly classified. Certainly, it increases the error term This again is a problem with the linear regression model.\",React.createElement(\"br\",null),React.createElement(\"br\",null),React.createElement(\"b\",null,\"The two limitations of using a linear regression model for classification problems are:\"),React.createElement(\"ul\",null,React.createElement(\"li\",null,\"The predicted value may exceed the range (0,1).\"),React.createElement(\"li\",null,\"Error rate increases if the data has outliers.\")),React.createElement(\"h3\",null,\"How does Logistic Regression Work?\"),React.createElement(\"b\",null,\"Sigmoid Function: \"),\"The sigmoid function is useful to map any predicted values of probabilities into another value between 0 and 1.\",React.createElement(\"br\",null),React.createElement(\"img\",{src:Sigmoid,alt:\"Equations\",className:\"responsive\",style:redesign}),React.createElement(\"br\",null),\"We started with a linear equation and ended up with a logistic regression model with the help of a sigmoid function.\",React.createElement(\"br\",null),React.createElement(\"ul\",null,React.createElement(\"li\",null,React.createElement(\"b\",null,\"Linear model: \"),\"\\u0177 = b0+b1x\"),React.createElement(\"li\",null,React.createElement(\"b\",null,\"Sigmoid function: \"),\"\\u03C3(z) = 1/(1+e\\u2212z)\"),React.createElement(\"li\",null,React.createElement(\"b\",null,\"Logistic regression model: \"),\"\\u0177 = \\u03C3(b0+b1x) = 1/(1+e-(b0+b1x))\")),React.createElement(\"br\",null),React.createElement(\"img\",{src:LinearVsLogistic,alt:\"Equations\",className:\"responsive\",style:redesign}),React.createElement(\"br\",null),React.createElement(\"br\",null),\"The image that depicts the working of the Logistic regression model.\",React.createElement(\"br\",null),React.createElement(\"img\",{src:Logistic,alt:\"Equations\",className:\"responsive\",style:redesign}),React.createElement(\"br\",null),React.createElement(\"br\",null),React.createElement(\"ul\",null,React.createElement(\"li\",null,\"A linear equation (z) is given to a sigmoidal activation function (\\u03C3) to predict the output (\\u0177).\"),React.createElement(\"li\",null,\"To evaluate the performance of the model, we calculate the loss. The most commonly used loss function is the mean squared error.\"),React.createElement(\"li\",null,\"But in logistic regression, as the output is a probability value between 0 or 1, mean squared error wouldn\\u2019t be the right choice. So, instead, we use the cross-entropy loss function.\"),React.createElement(\"li\",null,\"The cross-entropy loss function is used to measure the performance of a classification model whose output is a probability value.\")),React.createElement(\"br\",null),React.createElement(\"h3\",null,\"Predicting if a person would buy life insurnace based on his age using logistic regression\"),\"Above is a binary logistic regression problem as there are only two possible outcomes (i.e. if person buys insurance or doesn't).\",React.createElement(\"div\",{style:titles},React.createElement(PrismCode,{code:cluster,language:\"js\",plugins:[\"line-numbers\"]})),React.createElement(\"br\",null),React.createElement(\"h3\",null,\"Logistic Regression Multiclass\"),\"Predicting if a person would buy life insurnace based on his age using logistic regression.\",React.createElement(\"br\",null),React.createElement(\"br\",null),\"Above is a binary logistic regression problem as there are only two possible outcomes (i.e. if person buys insurance or doesn't).\",React.createElement(\"div\",{style:titles},React.createElement(PrismCode,{code:multiClass,language:\"js\",plugins:[\"line-numbers\"]})),React.createElement(\"br\",null),React.createElement(\"h3\",null,\"Defined sigmoid function and do the math\"),React.createElement(\"div\",{style:titles},React.createElement(PrismCode,{code:sigmoid,language:\"js\",plugins:[\"line-numbers\"]})),React.createElement(\"br\",null),React.createElement(\"h3\",null,\"Logistic Regration\"),React.createElement(\"div\",{style:titles},React.createElement(PrismCode,{code:stack,language:\"js\",plugins:[\"line-numbers\"]})),React.createElement(\"br\",null),React.createElement(\"b\",null,\"Testing\"),React.createElement(\"div\",{style:titles},React.createElement(PrismCode,{code:testings,language:\"js\",plugins:[\"line-numbers\"]})),React.createElement(\"br\",null),React.createElement(\"h3\",null,\"Polynomial Regression:\"),React.createElement(\"ul\",null,React.createElement(\"li\",null,\"If data points clearly will not fit a linear regression, it might be ideal for polynomial regression.\"),React.createElement(\"li\",null,\"Polynomial regression, like linear regression, uses the relationship between the variables x and y to find the best way to draw a line through the data points.\")),React.createElement(\"br\",null),React.createElement(\"b\",null,\"How Does it Work?\"),React.createElement(\"br\",null),\"Example, we have registered 18 cars as they were passing a certain tollbooth.\",React.createElement(\"br\",null),\"We have registered the car's speed, and the time of day (hour) the passing occurred.\",React.createElement(\"br\",null),React.createElement(\"br\",null),\"The x-axis represents the hours of the day and the y-axis represents the speed:\",React.createElement(\"br\",null),React.createElement(\"b\",null,\"ex. Start by drawing a scatter plot:\"),React.createElement(\"br\",null),React.createElement(\"br\",null),React.createElement(\"div\",{style:titles},React.createElement(PrismCode,{code:poly,language:\"js\",plugins:[\"line-numbers\"]})),React.createElement(\"br\",null),React.createElement(\"b\",null,\"Draw the line of Polynomial Regression.\"),React.createElement(\"div\",{style:titles},React.createElement(PrismCode,{code:poly_2,language:\"js\",plugins:[\"line-numbers\"]})),React.createElement(\"br\",null),React.createElement(\"h3\",null,\"R-Squared\"),\"The relationship is measured with a value of r-squared.\",React.createElement(\"br\",null),React.createElement(\"h3\",null,\"Multiple Regression:\"),\"Multiple regression is like linear regression, but with more than one independent value, meaning that we try to predict a value based on two/ more variables.\"))));}}]);return LogisticRegs;}(Component);export default withStyles(styles)(LogisticRegs);","map":{"version":3,"sources":["/home/mukeshs/Projects/edurights/src/components/ml/deepMl/logisticReg.js"],"names":["React","Component","Prism","Grid","Paper","withStyles","List","Sidebar","PrismCode","Equations","Thersolds","Outliner","Sigmoid","LinearVsLogistic","Logistic","titles","backgroundColor","padding","fontSize","redesign","height","width","styles","theme","paper","margin","spacing","smMargin","actionDiv","textAlign","cluster","trim","multiClass","sigmoid","stack","testings","poly","poly_2","LogisticRegs","setTimeout","highlightAll","classes","props"],"mappings":"6kBAAA,MAAOA,CAAAA,KAAP,EAAgBC,SAAhB,KAAiC,OAAjC,CACA,MAAOC,CAAAA,KAAP,KAAkB,SAAlB,CACA,OAASC,IAAT,CAAeC,KAAf,CAAsBC,UAAtB,CAAkCC,IAAlC,KAA8C,mBAA9C,CAEA,MAAO,0BAAP,CACA,MAAOC,CAAAA,OAAP,KAAoB,YAApB,CACA,MAAOC,CAAAA,SAAP,KAAsB,yBAAtB,CACA,MAAOC,CAAAA,SAAP,KAAsB,0CAAtB,CACA,MAAOC,CAAAA,SAAP,KAAsB,iCAAtB,CACA,MAAOC,CAAAA,QAAP,KAAqB,gDAArB,CACA,MAAOC,CAAAA,OAAP,KAAoB,yCAApB,CACA,MAAOC,CAAAA,gBAAP,KAA6B,yCAA7B,CACA,MAAOC,CAAAA,QAAP,KAAqB,iCAArB,CAGA,GAAMC,CAAAA,MAAM,CAAG,CAAEC,eAAe,CAAE,SAAnB,CAA8BC,OAAO,CAAE,KAAvC,CAA8CC,QAAQ,CAAE,MAAxD,CAAf,CAEA,GAAMC,CAAAA,QAAQ,CAAG,CACfC,MAAM,CAAE,GADO,CAEfC,KAAK,CAAE,GAFQ,CAAjB,CAKA,GAAMC,CAAAA,MAAM,CAAG,QAATA,CAAAA,MAAS,CAAAC,KAAK,QAAK,CACvBC,KAAK,CAAE,CACLC,MAAM,CAAEF,KAAK,CAACG,OAAN,CAAc,CAAd,CADH,CAELT,OAAO,CAAEM,KAAK,CAACG,OAAN,CAAc,CAAd,CAFJ,CADgB,CAKvBC,QAAQ,CAAE,CACRF,MAAM,CAAEF,KAAK,CAACG,OAAN,CAAc,CAAd,CADA,CALa,CAQvBE,SAAS,CAAE,CACTC,SAAS,CAAE,QADF,CARY,CAAL,EAApB,CAcA,GAAMC,CAAAA,OAAO,CAAG,4lCAqCdC,IArCc,EAAhB,CAuCA,GAAMC,CAAAA,UAAU,CAAG,uiBAkBjBD,IAlBiB,EAAnB,CAoBA,GAAME,CAAAA,OAAO,CAAG,4RAeZF,IAfY,EAAhB,CAiBA,GAAMG,CAAAA,KAAK,CAAG,wwCAoCVH,IApCU,EAAd,CAsCA,GAAMI,CAAAA,QAAQ,CAAG,otBAsBfJ,IAtBe,EAAjB,CAwBA,GAAMK,CAAAA,IAAI,CAAG,6LAQXL,IARW,EAAb,CAUA,GAAMM,CAAAA,MAAM,CAAG,6QAWbN,IAXa,EAAf,C,GAcMO,CAAAA,Y,oTACgB,CAClBC,UAAU,CAAC,iBAAMrC,CAAAA,KAAK,CAACsC,YAAN,EAAN,EAAD,CAA6B,CAA7B,CAAV,CACD,C,uCACQ,IACCC,CAAAA,OADD,CACa,KAAKC,KADlB,CACCD,OADD,CAEP,MACE,qBAAC,IAAD,EAAM,SAAS,KAAf,EACE,oBAAC,IAAD,EAAM,IAAI,KAAV,CAAW,EAAE,CAAE,CAAf,EACE,oBAAC,KAAD,EAAO,SAAS,CAAEA,OAAO,CAACjB,KAA1B,EACE,8BAAI,oBAAC,OAAD,MAAJ,CADF,CADF,CADF,CAME,oBAAC,IAAD,EAAM,IAAI,KAAV,CAAW,EAAE,CAAE,EAAf,EACE,oBAAC,KAAD,EAAO,SAAS,CAAEiB,OAAO,CAACjB,KAA1B,EACE,oBAAC,IAAD,MACE,oEADF,6JAIE,8BAJF,CAKE,8BALF,CAME,6BAAG,4DAAH,+DANF,CAOE,8BAPF,CAQE,8BARF,CASE,4DATF,CAUE,8BACE,8BAAI,4DAAJ,uDADF,CAEE,8BAAI,8DAAJ,kEAFF,CAVF,CAcE,8BAdF,CAeE,iEAfF,CAgBE,8BAhBF,8IAmBE,8BAnBF,CAoBE,8BApBF,CAsBE,8BACE,8BAAI,iEAAJ,iFADF,CAEE,8BACE,8BAAI,8CAAJ,2BADF,CAEE,8BAAI,6CAAJ,uCAFF,CAFF,CAME,8BANF,CAOE,8BAAI,6DAAJ,wFAPF,CAQE,8BACE,8BAAI,iDAAJ,8DADF,CAEE,8BAAI,8CAAJ,2CAFF,CARF,CAtBF,CAmCE,8BAnCF,+MAsCE,8BAtCF,CAuCE,2BAAK,GAAG,CAAEf,SAAV,CAAqB,GAAG,CAAC,WAAzB,CAAqC,SAAS,CAAC,YAA/C,CAA4D,KAAK,CAAEU,QAAnE,EAvCF,CAwCE,8BAxCF,4GA0CE,8BA1CF,CA2CE,8BA3CF,4FA6CE,8BA7CF,CA8CE,2BAAK,GAAG,CAAET,SAAV,CAAqB,GAAG,CAAC,WAAzB,CAAqC,SAAS,CAAC,YAA/C,CAA4D,KAAK,CAAES,QAAnE,EA9CF,CA+CE,8BACE,8BAAI,wCAAJ,uGADF,CAEE,8BAAI,wCAAJ,gHAFF,CAGE,8BAAI,wCAAJ,iDAHF,CAIE,8BAAI,wCAAJ,gDAJF,CA/CF,gRAwDE,8BAxDF,CAyDE,8BAzDF,4JA4DE,8BA5DF,CA6DE,2BAAK,GAAG,CAAER,QAAV,CAAoB,GAAG,CAAC,WAAxB,CAAoC,SAAS,CAAC,YAA9C,CAA2D,KAAK,CAAEQ,QAAlE,EA7DF,CA8DE,8BA9DF,kRAkEE,8BAlEF,CAmEE,8BAnEF,CAoEE,uHApEF,CAqEE,8BACE,gFADF,CAEE,+EAFF,CArEF,CA0EE,mEA1EF,CA2EE,kDA3EF,mHA4EE,8BA5EF,CA6EE,2BAAK,GAAG,CAAEP,OAAV,CAAmB,GAAG,CAAC,WAAvB,CAAmC,SAAS,CAAC,YAA7C,CAA0D,KAAK,CAAEO,QAAjE,EA7EF,CA8EE,8BA9EF,wHAiFE,8BAjFF,CAkFE,8BACE,8BAAI,8CAAJ,mBADF,CAEE,8BAAI,kDAAJ,8BAFF,CAGE,8BAAI,2DAAJ,8CAHF,CAlFF,CAuFE,8BAvFF,CAwFE,2BAAK,GAAG,CAAEN,gBAAV,CAA4B,GAAG,CAAC,WAAhC,CAA4C,SAAS,CAAC,YAAtD,CAAmE,KAAK,CAAEM,QAA1E,EAxFF,CAyFE,8BAzFF,CA0FE,8BA1FF,wEA4FE,8BA5FF,CA6FE,2BAAK,GAAG,CAAEL,QAAV,CAAoB,GAAG,CAAC,WAAxB,CAAoC,SAAS,CAAC,YAA9C,CAA2D,KAAK,CAAEK,QAAlE,EA7FF,CA8FE,8BA9FF,CA+FE,8BA/FF,CAiGE,8BACE,2IADF,CAEE,iKAFF,CAGE,4NAHF,CAIE,kKAJF,CAjGF,CAuGE,8BAvGF,CAyGE,2HAzGF,qIA2GE,2BAAK,KAAK,CAAEJ,MAAZ,EACE,oBAAC,SAAD,EACE,IAAI,CAAEe,OADR,CAEE,QAAQ,CAAC,IAFX,CAGE,OAAO,CAAE,CAAC,cAAD,CAHX,EADF,CA3GF,CAkHE,8BAlHF,CAoHE,+DApHF,+FAsHE,8BAtHF,CAuHE,8BAvHF,qIAyHE,2BAAK,KAAK,CAAEf,MAAZ,EACE,oBAAC,SAAD,EACE,IAAI,CAAEiB,UADR,CAEE,QAAQ,CAAC,IAFX,CAGE,OAAO,CAAE,CAAC,cAAD,CAHX,EADF,CAzHF,CAgIE,8BAhIF,CAkIE,yEAlIF,CAmIE,2BAAK,KAAK,CAAEjB,MAAZ,EACE,oBAAC,SAAD,EACE,IAAI,CAAEkB,OADR,CAEE,QAAQ,CAAC,IAFX,CAGE,OAAO,CAAE,CAAC,cAAD,CAHX,EADF,CAnIF,CA0IE,8BA1IF,CA4IE,mDA5IF,CA6IE,2BAAK,KAAK,CAAElB,MAAZ,EACE,oBAAC,SAAD,EACE,IAAI,CAAEmB,KADR,CAEE,QAAQ,CAAC,IAFX,CAGE,OAAO,CAAE,CAAC,cAAD,CAHX,EADF,CA7IF,CAoJE,8BApJF,CAsJE,uCAtJF,CAuJE,2BAAK,KAAK,CAAEnB,MAAZ,EACE,oBAAC,SAAD,EACE,IAAI,CAAEoB,QADR,CAEE,QAAQ,CAAC,IAFX,CAGE,OAAO,CAAE,CAAC,cAAD,CAHX,EADF,CAvJF,CA8JE,8BA9JF,CAgKE,uDAhKF,CAiKE,8BACE,sIADF,CAEE,gMAFF,CAjKF,CAsKE,8BAtKF,CAwKE,iDAxKF,CAwK0B,8BAxK1B,iFA0KE,8BA1KF,wFA4KE,8BA5KF,CA6KE,8BA7KF,mFA+KE,8BA/KF,CAgLE,oEAhLF,CAiLE,8BAjLF,CAkLE,8BAlLF,CAmLE,2BAAK,KAAK,CAAEpB,MAAZ,EACE,oBAAC,SAAD,EACE,IAAI,CAAEqB,IADR,CAEE,QAAQ,CAAC,IAFX,CAGE,OAAO,CAAE,CAAC,cAAD,CAHX,EADF,CAnLF,CA0LE,8BA1LF,CA4LE,uEA5LF,CA6LE,2BAAK,KAAK,CAAErB,MAAZ,EACE,oBAAC,SAAD,EACE,IAAI,CAAEsB,MADR,CAEE,QAAQ,CAAC,IAFX,CAGE,OAAO,CAAE,CAAC,cAAD,CAHX,EADF,CA7LF,CAoME,8BApMF,CAsME,0CAtMF,2DAwME,8BAxMF,CA0ME,qDA1MF,iKADF,CADF,CANF,CADF,CA2ND,C,0BAjOwBpC,S,EAoO3B,cAAgBI,CAAAA,UAAU,CAACiB,MAAD,CAAV,CAAmBgB,YAAnB,CAAhB","sourcesContent":["import React, { Component } from 'react';\nimport Prism from \"prismjs\"\nimport { Grid, Paper, withStyles, List } from \"@material-ui/core\";\n\nimport '../../ReactJs/styles.css'\nimport Sidebar from '../sidebar';\nimport PrismCode from '../../ReactJs/prismCode';\nimport Equations from '../../../assets/ML/linear_regression.png'\nimport Thersolds from '../../../assets/ML/thersold.png'\nimport Outliner from '../../../assets/ML/regression_with_outlier.png'\nimport Sigmoid from '../../../assets/ML/sigmoid_function.png'\nimport LinearVsLogistic from '../../../assets/ML/LinearVsLogistic.jpg'\nimport Logistic from '../../../assets/ML/logistic.jpg'\n\n\nconst titles = { backgroundColor: '#F0F8FF', padding: '1px', fontSize: '16px' }\n\nconst redesign = {\n  height: 350,\n  width: 600\n}\n\nconst styles = theme => ({\n  paper: {\n    margin: theme.spacing(1),\n    padding: theme.spacing(1)\n  },\n  smMargin: {\n    margin: theme.spacing(1)\n  },\n  actionDiv: {\n    textAlign: \"center\"\n  }\n})\n\n\nconst cluster = `\nfrom matplotlib import pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\n%matplotlib inline\n\ndf = pd.read_csv(\"insurance_data.csv\")\n\nplt.scatter(df.age,df.bought_insurance,marker='+',color='red')\nX_train, X_test, y_train, y_test = train_test_split(df[['age']],df.bought_insurance,train_size=0.8)\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train)\n\ny_predicted = model.predict(X_test)\nmodel.predict_proba(X_test)\nmodel.score(X_test,y_test)\n\ny_predicted\nmodel.coef_                                                       #model.coef_ indicates value of m in y=m*x + b.\nmodel.intercept_                                                  #model.intercept_ indicates value of b in y=m*x + b.\n\n\n#Lets defined sigmoid function and do the math.\nimport math\ndef sigmoid(x):\n  return 1 / (1 + math.exp(-x))\n  \ndef prediction_function(age):\n  z = 0.042 * age - 1.53 # 0.04150133 ~ 0.042 and -1.52726963 ~ -1.53\n  y = sigmoid(z)\n  return y\n  \nage = 35\nprediction_function(age)\n\nage = 43\nprediction_function(age)\n`.trim();\n\nconst multiClass = `\nfrom matplotlib import pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\n%matplotlib inline\n\ndf = pd.read_csv(\"insurance_data.csv\")\n\nplt.scatter(df.age,df.bought_insurance,marker='+',color='red')\nX_train, X_test, y_train, y_test = train_test_split(df[['age']],df.bought_insurance,train_size=0.8)\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train)\n\ny_predicted = model.predict(X_test)\nmodel.predict_proba(X_test)\nmodel.score(X_test,y_test)\n\ny_predicted\n`.trim();\n\nconst sigmoid = `\nimport math\ndef sigmoid(x):\n  return 1 / (1 + math.exp(-x))\n  \ndef prediction_function(age):\n  z = 0.042 * age - 1.53 # 0.04150133 ~ 0.042 and -1.52726963 ~ -1.53\n  y = sigmoid(z)\n  return y\n  \nage = 35\nprediction_function(age)\n\nage = 43\nprediction_function(age)\n  `.trim();\n\nconst stack = `\n  import numpy as np\n  \n  \n  class LogisticRegression:\n      def __init__(self, learning_rate=0.001, n_iters=1000):\n          self.lr = learning_rate\n          self.n_iters = n_iters\n          self.weights = None\n          self.bias = None\n  \n      def fit(self, X, y):\n          n_samples, n_features = X.shape\n  \n          self.weights = np.zeros(n_features)\n          self.bias = 0\n  \n          for _ in range(self.n_iters):\n              # approximate y with linear combination of weights and x, plus bias\n              linear_model = np.dot(X, self.weights) + self.bias\n              y_predicted = self._sigmoid(linear_model)                                 # apply sigmoid function\n  \n              dw = (1 / n_samples) * np.dot(X.T, (y_predicted - y))\n              db = (1 / n_samples) * np.sum(y_predicted - y)\n              \n              self.weights -= self.lr * dw\n              self.bias -= self.lr * db\n  \n      def predict(self, X):\n          linear_model = np.dot(X, self.weights) + self.bias\n          y_predicted = self._sigmoid(linear_model)\n          y_predicted_cls = [1 if i > 0.5 else 0 for i in y_predicted]\n          return np.array(y_predicted_cls)\n  \n      def _sigmoid(self, x):\n          return 1 / (1 + np.exp(-x))\n  `.trim();\n\nconst testings = `\nif __name__ == \"__main__\":\n      # Imports\n      from sklearn.model_selection import train_test_split\n      from sklearn import datasets\n  \n      def accuracy(y_true, y_pred):\n          accuracy = np.sum(y_true == y_pred) / len(y_true)\n          return accuracy\n  \n      bc = datasets.load_breast_cancer()\n      X, y = bc.data, bc.target\n  \n      X_train, X_test, y_train, y_test = train_test_split(\n          X, y, test_size=0.2, random_state=1234\n      )\n  \n      regressor = LogisticRegression(learning_rate=0.0001, n_iters=1000)\n      regressor.fit(X_train, y_train)\n      predictions = regressor.predict(X_test)\n  \n      print(\"LR classification accuracy:\", accuracy(y_test, predictions))\n`.trim();\n\nconst poly = `\nimport matplotlib.pyplot as plt\n\nx = [1,2,3,5,6,7,8,9,10,12,13,14,15,16,18,19,21,22]\ny = [100,90,80,60,60,55,60,65,70,70,75,76,78,79,90,99,99,100]\n\nplt.scatter(x, y)\nplt.show()\n`.trim()\n\nconst poly_2 = `\nx = [1,2,3,5,6,7,8,9,10,12,13,14,15,16,18,19,21,22]\ny = [100,90,80,60,60,55,60,65,70,70,75,76,78,79,90,99,99,100]\n\nmymodel = np.poly1d(np.polyfit(x, y, 3))\n\nmyline = np.linspace(1, 22, 100)\n\nplt.scatter(x, y)\nplt.plot(myline, mymodel(myline))\nplt.show()\n`.trim()\n\n\nclass LogisticRegs extends Component {\n  componentDidMount() {\n    setTimeout(() => Prism.highlightAll(), 0)\n  }\n  render() {\n    const { classes } = this.props;\n    return (\n      <Grid container>\n        <Grid item xs={2}>\n          <Paper className={classes.paper}>\n            <h4><Sidebar /></h4>\n          </Paper>\n        </Grid>\n        <Grid item xs={10}>\n          <Paper className={classes.paper}>\n            <List>\n              <h3>Logistic Regression (Supervised ML)</h3>\n              Used to model the probability of a certain class/ event. It is used when the data is linearly\n              separable and the outcome is binary/ dichotomous in nature.\n              <br />\n              <br />\n              <i><b>Ex. of Binary classification</b> Yes/No, Pass/Fail, Win/Lose, Cancerous/Non-cancerous, etc.</i>\n              <br />\n              <br />\n              <b>Types of Logistic Regression</b>\n              <ul>\n                <li><b>Simple Logistic Regression: </b>A single independent is used to predict the output.</li>\n                <li><b>Multiple logistic regression: </b>Multiple independent variables are used to predict the output.</li>\n              </ul>\n              <br />\n              <b>Extensions of Logistic Regression</b>\n              <br />\n              Although it is said Logistic regression is used for Binary Classification, it can be extended to\n              solve multiclass classification problems.\n              <br />\n              <br />\n\n              <ul>\n                <li><b>Multinomial Logistic Regression: </b>The o/p variable is discrete in three/ more classes with no natural ordering.</li>\n                <ul>\n                  <li><b>Food texture: </b>Crunchy, Mushy, Crispy.</li>\n                  <li><b>Hair colour: </b>Blonde, Brown, Brunette, Red​.</li>\n                </ul>\n                <br />\n                <li><b>Ordered Logistic Regression: </b>The o/p variable is discrete in three/ more classes with the ordering of the levels.</li>\n                <ul>\n                  <li><b>Customer Rating: </b>extremely dislike, dislike, neutral, like, extremely like.</li>\n                  <li><b>Income level: </b>low income, middle income, high income.</li>\n                </ul>\n              </ul>\n              <br />\n              Now, let us try if we can use linear regression to solve a binary class classification problem. Assume we have a\n              dataset that is linearly separable and has the o/p that is discrete in two classes (0, 1).\n              <br />\n              <img src={Equations} alt=\"Equations\" className=\"responsive\" style={redesign} />\n              <br />\n              we draw a straight line L1 such that the sum of distances of all the data points to the line is minimal.\n              <br />\n              <br />\n              We define a threshold T = 0.5, above which the o/p belongs to class 1 otherwise class 0.\n              <br />\n              <img src={Thersolds} alt=\"Equations\" className=\"responsive\" style={redesign} />\n              <ul>\n                <li><b>Case 1: </b>The predicted value for x1 is ≈0.2 which is less than the threshold, so x1 belongs to class 0.</li>\n                <li><b>Case 2: </b>Predicted value for the point x2 is ≈0.6 which is greater than the threshold, so x2 belongs to class 1.</li>\n                <li><b>Case 3: </b>Predicted value for the point x3 is beyond 1.</li>\n                <li><b>Case 4: </b>Predicted value for the point x4 is below 0.</li>\n              </ul>\n              The predicted values for the points x3, x4 exceed the range (0,1) which doesn’t make sense because the\n              probability values always lie between 0 and 1. And our output can have only two values either 0 or 1. Hence,\n              this is a problem with the linear regression model.\n              <br />\n              <br />\n              Now, introduce an outlier and see what happens. The regression line gets deviated to keep the distance of all the\n              data points to the line to be minimal.\n              <br />\n              <img src={Outliner} alt=\"Equations\" className=\"responsive\" style={redesign} />\n              <br />\n              L2 is the new best-fit line after the addition of an outlier. Seems good till now. But the problem is,\n              if we closely observe, some of the data points are wrongly classified. Certainly, it increases the error term\n              This again is a problem with the linear regression model.\n              <br />\n              <br />\n              <b>The two limitations of using a linear regression model for classification problems are:</b>\n              <ul>\n                <li>The predicted value may exceed the range (0,1).</li>\n                <li>Error rate increases if the data has outliers.</li>\n              </ul>\n\n              <h3>How does Logistic Regression Work?</h3>\n              <b>Sigmoid Function: </b>The sigmoid function is useful to map any predicted values of probabilities into another value between 0 and 1.\n              <br />\n              <img src={Sigmoid} alt=\"Equations\" className=\"responsive\" style={redesign} />\n              <br />\n\n              We started with a linear equation and ended up with a logistic regression model with the help of a sigmoid function.\n              <br />\n              <ul>\n                <li><b>Linear model: </b>ŷ = b0+b1x</li>\n                <li><b>Sigmoid function: </b>σ(z) = 1/(1+e−z)</li>\n                <li><b>Logistic regression model: </b>ŷ = σ(b0+b1x) = 1/(1+e-(b0+b1x))</li>\n              </ul>\n              <br />\n              <img src={LinearVsLogistic} alt=\"Equations\" className=\"responsive\" style={redesign} />\n              <br />\n              <br />\n              The image that depicts the working of the Logistic regression model.\n              <br />\n              <img src={Logistic} alt=\"Equations\" className=\"responsive\" style={redesign} />\n              <br />\n              <br />\n\n              <ul>\n                <li>A linear equation (z) is given to a sigmoidal activation function (σ) to predict the output (ŷ).</li>\n                <li>To evaluate the performance of the model, we calculate the loss. The most commonly used loss function is the mean squared error.</li>\n                <li>But in logistic regression, as the output is a probability value between 0 or 1, mean squared error wouldn’t be the right choice. So, instead, we use the cross-entropy loss function.</li>\n                <li>The cross-entropy loss function is used to measure the performance of a classification model whose output is a probability value.</li>\n              </ul>\n              <br />\n\n              <h3>Predicting if a person would buy life insurnace based on his age using logistic regression</h3>\n              Above is a binary logistic regression problem as there are only two possible outcomes (i.e. if person buys insurance or doesn't).\n              <div style={titles}>\n                <PrismCode\n                  code={cluster}\n                  language=\"js\"\n                  plugins={[\"line-numbers\"]}\n                />\n              </div>\n              <br />\n\n              <h3>Logistic Regression Multiclass</h3>\n              Predicting if a person would buy life insurnace based on his age using logistic regression.\n              <br />\n              <br />\n              Above is a binary logistic regression problem as there are only two possible outcomes (i.e. if person buys insurance or doesn't).\n              <div style={titles}>\n                <PrismCode\n                  code={multiClass}\n                  language=\"js\"\n                  plugins={[\"line-numbers\"]}\n                />\n              </div>\n              <br />\n\n              <h3>Defined sigmoid function and do the math</h3>\n              <div style={titles}>\n                <PrismCode\n                  code={sigmoid}\n                  language=\"js\"\n                  plugins={[\"line-numbers\"]}\n                />\n              </div>\n              <br />\n\n              <h3>Logistic Regration</h3>\n              <div style={titles}>\n                <PrismCode\n                  code={stack}\n                  language=\"js\"\n                  plugins={[\"line-numbers\"]}\n                />\n              </div>\n              <br />\n\n              <b>Testing</b>\n              <div style={titles}>\n                <PrismCode\n                  code={testings}\n                  language=\"js\"\n                  plugins={[\"line-numbers\"]}\n                />\n              </div>\n              <br />\n\n              <h3>Polynomial Regression:</h3>\n              <ul>\n                <li>If data points clearly will not fit a linear regression, it might be ideal for polynomial regression.</li>\n                <li>Polynomial regression, like linear regression, uses the relationship between the variables x and y to find the best\n                  way to draw a line through the data points.</li>\n              </ul>\n              <br />\n\n              <b>How Does it Work?</b><br />\n              Example, we have registered 18 cars as they were passing a certain tollbooth.\n              <br />\n              We have registered the car's speed, and the time of day (hour) the passing occurred.\n              <br />\n              <br />\n              The x-axis represents the hours of the day and the y-axis represents the speed:\n              <br />\n              <b>ex. Start by drawing a scatter plot:</b>\n              <br />\n              <br />\n              <div style={titles}>\n                <PrismCode\n                  code={poly}\n                  language=\"js\"\n                  plugins={[\"line-numbers\"]}\n                />\n              </div>\n              <br />\n\n              <b>Draw the line of Polynomial Regression.</b>\n              <div style={titles}>\n                <PrismCode\n                  code={poly_2}\n                  language=\"js\"\n                  plugins={[\"line-numbers\"]}\n                />\n              </div>\n              <br />\n\n              <h3>R-Squared</h3>\n              The relationship is measured with a value of r-squared.\n              <br />\n\n              <h3>Multiple Regression:</h3>\n              Multiple regression is like linear regression, but with more than one independent value, meaning that we try to\n              predict a value based on two/ more variables.\n            </List>\n          </Paper>\n        </Grid>\n      </Grid>\n    )\n  }\n}\n\nexport default (withStyles(styles)(LogisticRegs));\n"]},"metadata":{},"sourceType":"module"}