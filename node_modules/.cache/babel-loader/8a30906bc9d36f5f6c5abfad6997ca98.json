{"ast":null,"code":"import _classCallCheck from\"/home/mukeshs/Projects/edurights/node_modules/@babel/runtime/helpers/esm/classCallCheck\";import _createClass from\"/home/mukeshs/Projects/edurights/node_modules/@babel/runtime/helpers/esm/createClass\";import _possibleConstructorReturn from\"/home/mukeshs/Projects/edurights/node_modules/@babel/runtime/helpers/esm/possibleConstructorReturn\";import _getPrototypeOf from\"/home/mukeshs/Projects/edurights/node_modules/@babel/runtime/helpers/esm/getPrototypeOf\";import _inherits from\"/home/mukeshs/Projects/edurights/node_modules/@babel/runtime/helpers/esm/inherits\";import React,{Component}from'react';import Prism from\"prismjs\";import{Grid,Paper,withStyles,List}from\"@material-ui/core\";import'../../ReactJs/styles.css';import Sidebar from'../sidebar';import PrismCode from'../../ReactJs/prismCode';import gradient from'../../../assets/ML/output.png';var titles={backgroundColor:'#F0F8FF',padding:'1px',fontSize:'16px'};var redesign={height:200,width:500};var styles=function styles(theme){return{paper:{margin:theme.spacing(1),padding:theme.spacing(1)},smMargin:{margin:theme.spacing(1)},actionDiv:{textAlign:\"center\"}};};var cluster=\"\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\n\\n%matplotlib inline\\ndef gradient_descent(x,y):\\n    m_curr = b_curr = 0\\n    rate = 0.01\\n    n = len(x)\\n    plt.scatter(x,y,color='red',marker='+',linewidth='5')\\n    for i in range(10000):\\n        y_predicted = m_curr * x + b_curr\\n        print (m_curr,b_curr, i)\\n        plt.plot(x,y_predicted,color='green')\\n        md = -(2/n)*sum(x*(y-y_predicted))\\n        yd = -(2/n)*sum(y-y_predicted)\\n        m_curr = m_curr - rate * md\\n        b_curr = b_curr - rate * yd\\n        \\n        \\nx = np.array([1,2,3,4,5])\\ny = np.array([5,7,9,11,13])\\n\\ngradient_descent(x,y) \\n\".trim();var gradientPy=\"\\nimport numpy as np\\n\\ndef gradient_descent(x,y):\\n    m=b = 0\\n    iterations = 1000\\n    n = len(x)\\n    learning_rate = 0.08\\n\\n    for i in range(iterations):\\n        y_predicted = m*x + b\\n        cost = (1/n) * sum([val**2 for val in (y-y_predicted)])\\n        md = -(2/n)*sum(x*(y-y_predicted))\\n        bd = -(2/n)*sum(y-y_predicted)\\n        m = m-learning_rate * md\\n        b = b-learning_rate * bd\\n        print (\\\"m {}, b {}, cost {} iteration {}\\\".format(m,b,cost, i))\\n\\nx = np.array([1,2,3,4,5])\\ny = np.array([5,7,9,11,13])\\n\\ngradient_descent(x,y)\".trim();var chains=\"\\ndl/dw = dl/de * de/dy * dy/dw\\n\".trim();var Gradient=/*#__PURE__*/function(_Component){_inherits(Gradient,_Component);function Gradient(){_classCallCheck(this,Gradient);return _possibleConstructorReturn(this,_getPrototypeOf(Gradient).apply(this,arguments));}_createClass(Gradient,[{key:\"componentDidMount\",value:function componentDidMount(){setTimeout(function(){return Prism.highlightAll();},0);}},{key:\"render\",value:function render(){var classes=this.props.classes;return React.createElement(Grid,{container:true},React.createElement(Grid,{item:true,xs:2},React.createElement(Paper,{className:classes.paper},React.createElement(\"h4\",null,React.createElement(Sidebar,null)))),React.createElement(Grid,{item:true,xs:10},React.createElement(Paper,{className:classes.paper},React.createElement(List,null,React.createElement(\"h3\",null,\"Gradient descent\"),\"Gradient descent is an optimization algorithm that's used when training a ML model. It's based on a convex function and tweaks its parameters iteratively to minimize a given function to its local minimum.\",React.createElement(\"br\",null),React.createElement(\"br\",null),React.createElement(\"b\",null,\"A gradient measures how much the output of a function changes if you change the inputs a little bit\"),React.createElement(\"br\",null),React.createElement(\"h3\",null,\"WHAT IS A GRADIENT?\"),\"A gradient is a derivative of a function that has more than one input variable. Known as the slope of a function, the gradient simply measures the change in all weights w.r.t the change in error.\",React.createElement(\"br\",null),React.createElement(\"br\",null),React.createElement(\"b\",null,\"Importance of the Learning Rate:\"),React.createElement(\"ul\",null,React.createElement(\"li\",null,\"How big the steps are gradient descent takes into the direction of the local minimum are determined by the learning rate, which figures out how fast/ slow we will move towards the optimal weights.\"),React.createElement(\"li\",null,\"To reach the local minimum we must set the learning rate to an appropriate value, which is neither too low nor too high. This is important because if the steps it takes are too big, it may not reach the local minimum because it bounces back and forth between the convex function of gradient descent. If we set the learning rate to a very small value, gradient descent will eventually reach the local minimum but that may take a while. \")),React.createElement(\"br\",null),React.createElement(\"h3\",null,\"Types of Gradient Descent\"),\"Three types based on amount of data they use.\",React.createElement(\"ul\",null,React.createElement(\"li\",null,React.createElement(\"b\",null,\"BATCH GRADIENT DESCENT(vanilla gradient descen): \"),\"Calculates the error.\"),React.createElement(\"ul\",null,React.createElement(\"li\",null,\"Advantages of batch gradient descent are its computational efficient,it produces a stable error gradient and a stable convergence.\"),React.createElement(\"li\",null,\"we go through all training samples and calculate cumulative error.\"),React.createElement(\"li\",null,\"Now back propagate and adjust weights.\"),React.createElement(\"li\",null,\"Good for small training set.\")),React.createElement(\"br\",null),React.createElement(\"li\",null,React.createElement(\"b\",null,\"STOCHASTIC GRADIENT DESCENT: \"),\"It updates the parameters for each training example one by one.\"),React.createElement(\"ul\",null,React.createElement(\"li\",null,\"Use one(randomly picked) sample for a forward pass and then adjust weights.\"),React.createElement(\"li\",null,\"Good when training set is very big and we don't want too much computation.\")),React.createElement(\"br\",null),React.createElement(\"li\",null,React.createElement(\"b\",null,\"MINI-BATCH GRADIENT DESCENT: \"),\"Combination of the concepts of SGD and batch gradient descent. \"),React.createElement(\"ul\",null,React.createElement(\"li\",null,\"It simply splits the training dataset into small batches and performs an update for each of those batches. This creates a balance between the robustness of SGD and the efficiency of batch gradient descent.\"),React.createElement(\"li\",null,\"Is like SGD instead of choosing 1 randomly picked training sample, We will use a batch of randomly picked training samples.\"),React.createElement(\"li\",null,\"Use a batch of (randomly picked) samples for a forward pass and then adjust weights.\"),React.createElement(\"li\",null,\"Common mini-batch sizes range between 50 and 256.\"))),React.createElement(\"br\",null),React.createElement(\"img\",{src:gradient,alt:\"gradient\",className:\"responsive\",style:redesign}),React.createElement(\"div\",{style:titles},React.createElement(PrismCode,{code:cluster,language:\"js\",plugins:[\"line-numbers\"]})),React.createElement(\"br\",null),React.createElement(\"h3\",null,\"With Python\"),React.createElement(\"div\",{style:titles},React.createElement(PrismCode,{code:gradientPy,language:\"js\",plugins:[\"line-numbers\"]})),React.createElement(\"br\",null),React.createElement(\"h3\",null,\"Chain Rule\"),React.createElement(\"div\",{style:titles},React.createElement(PrismCode,{code:chains,language:\"js\",plugins:[\"line-numbers\"]}))))));}}]);return Gradient;}(Component);export default withStyles(styles)(Gradient);","map":{"version":3,"sources":["/home/mukeshs/Projects/edurights/src/components/ml/deepMl/gredient_decents.js"],"names":["React","Component","Prism","Grid","Paper","withStyles","List","Sidebar","PrismCode","gradient","titles","backgroundColor","padding","fontSize","redesign","height","width","styles","theme","paper","margin","spacing","smMargin","actionDiv","textAlign","cluster","trim","gradientPy","chains","Gradient","setTimeout","highlightAll","classes","props"],"mappings":"6kBAAA,MAAOA,CAAAA,KAAP,EAAgBC,SAAhB,KAAiC,OAAjC,CACA,MAAOC,CAAAA,KAAP,KAAkB,SAAlB,CACA,OAASC,IAAT,CAAeC,KAAf,CAAsBC,UAAtB,CAAkCC,IAAlC,KAA8C,mBAA9C,CAEA,MAAO,0BAAP,CACA,MAAOC,CAAAA,OAAP,KAAoB,YAApB,CACA,MAAOC,CAAAA,SAAP,KAAsB,yBAAtB,CAEA,MAAOC,CAAAA,QAAP,KAAqB,+BAArB,CAGA,GAAMC,CAAAA,MAAM,CAAG,CAAEC,eAAe,CAAE,SAAnB,CAA8BC,OAAO,CAAE,KAAvC,CAA8CC,QAAQ,CAAE,MAAxD,CAAf,CAEA,GAAMC,CAAAA,QAAQ,CAAG,CACfC,MAAM,CAAE,GADO,CAEfC,KAAK,CAAE,GAFQ,CAAjB,CAKA,GAAMC,CAAAA,MAAM,CAAG,QAATA,CAAAA,MAAS,CAAAC,KAAK,QAAK,CACvBC,KAAK,CAAE,CACLC,MAAM,CAAEF,KAAK,CAACG,OAAN,CAAc,CAAd,CADH,CAELT,OAAO,CAAEM,KAAK,CAACG,OAAN,CAAc,CAAd,CAFJ,CADgB,CAKvBC,QAAQ,CAAE,CACRF,MAAM,CAAEF,KAAK,CAACG,OAAN,CAAc,CAAd,CADA,CALa,CAQvBE,SAAS,CAAE,CACTC,SAAS,CAAE,QADF,CARY,CAAL,EAApB,CAcA,GAAMC,CAAAA,OAAO,CAAG,6nBAwBdC,IAxBc,EAAhB,CA0BA,GAAMC,CAAAA,UAAU,CAAG,2jBAqBID,IArBJ,EAAnB,CAuBA,GAAME,CAAAA,MAAM,CAAG,oCAEbF,IAFa,EAAf,C,GAIMG,CAAAA,Q,gSACgB,CAClBC,UAAU,CAAC,iBAAM5B,CAAAA,KAAK,CAAC6B,YAAN,EAAN,EAAD,CAA6B,CAA7B,CAAV,CACD,C,uCACQ,IACCC,CAAAA,OADD,CACa,KAAKC,KADlB,CACCD,OADD,CAEP,MACE,qBAAC,IAAD,EAAM,SAAS,KAAf,EACE,oBAAC,IAAD,EAAM,IAAI,KAAV,CAAW,EAAE,CAAE,CAAf,EACE,oBAAC,KAAD,EAAO,SAAS,CAAEA,OAAO,CAACb,KAA1B,EACE,8BAAI,oBAAC,OAAD,MAAJ,CADF,CADF,CADF,CAME,oBAAC,IAAD,EAAM,IAAI,KAAV,CAAW,EAAE,CAAE,EAAf,EACE,oBAAC,KAAD,EAAO,SAAS,CAAEa,OAAO,CAACb,KAA1B,EACE,oBAAC,IAAD,MACE,iDADF,gNAIE,8BAJF,CAKE,8BALF,CAME,mIANF,CAOE,8BAPF,CASE,oDATF,uMAaE,8BAbF,CAcE,8BAdF,CAeE,gEAfF,CAgBE,8BACE,qOADF,CAGE,odAHF,CAhBF,CAyBE,8BAzBF,CA2BE,0DA3BF,iDA6BE,8BACE,8BAAI,iFAAJ,yBADF,CAEE,8BACE,mKADF,CAGE,mGAHF,CAIE,uEAJF,CAKE,6DALF,CAFF,CASE,8BATF,CAUE,8BAAI,6DAAJ,mEAVF,CAWE,8BACE,4GADF,CAEE,2GAFF,CAXF,CAeE,8BAfF,CAgBE,8BAAI,6DAAJ,mEAhBF,CAiBE,8BACE,8OADF,CAIE,4JAJF,CAKE,qHALF,CAME,kFANF,CAjBF,CA7BF,CAuDE,8BAvDF,CAyDE,2BAAK,GAAG,CAAEV,QAAV,CAAoB,GAAG,CAAC,UAAxB,CAAmC,SAAS,CAAC,YAA7C,CAA0D,KAAK,CAAEK,QAAjE,EAzDF,CA0DE,2BAAK,KAAK,CAAEJ,MAAZ,EACE,oBAAC,SAAD,EACE,IAAI,CAAEe,OADR,CAEE,QAAQ,CAAC,IAFX,CAGE,OAAO,CAAE,CAAC,cAAD,CAHX,EADF,CA1DF,CAiEE,8BAjEF,CAmEE,4CAnEF,CAoEE,2BAAK,KAAK,CAAEf,MAAZ,EACE,oBAAC,SAAD,EACE,IAAI,CAAEiB,UADR,CAEE,QAAQ,CAAC,IAFX,CAGE,OAAO,CAAE,CAAC,cAAD,CAHX,EADF,CApEF,CA2EE,8BA3EF,CA6EE,2CA7EF,CA8EE,2BAAK,KAAK,CAAEjB,MAAZ,EACE,oBAAC,SAAD,EACE,IAAI,CAAEkB,MADR,CAEE,QAAQ,CAAC,IAFX,CAGE,OAAO,CAAE,CAAC,cAAD,CAHX,EADF,CA9EF,CADF,CADF,CANF,CADF,CAmGD,C,sBAzGoB3B,S,EA4GvB,cAAgBI,CAAAA,UAAU,CAACY,MAAD,CAAV,CAAmBY,QAAnB,CAAhB","sourcesContent":["import React, { Component } from 'react';\nimport Prism from \"prismjs\"\nimport { Grid, Paper, withStyles, List } from \"@material-ui/core\";\n\nimport '../../ReactJs/styles.css'\nimport Sidebar from '../sidebar';\nimport PrismCode from '../../ReactJs/prismCode';\n\nimport gradient from '../../../assets/ML/output.png'\n\n\nconst titles = { backgroundColor: '#F0F8FF', padding: '1px', fontSize: '16px' }\n\nconst redesign = {\n  height: 200,\n  width: 500\n}\n\nconst styles = theme => ({\n  paper: {\n    margin: theme.spacing(1),\n    padding: theme.spacing(1)\n  },\n  smMargin: {\n    margin: theme.spacing(1)\n  },\n  actionDiv: {\n    textAlign: \"center\"\n  }\n})\n\n\nconst cluster = `\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n%matplotlib inline\ndef gradient_descent(x,y):\n    m_curr = b_curr = 0\n    rate = 0.01\n    n = len(x)\n    plt.scatter(x,y,color='red',marker='+',linewidth='5')\n    for i in range(10000):\n        y_predicted = m_curr * x + b_curr\n        print (m_curr,b_curr, i)\n        plt.plot(x,y_predicted,color='green')\n        md = -(2/n)*sum(x*(y-y_predicted))\n        yd = -(2/n)*sum(y-y_predicted)\n        m_curr = m_curr - rate * md\n        b_curr = b_curr - rate * yd\n        \n        \nx = np.array([1,2,3,4,5])\ny = np.array([5,7,9,11,13])\n\ngradient_descent(x,y) \n`.trim();\n\nconst gradientPy = `\nimport numpy as np\n\ndef gradient_descent(x,y):\n    m=b = 0\n    iterations = 1000\n    n = len(x)\n    learning_rate = 0.08\n\n    for i in range(iterations):\n        y_predicted = m*x + b\n        cost = (1/n) * sum([val**2 for val in (y-y_predicted)])\n        md = -(2/n)*sum(x*(y-y_predicted))\n        bd = -(2/n)*sum(y-y_predicted)\n        m = m-learning_rate * md\n        b = b-learning_rate * bd\n        print (\"m {}, b {}, cost {} iteration {}\".format(m,b,cost, i))\n\nx = np.array([1,2,3,4,5])\ny = np.array([5,7,9,11,13])\n\ngradient_descent(x,y)`.trim();\n\nconst chains = `\ndl/dw = dl/de * de/dy * dy/dw\n`.trim();\n\nclass Gradient extends Component {\n  componentDidMount() {\n    setTimeout(() => Prism.highlightAll(), 0)\n  }\n  render() {\n    const { classes } = this.props;\n    return (\n      <Grid container>\n        <Grid item xs={2}>\n          <Paper className={classes.paper}>\n            <h4><Sidebar /></h4>\n          </Paper>\n        </Grid>\n        <Grid item xs={10}>\n          <Paper className={classes.paper}>\n            <List>\n              <h3>Gradient descent</h3>\n              Gradient descent is an optimization algorithm that's used when training a ML model. It's based on a\n              convex function and tweaks its parameters iteratively to minimize a given function to its local minimum.\n              <br />\n              <br />\n              <b>A gradient measures how much the output of a function changes if you change the inputs a little bit</b>\n              <br />\n\n              <h3>WHAT IS A GRADIENT?</h3>\n              A gradient is a derivative of a function that has more than one input variable.\n              Known as the slope of a function, the gradient simply measures the change in all weights w.r.t the change\n              in error.\n              <br />\n              <br />\n              <b>Importance of the Learning Rate:</b>\n              <ul>\n                <li>How big the steps are gradient descent takes into the direction of the local minimum are determined by\n                  the learning rate, which figures out how fast/ slow we will move towards the optimal weights.</li>\n                <li>To reach the local minimum we must set the learning rate to an appropriate value, which is neither\n                  too low nor too high. This is important because if the steps it takes are too big, it may not reach\n                  the local minimum because it bounces back and forth between the convex function of gradient descent.\n                  If we set the learning rate to a very small value, gradient descent will eventually reach the local minimum but\n                  that may take a while. </li>\n              </ul>\n              <br />\n\n              <h3>Types of Gradient Descent</h3>\n              Three types based on amount of data they use.\n              <ul>\n                <li><b>BATCH GRADIENT DESCENT(vanilla gradient descen): </b>Calculates the error.</li>\n                <ul>\n                  <li>Advantages of batch gradient descent are its computational efficient,it produces a\n                    stable error gradient and a stable convergence.</li>\n                  <li>we go through all training samples and calculate cumulative error.</li>\n                  <li>Now back propagate and adjust weights.</li>\n                  <li>Good for small training set.</li>\n                </ul>\n                <br />\n                <li><b>STOCHASTIC GRADIENT DESCENT: </b>It updates the parameters for each training example one by one.</li>\n                <ul>\n                  <li>Use one(randomly picked) sample for a forward pass and then adjust weights.</li>\n                  <li>Good when training set is very big and we don't want too much computation.</li>\n                </ul>\n                <br />\n                <li><b>MINI-BATCH GRADIENT DESCENT: </b>Combination of the concepts of SGD and batch gradient descent. </li>\n                <ul>\n                  <li>It simply splits the training dataset into small batches and performs an update for each of\n                    those batches. This creates a balance between the robustness of SGD and the\n                    efficiency of batch gradient descent.</li>\n                  <li>Is like SGD instead of choosing 1 randomly picked training sample, We will use a batch of randomly picked training samples.</li>\n                  <li>Use a batch of (randomly picked) samples for a forward pass and then adjust weights.</li>\n                  <li>Common mini-batch sizes range between 50 and 256.</li>\n                </ul>\n              </ul>\n              <br />\n\n              <img src={gradient} alt=\"gradient\" className=\"responsive\" style={redesign} />\n              <div style={titles}>\n                <PrismCode\n                  code={cluster}\n                  language=\"js\"\n                  plugins={[\"line-numbers\"]}\n                />\n              </div>\n              <br />\n\n              <h3>With Python</h3>\n              <div style={titles}>\n                <PrismCode\n                  code={gradientPy}\n                  language=\"js\"\n                  plugins={[\"line-numbers\"]}\n                />\n              </div>\n              <br />\n\n              <h3>Chain Rule</h3>\n              <div style={titles}>\n                <PrismCode\n                  code={chains}\n                  language=\"js\"\n                  plugins={[\"line-numbers\"]}\n                />\n              </div>\n            </List>\n          </Paper>\n        </Grid>\n      </Grid>\n    )\n  }\n}\n\nexport default (withStyles(styles)(Gradient));\n"]},"metadata":{},"sourceType":"module"}