{"ast":null,"code":"import _classCallCheck from\"/home/mukeshs/Projects/edurights/node_modules/@babel/runtime/helpers/esm/classCallCheck\";import _createClass from\"/home/mukeshs/Projects/edurights/node_modules/@babel/runtime/helpers/esm/createClass\";import _possibleConstructorReturn from\"/home/mukeshs/Projects/edurights/node_modules/@babel/runtime/helpers/esm/possibleConstructorReturn\";import _getPrototypeOf from\"/home/mukeshs/Projects/edurights/node_modules/@babel/runtime/helpers/esm/getPrototypeOf\";import _inherits from\"/home/mukeshs/Projects/edurights/node_modules/@babel/runtime/helpers/esm/inherits\";import React,{Component}from'react';import Prism from\"prismjs\";import{Grid,Paper,withStyles,List}from\"@material-ui/core\";import'../../ReactJs/styles.css';import Sidebar from'../sidebar';import PrismCode from'../../ReactJs/prismCode';import Activations from'../../../assets/AI/activatins.png';var titles={backgroundColor:'#F0F8FF',padding:'1px',fontSize:'16px'};var redesign={height:200,width:500};var styles=function styles(theme){return{paper:{margin:theme.spacing(1),padding:theme.spacing(1)},smMargin:{margin:theme.spacing(1)},actionDiv:{textAlign:\"center\"}};};var childsFile=\"\\ndef sigmoid(x):\\n  return 1 / (1 + math.exp(-x))\\n\\nsigmoid(100)  \\nsigmoid(1)\\nsigmoid(-56)\\nsigmoid(0.5)\\n\".trim();var tanh=\"\\ndef tanh(x):\\n  return (math.exp(x) - math.exp(-x)) / (math.exp(x) + math.exp(-x))\\n  \\ntanh(-56)\\ntanh(50)\\ntanh(1)\\n\".trim();var reLU=\"\\ndef relu(x):\\n    return max(0,x)\\n    \\nrelu(-100)\\nrelu(8)\\n\".trim();var leaky=\"\\ndef leaky_relu(x):\\n    return max(0.1*x,x)\\n    \\nleaky_relu(-100)\\nleaky_relu(8)\\n\".trim();var ActivationFuns=/*#__PURE__*/function(_Component){_inherits(ActivationFuns,_Component);function ActivationFuns(){_classCallCheck(this,ActivationFuns);return _possibleConstructorReturn(this,_getPrototypeOf(ActivationFuns).apply(this,arguments));}_createClass(ActivationFuns,[{key:\"componentDidMount\",value:function componentDidMount(){setTimeout(function(){return Prism.highlightAll();},0);}},{key:\"render\",value:function render(){var classes=this.props.classes;return React.createElement(Grid,{container:true},React.createElement(Grid,{item:true,xs:2},React.createElement(Paper,{className:classes.paper},React.createElement(\"h4\",null,React.createElement(Sidebar,null)))),React.createElement(Grid,{item:true,xs:10},React.createElement(Paper,{className:classes.paper},React.createElement(List,null,React.createElement(\"h3\",null,\"Neural networks\"),React.createElement(\"ul\",null,React.createElement(\"li\",null,\"A neural network is a very powerful ML mechanism which basically mimics how a human brain learns.\"),React.createElement(\"li\",null,\"The brain receives the stimulus from the outside world, does the processing on the i/p, and then generates the o/p. As the task gets complicated, multiple neurons form a complex network, passing information among themselves.\"),React.createElement(\"li\",null,\"Each neuron is characterized by its weight, bias and activation function.\")),React.createElement(\"br\",null),React.createElement(\"h3\",null,\"Popular types of activation functions\"),React.createElement(\"ul\",null,React.createElement(\"li\",null,React.createElement(\"b\",null,\"1.Binary Step Activation Function\")),React.createElement(\"ul\",null,React.createElement(\"li\",null,\"Returns value either 0 or 1.\"),React.createElement(\"li\",null,\"Returns 0 if i/p is the less then zero.\"),React.createElement(\"li\",null,\"Returns 1 if the i/p is greater than zero.\")),React.createElement(\"br\",null),React.createElement(\"li\",null,React.createElement(\"b\",null,\"2.Linear Activation Function\")),React.createElement(\"ul\",null,React.createElement(\"li\",null,\"Returns what it gets as i/p.\")),React.createElement(\"br\",null),React.createElement(\"li\",null,React.createElement(\"b\",null,\"3.Sigmoid Activation Function\")),React.createElement(\"ul\",null,React.createElement(\"li\",null,\"It returns the value beteen 0 and 1.\"),React.createElement(\"li\",null,\"For activation function in deep learning network, Sigmoid function is considered not good since near the boundaries the network doesn't learn quickly. Because gradient is almost zero near the boundaries.\")),React.createElement(\"br\",null),React.createElement(\"li\",null,React.createElement(\"b\",null,\"4.Tanh Activation Function\")),React.createElement(\"ul\",null,React.createElement(\"li\",null,\"Tanh is nonlinear activation function. Tanh o/p between -1 and 1. \"),React.createElement(\"li\",null,\"It also suffers from gradient problem near the boundaries\")),React.createElement(\"br\",null),React.createElement(\"li\",null,React.createElement(\"b\",null,\"5.RELU Activation Function\")),React.createElement(\"ul\",null,React.createElement(\"li\",null,\"RELU is less computational expensive than the other non linear activation functions.\"),React.createElement(\"li\",null,\"Returns 0 if i/p is less than 0.\"),React.createElement(\"li\",null,\"Returns x if i/p is greater than 0.\")),React.createElement(\"br\",null),React.createElement(\"li\",null,React.createElement(\"b\",null,\"6.Softmax Activation Function\")),React.createElement(\"ul\",null,React.createElement(\"li\",null,\"Softmax turns logits, the numeric o/p of the last linear layer of a multi-class classification neural network into probabilities.\"))),React.createElement(\"br\",null),React.createElement(\"h3\",null,\"Implementation of activation functions in python.\"),React.createElement(\"img\",{src:Activations,alt:\"Theata\",className:\"responsive2\",style:redesign}),React.createElement(\"br\",null),React.createElement(\"br\",null),React.createElement(\"b\",null,\"Sigmoid\"),React.createElement(\"div\",{style:titles},React.createElement(PrismCode,{code:childsFile,language:\"js\",plugins:[\"line-numbers\"]})),React.createElement(\"br\",null),React.createElement(\"h3\",null,\"Tanh\"),React.createElement(\"div\",{style:titles},React.createElement(PrismCode,{code:tanh,language:\"js\",plugins:[\"line-numbers\"]})),React.createElement(\"br\",null),React.createElement(\"h3\",null,\"ReLU\"),React.createElement(\"div\",{style:titles},React.createElement(PrismCode,{code:reLU,language:\"js\",plugins:[\"line-numbers\"]})),React.createElement(\"br\",null),React.createElement(\"h3\",null,\"Leaky ReLU\"),React.createElement(\"div\",{style:titles},React.createElement(PrismCode,{code:leaky,language:\"js\",plugins:[\"line-numbers\"]})),React.createElement(\"br\",null),React.createElement(\"h3\",null),React.createElement(\"div\",{style:titles},React.createElement(PrismCode,{code:childsFile,language:\"js\",plugins:[\"line-numbers\"]}))))));}}]);return ActivationFuns;}(Component);export default withStyles(styles)(ActivationFuns);","map":{"version":3,"sources":["/home/mukeshs/Projects/edurights/src/components/angularjs/deepAngularjs/activationFunctions.js"],"names":["React","Component","Prism","Grid","Paper","withStyles","List","Sidebar","PrismCode","Activations","titles","backgroundColor","padding","fontSize","redesign","height","width","styles","theme","paper","margin","spacing","smMargin","actionDiv","textAlign","childsFile","trim","tanh","reLU","leaky","ActivationFuns","setTimeout","highlightAll","classes","props"],"mappings":"6kBAAA,MAAOA,CAAAA,KAAP,EAAgBC,SAAhB,KAAiC,OAAjC,CACA,MAAOC,CAAAA,KAAP,KAAkB,SAAlB,CACA,OAASC,IAAT,CAAeC,KAAf,CAAsBC,UAAtB,CAAkCC,IAAlC,KAA8C,mBAA9C,CAEA,MAAO,0BAAP,CACA,MAAOC,CAAAA,OAAP,KAAoB,YAApB,CACA,MAAOC,CAAAA,SAAP,KAAsB,yBAAtB,CAEA,MAAOC,CAAAA,WAAP,KAAwB,mCAAxB,CAGA,GAAMC,CAAAA,MAAM,CAAG,CAAEC,eAAe,CAAE,SAAnB,CAA8BC,OAAO,CAAE,KAAvC,CAA8CC,QAAQ,CAAE,MAAxD,CAAf,CAEA,GAAMC,CAAAA,QAAQ,CAAG,CACfC,MAAM,CAAE,GADO,CAEfC,KAAK,CAAE,GAFQ,CAAjB,CAKA,GAAMC,CAAAA,MAAM,CAAG,QAATA,CAAAA,MAAS,CAAAC,KAAK,QAAK,CACvBC,KAAK,CAAE,CACLC,MAAM,CAAEF,KAAK,CAACG,OAAN,CAAc,CAAd,CADH,CAELT,OAAO,CAAEM,KAAK,CAACG,OAAN,CAAc,CAAd,CAFJ,CADgB,CAKvBC,QAAQ,CAAE,CACRF,MAAM,CAAEF,KAAK,CAACG,OAAN,CAAc,CAAd,CADA,CALa,CAQvBE,SAAS,CAAE,CACTC,SAAS,CAAE,QADF,CARY,CAAL,EAApB,CAcA,GAAMC,CAAAA,UAAU,CAAG,iHAQjBC,IARiB,EAAnB,CAUA,GAAMC,CAAAA,IAAI,CAAG,2HAOXD,IAPW,EAAb,CASA,GAAME,CAAAA,IAAI,CAAG,mEAMXF,IANW,EAAb,CAQA,GAAMG,CAAAA,KAAK,CAAG,yFAMZH,IANY,EAAd,C,GASMI,CAAAA,c,8TACgB,CAClBC,UAAU,CAAC,iBAAM7B,CAAAA,KAAK,CAAC8B,YAAN,EAAN,EAAD,CAA6B,CAA7B,CAAV,CACD,C,uCACQ,IACCC,CAAAA,OADD,CACa,KAAKC,KADlB,CACCD,OADD,CAEP,MACE,qBAAC,IAAD,EAAM,SAAS,KAAf,EACE,oBAAC,IAAD,EAAM,IAAI,KAAV,CAAW,EAAE,CAAE,CAAf,EACE,oBAAC,KAAD,EAAO,SAAS,CAAEA,OAAO,CAACd,KAA1B,EACE,8BAAI,oBAAC,OAAD,MAAJ,CADF,CADF,CADF,CAME,oBAAC,IAAD,EAAM,IAAI,KAAV,CAAW,EAAE,CAAE,EAAf,EACE,oBAAC,KAAD,EAAO,SAAS,CAAEc,OAAO,CAACd,KAA1B,EACE,oBAAC,IAAD,MACE,gDADF,CAEE,8BACE,kIADF,CAEE,iQAFF,CAIE,0GAJF,CAFF,CAQE,8BARF,CAUE,sEAVF,CAWE,8BACE,8BAAI,iEAAJ,CADF,CAEE,8BACE,6DADF,CAEE,wEAFF,CAGE,2EAHF,CAFF,CAOE,8BAPF,CASE,8BAAI,4DAAJ,CATF,CAUE,8BACE,6DADF,CAVF,CAaE,8BAbF,CAeE,8BAAI,6DAAJ,CAfF,CAgBE,8BACE,qEADF,CAEE,4OAFF,CAhBF,CAqBE,8BArBF,CAuBE,8BAAI,0DAAJ,CAvBF,CAwBE,8BACE,mGADF,CAEE,0FAFF,CAxBF,CA4BE,8BA5BF,CA8BE,8BAAI,0DAAJ,CA9BF,CA+BE,8BACE,qHADF,CAEE,iEAFF,CAGE,oEAHF,CA/BF,CAoCE,8BApCF,CAsCE,8BAAI,6DAAJ,CAtCF,CAuCE,8BACE,kKADF,CAvCF,CAXF,CAsDE,8BAtDF,CAwDE,kFAxDF,CAyDE,2BAAK,GAAG,CAAEV,WAAV,CAAuB,GAAG,CAAC,QAA3B,CAAoC,SAAS,CAAC,aAA9C,CAA4D,KAAK,CAAEK,QAAnE,EAzDF,CA0DE,8BA1DF,CA2DE,8BA3DF,CA4DE,uCA5DF,CA6DE,2BAAK,KAAK,CAAEJ,MAAZ,EACE,oBAAC,SAAD,EACE,IAAI,CAAEe,UADR,CAEE,QAAQ,CAAC,IAFX,CAGE,OAAO,CAAE,CAAC,cAAD,CAHX,EADF,CA7DF,CAoEE,8BApEF,CAsEE,qCAtEF,CAuEE,2BAAK,KAAK,CAAEf,MAAZ,EACE,oBAAC,SAAD,EACE,IAAI,CAAEiB,IADR,CAEE,QAAQ,CAAC,IAFX,CAGE,OAAO,CAAE,CAAC,cAAD,CAHX,EADF,CAvEF,CA8EE,8BA9EF,CAgFE,qCAhFF,CAiFE,2BAAK,KAAK,CAAEjB,MAAZ,EACE,oBAAC,SAAD,EACE,IAAI,CAAEkB,IADR,CAEE,QAAQ,CAAC,IAFX,CAGE,OAAO,CAAE,CAAC,cAAD,CAHX,EADF,CAjFF,CAwFE,8BAxFF,CA0FE,2CA1FF,CA2FE,2BAAK,KAAK,CAAElB,MAAZ,EACE,oBAAC,SAAD,EACE,IAAI,CAAEmB,KADR,CAEE,QAAQ,CAAC,IAFX,CAGE,OAAO,CAAE,CAAC,cAAD,CAHX,EADF,CA3FF,CAkGE,8BAlGF,CAoGE,8BApGF,CAqGE,2BAAK,KAAK,CAAEnB,MAAZ,EACE,oBAAC,SAAD,EACE,IAAI,CAAEe,UADR,CAEE,QAAQ,CAAC,IAFX,CAGE,OAAO,CAAE,CAAC,cAAD,CAHX,EADF,CArGF,CADF,CADF,CANF,CADF,CA0HD,C,4BAhI0BxB,S,EAoI7B,cAAgBI,CAAAA,UAAU,CAACY,MAAD,CAAV,CAAmBa,cAAnB,CAAhB","sourcesContent":["import React, { Component } from 'react';\nimport Prism from \"prismjs\"\nimport { Grid, Paper, withStyles, List } from \"@material-ui/core\";\n\nimport '../../ReactJs/styles.css'\nimport Sidebar from '../sidebar';\nimport PrismCode from '../../ReactJs/prismCode';\n\nimport Activations from '../../../assets/AI/activatins.png'\n\n\nconst titles = { backgroundColor: '#F0F8FF', padding: '1px', fontSize: '16px' }\n\nconst redesign = {\n  height: 200,\n  width: 500\n}\n\nconst styles = theme => ({\n  paper: {\n    margin: theme.spacing(1),\n    padding: theme.spacing(1)\n  },\n  smMargin: {\n    margin: theme.spacing(1)\n  },\n  actionDiv: {\n    textAlign: \"center\"\n  }\n})\n\n\nconst childsFile = `\ndef sigmoid(x):\n  return 1 / (1 + math.exp(-x))\n\nsigmoid(100)  \nsigmoid(1)\nsigmoid(-56)\nsigmoid(0.5)\n`.trim();\n\nconst tanh = `\ndef tanh(x):\n  return (math.exp(x) - math.exp(-x)) / (math.exp(x) + math.exp(-x))\n  \ntanh(-56)\ntanh(50)\ntanh(1)\n`.trim();\n\nconst reLU = `\ndef relu(x):\n    return max(0,x)\n    \nrelu(-100)\nrelu(8)\n`.trim();\n\nconst leaky = `\ndef leaky_relu(x):\n    return max(0.1*x,x)\n    \nleaky_relu(-100)\nleaky_relu(8)\n`.trim();\n\n\nclass ActivationFuns extends Component {\n  componentDidMount() {\n    setTimeout(() => Prism.highlightAll(), 0)\n  }\n  render() {\n    const { classes } = this.props;\n    return (\n      <Grid container>\n        <Grid item xs={2}>\n          <Paper className={classes.paper}>\n            <h4><Sidebar /></h4>\n          </Paper>\n        </Grid>\n        <Grid item xs={10}>\n          <Paper className={classes.paper}>\n            <List>\n              <h3>Neural networks</h3>\n              <ul>\n                <li>A neural network is a very powerful ML mechanism which basically mimics how a human brain learns.</li>\n                <li>The brain receives the stimulus from the outside world, does the processing on the i/p, and then generates the o/p. As\n                  the task gets complicated, multiple neurons form a complex network, passing information among themselves.</li>\n                <li>Each neuron is characterized by its weight, bias and activation function.</li>\n              </ul>\n              <br />\n\n              <h3>Popular types of activation functions</h3>\n              <ul>\n                <li><b>1.Binary Step Activation Function</b></li>\n                <ul>\n                  <li>Returns value either 0 or 1.</li>\n                  <li>Returns 0 if i/p is the less then zero.</li>\n                  <li>Returns 1 if the i/p is greater than zero.</li>\n                </ul>\n                <br />\n\n                <li><b>2.Linear Activation Function</b></li>\n                <ul>\n                  <li>Returns what it gets as i/p.</li>\n                </ul>\n                <br />\n\n                <li><b>3.Sigmoid Activation Function</b></li>\n                <ul>\n                  <li>It returns the value beteen 0 and 1.</li>\n                  <li>For activation function in deep learning network, Sigmoid function is considered not good since near the boundaries the\n                    network doesn't learn quickly. Because gradient is almost zero near the boundaries.</li>\n                </ul>\n                <br />\n\n                <li><b>4.Tanh Activation Function</b></li>\n                <ul>\n                  <li>Tanh is nonlinear activation function. Tanh o/p between -1 and 1. </li>\n                  <li>It also suffers from gradient problem near the boundaries</li>\n                </ul>\n                <br />\n\n                <li><b>5.RELU Activation Function</b></li>\n                <ul>\n                  <li>RELU is less computational expensive than the other non linear activation functions.</li>\n                  <li>Returns 0 if i/p is less than 0.</li>\n                  <li>Returns x if i/p is greater than 0.</li>\n                </ul>\n                <br />\n\n                <li><b>6.Softmax Activation Function</b></li>\n                <ul>\n                  <li>Softmax turns logits, the numeric o/p of the last linear layer of a multi-class classification neural network into probabilities.</li>\n                </ul>\n              </ul>\n              <br />\n\n              <h3>Implementation of activation functions in python.</h3>\n              <img src={Activations} alt=\"Theata\" className=\"responsive2\" style={redesign} />\n              <br />\n              <br />\n              <b>Sigmoid</b>\n              <div style={titles}>\n                <PrismCode\n                  code={childsFile}\n                  language=\"js\"\n                  plugins={[\"line-numbers\"]}\n                />\n              </div>\n              <br />\n\n              <h3>Tanh</h3>\n              <div style={titles}>\n                <PrismCode\n                  code={tanh}\n                  language=\"js\"\n                  plugins={[\"line-numbers\"]}\n                />\n              </div>\n              <br />\n\n              <h3>ReLU</h3>\n              <div style={titles}>\n                <PrismCode\n                  code={reLU}\n                  language=\"js\"\n                  plugins={[\"line-numbers\"]}\n                />\n              </div>\n              <br />\n\n              <h3>Leaky ReLU</h3>\n              <div style={titles}>\n                <PrismCode\n                  code={leaky}\n                  language=\"js\"\n                  plugins={[\"line-numbers\"]}\n                />\n              </div>\n              <br />\n\n              <h3></h3>\n              <div style={titles}>\n                <PrismCode\n                  code={childsFile}\n                  language=\"js\"\n                  plugins={[\"line-numbers\"]}\n                />\n              </div>\n            </List>\n          </Paper>\n        </Grid>\n      </Grid>\n    )\n  }\n}\n\n\nexport default (withStyles(styles)(ActivationFuns));\n"]},"metadata":{},"sourceType":"module"}