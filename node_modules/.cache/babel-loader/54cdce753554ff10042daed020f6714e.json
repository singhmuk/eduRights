{"ast":null,"code":"import _classCallCheck from\"/home/mukeshs/Projects/edurights/node_modules/@babel/runtime/helpers/esm/classCallCheck\";import _createClass from\"/home/mukeshs/Projects/edurights/node_modules/@babel/runtime/helpers/esm/createClass\";import _possibleConstructorReturn from\"/home/mukeshs/Projects/edurights/node_modules/@babel/runtime/helpers/esm/possibleConstructorReturn\";import _getPrototypeOf from\"/home/mukeshs/Projects/edurights/node_modules/@babel/runtime/helpers/esm/getPrototypeOf\";import _inherits from\"/home/mukeshs/Projects/edurights/node_modules/@babel/runtime/helpers/esm/inherits\";import React,{Component}from'react';import Prism from\"prismjs\";import{Grid,Paper,withStyles,List}from\"@material-ui/core\";import'../../ReactJs/styles.css';import Sidebar from'../sidebar';import PrismCode from'../../ReactJs/prismCode';var titles={backgroundColor:'#F0F8FF',padding:'1px',fontSize:'16px'};var styles=function styles(theme){return{paper:{margin:theme.spacing(1),padding:theme.spacing(1)},smMargin:{margin:theme.spacing(1)},actionDiv:{textAlign:\"center\"}};};var cluster=\"\\nfrom sklearn import svm, datasets\\niris = datasets.load_iris()\\n\\nimport pandas as pd\\ndf = pd.DataFrame(iris.data,columns=iris.feature_names)\\ndf['flower'] = iris.target\\ndf['flower'] = df['flower'].apply(lambda x: iris.target_names[x])\\ndf[47:150]\\n\".trim();var exactly=\"\\nfrom sklearn.model_selection import GridSearchCV\\n\\nclf = GridSearchCV(svm.SVC(gamma='auto'), {'C': [1,10,20], 'kernel': ['rbf','linear']}, cv=5, return_train_score=False)\\nclf.fit(iris.data, iris.target)\\nclf.cv_results_\\n\\ndf = pd.DataFrame(clf.cv_results_)\\ndf[['param_C','param_kernel','mean_test_score']]\\n\\nclf.best_params_\\nclf.best_score_\\n\\ndir(clf)\\n\".trim();var reduceNum=\"\\nfrom sklearn.model_selection import RandomizedSearchCV\\nfrom sklearn import svm\\nfrom sklearn.ensemble import RandomForestClassifier\\nfrom sklearn.linear_model import LogisticRegression\\n\\nrs = RandomizedSearchCV(svm.SVC(gamma='auto'), {'C': [1,10,20],'kernel': ['rbf','linear']}, \\n    cv=5, return_train_score=False, n_iter=2)\\n    \\nrs.fit(iris.data, iris.target)\\npd.DataFrame(rs.cv_results_)[['param_C','param_kernel','mean_test_score']]\\n\\n\\n#How about different models with different hyperparameters?\\n\\nmodel_params = {\\n    'svm': {\\n        'model': svm.SVC(gamma='auto'),\\n        'params' : {'C': [1,10,20],'kernel': ['rbf','linear']}  \\n    },\\n    'random_forest': {\\n        'model': RandomForestClassifier(),\\n        'params' : {'n_estimators': [1,5,10]}\\n    },\\n    'logistic_regression' : {\\n        'model': LogisticRegression(solver='liblinear',multi_class='auto'),\\n        'params': {'C': [1,5,10]}\\n    }\\n}\\n\\n\\nscores = []\\n\\nfor model_name, mp in model_params.items():\\n    clf =  GridSearchCV(mp['model'], mp['params'], cv=5, return_train_score=False)\\n    clf.fit(iris.data, iris.target)\\n    scores.append({'model': model_name,'best_score': clf.best_score_,'best_params': clf.best_params_})\\n    \\ndf = pd.DataFrame(scores,columns=['model','best_score','best_params'])\\ndf\\n\".trim();var approachOne=\"\\nfrom sklearn.model_selection import train_test_split\\nX_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.3)\\n\\nmodel = svm.SVC(kernel='rbf',C=30,gamma='auto')\\nmodel.fit(X_train,y_train)\\nmodel.score(X_test, y_test)\\n\".trim();var parameters=\"\\ncross_val_score(svm.SVC(kernel='linear',C=10,gamma='auto'),iris.data, iris.target, cv=5)\\ncross_val_score(svm.SVC(kernel='rbf',C=10,gamma='auto'),iris.data, iris.target, cv=5)\\ncross_val_score(svm.SVC(kernel='rbf',C=20,gamma='auto'),iris.data, iris.target, cv=5)\\n\\n\\n#Above approach is tiresome and very manual. We can use for loop as an alternative.\\nkernels = ['rbf', 'linear']\\nC = [1,10,20]\\navg_scores = {}\\nfor kval in kernels:\\n    for cval in C:\\n        cv_scores = cross_val_score(svm.SVC(kernel=kval,C=cval,gamma='auto'),iris.data, iris.target, cv=5)\\n        avg_scores[kval + '_' + str(cval)] = np.average(cv_scores)\\n\\navg_scores\\n\\n\".trim();var GreedSearch=/*#__PURE__*/function(_Component){_inherits(GreedSearch,_Component);function GreedSearch(){_classCallCheck(this,GreedSearch);return _possibleConstructorReturn(this,_getPrototypeOf(GreedSearch).apply(this,arguments));}_createClass(GreedSearch,[{key:\"componentDidMount\",value:function componentDidMount(){setTimeout(function(){return Prism.highlightAll();},0);}},{key:\"render\",value:function render(){var classes=this.props.classes;return React.createElement(Grid,{container:true},React.createElement(Grid,{item:true,xs:2},React.createElement(Paper,{className:classes.paper},React.createElement(\"h4\",null,React.createElement(Sidebar,null)))),React.createElement(Grid,{item:true,xs:10},React.createElement(Paper,{className:classes.paper},React.createElement(List,null,React.createElement(\"h3\",null,\"Greedy Algorithms\"),\"Greedy algorithms aim to make the optimal choice at given moment. Each step it chooses the optimal choice, without knowing the future. It attempts to find the globally optimal way to solve the entire problem using this method.\",React.createElement(\"br\",null),React.createElement(\"br\",null),React.createElement(\"ul\",null,React.createElement(\"li\",null,\"Check which Model is best fit for given problem. So it's a Model selection technique. Also check which parameter good for model.\"),React.createElement(\"li\",null,\"Greedy algorithms are greedy. They do not look into the future to decide the global optimal solution. They are only concerned with the optimal solution locally. This means that the overall optimal solution may differ from the solution the algorithm chooses.\"),React.createElement(\"li\",null,\"Greedy algorithms don\\u2019t guarantee solutions, but are very time efficient.\"),React.createElement(\"li\",null,\"Greedy algorithms are quick than \",React.createElement(\"b\",null,\"Divide & Conquer and Dynamic Programming\"),\".\")),React.createElement(\"br\",null),React.createElement(\"h3\",null,\"Finding best model and hyper parameter tunning using GridSearchCV\"),\"For iris flower dataset in sklearn library, we are going to find out best model and best hyper parameters using GridSearchCV.\",React.createElement(\"div\",{style:titles},React.createElement(PrismCode,{code:cluster,language:\"js\",plugins:[\"line-numbers\"]})),React.createElement(\"br\",null),React.createElement(\"h3\",null,\"Approach 1\"),\"Use train_test_split and manually tune parameters by trial and error.\",React.createElement(\"div\",{style:titles},React.createElement(PrismCode,{code:approachOne,language:\"js\",plugins:[\"line-numbers\"]})),React.createElement(\"br\",null),React.createElement(\"h3\",null,\"Approach 2: Use K Fold Cross validation.\"),\"Manually try suppling models with different parameters to cross_val_score function with 5 fold cross validation.\",React.createElement(\"div\",{style:titles},React.createElement(PrismCode,{code:parameters,language:\"js\",plugins:[\"line-numbers\"]})),React.createElement(\"br\",null),React.createElement(\"h3\",null,\"Approach 3: Use GridSearchCV\"),\"GridSearchCV does exactly same thing as for loop above but in a single line of code.\",React.createElement(\"div\",{style:titles},React.createElement(PrismCode,{code:exactly,language:\"js\",plugins:[\"line-numbers\"]})),React.createElement(\"br\",null),React.createElement(\"h3\",null,\"Use RandomizedSearchCV to reduce number of iterations and with random combination of parameters. This is useful when you have too many parameters to try and your training time is longer. It helps reduce the cost of computation\"),React.createElement(\"div\",{style:titles},React.createElement(PrismCode,{code:reduceNum,language:\"js\",plugins:[\"line-numbers\"]}))))));}}]);return GreedSearch;}(Component);export default withStyles(styles)(GreedSearch);","map":{"version":3,"sources":["/home/mukeshs/Projects/edurights/src/components/ml/deepMl/greedSearch.js"],"names":["React","Component","Prism","Grid","Paper","withStyles","List","Sidebar","PrismCode","titles","backgroundColor","padding","fontSize","styles","theme","paper","margin","spacing","smMargin","actionDiv","textAlign","cluster","trim","exactly","reduceNum","approachOne","parameters","GreedSearch","setTimeout","highlightAll","classes","props"],"mappings":"6kBAAA,MAAOA,CAAAA,KAAP,EAAgBC,SAAhB,KAAiC,OAAjC,CACA,MAAOC,CAAAA,KAAP,KAAkB,SAAlB,CACA,OAASC,IAAT,CAAeC,KAAf,CAAsBC,UAAtB,CAAkCC,IAAlC,KAA8C,mBAA9C,CAEA,MAAO,0BAAP,CACA,MAAOC,CAAAA,OAAP,KAAoB,YAApB,CACA,MAAOC,CAAAA,SAAP,KAAsB,yBAAtB,CAGA,GAAMC,CAAAA,MAAM,CAAG,CAAEC,eAAe,CAAE,SAAnB,CAA8BC,OAAO,CAAE,KAAvC,CAA8CC,QAAQ,CAAE,MAAxD,CAAf,CAEA,GAAMC,CAAAA,MAAM,CAAG,QAATA,CAAAA,MAAS,CAAAC,KAAK,QAAK,CACvBC,KAAK,CAAE,CACLC,MAAM,CAAEF,KAAK,CAACG,OAAN,CAAc,CAAd,CADH,CAELN,OAAO,CAAEG,KAAK,CAACG,OAAN,CAAc,CAAd,CAFJ,CADgB,CAKvBC,QAAQ,CAAE,CACRF,MAAM,CAAEF,KAAK,CAACG,OAAN,CAAc,CAAd,CADA,CALa,CAQvBE,SAAS,CAAE,CACTC,SAAS,CAAE,QADF,CARY,CAAL,EAApB,CAcA,GAAMC,CAAAA,OAAO,CAAG,gQASdC,IATc,EAAhB,CAWA,GAAMC,CAAAA,OAAO,CAAG,6WAcdD,IAdc,EAAhB,CAgBA,GAAME,CAAAA,SAAS,CAAG,8xCAwChBF,IAxCgB,EAAlB,CA0CA,GAAMG,CAAAA,WAAW,CAAG,mQAOlBH,IAPkB,EAApB,CASA,GAAMI,CAAAA,UAAU,CAAG,6oBAiBjBJ,IAjBiB,EAAnB,C,GAoBMK,CAAAA,W,+SACgB,CAClBC,UAAU,CAAC,iBAAM1B,CAAAA,KAAK,CAAC2B,YAAN,EAAN,EAAD,CAA6B,CAA7B,CAAV,CACD,C,uCACQ,IACCC,CAAAA,OADD,CACa,KAAKC,KADlB,CACCD,OADD,CAEP,MACE,qBAAC,IAAD,EAAM,SAAS,KAAf,EACE,oBAAC,IAAD,EAAM,IAAI,KAAV,CAAW,EAAE,CAAE,CAAf,EACE,oBAAC,KAAD,EAAO,SAAS,CAAEA,OAAO,CAACf,KAA1B,EACE,8BAAI,oBAAC,OAAD,MAAJ,CADF,CADF,CADF,CAME,oBAAC,IAAD,EAAM,IAAI,KAAV,CAAW,EAAE,CAAE,EAAf,EACE,oBAAC,KAAD,EAAO,SAAS,CAAEe,OAAO,CAACf,KAA1B,EACE,oBAAC,IAAD,MACE,kDADF,sOAIE,8BAJF,CAKE,8BALF,CAME,8BACE,iKADF,CAEE,kSAFF,CAKE,+GALF,CAME,kEAAqC,wEAArC,KANF,CANF,CAcE,8BAdF,CAgBE,kGAhBF,iIAkBE,2BAAK,KAAK,CAAEN,MAAZ,EACE,oBAAC,SAAD,EACE,IAAI,CAAEY,OADR,CAEE,QAAQ,CAAC,IAFX,CAGE,OAAO,CAAE,CAAC,cAAD,CAHX,EADF,CAlBF,CAyBE,8BAzBF,CA2BE,2CA3BF,yEA6BE,2BAAK,KAAK,CAAEZ,MAAZ,EACE,oBAAC,SAAD,EACE,IAAI,CAAEgB,WADR,CAEE,QAAQ,CAAC,IAFX,CAGE,OAAO,CAAE,CAAC,cAAD,CAHX,EADF,CA7BF,CAoCE,8BApCF,CAsCE,yEAtCF,oHAwCE,2BAAK,KAAK,CAAEhB,MAAZ,EACE,oBAAC,SAAD,EACE,IAAI,CAAEiB,UADR,CAEE,QAAQ,CAAC,IAFX,CAGE,OAAO,CAAE,CAAC,cAAD,CAHX,EADF,CAxCF,CA+CE,8BA/CF,CAiDE,6DAjDF,wFAmDE,2BAAK,KAAK,CAAEjB,MAAZ,EACE,oBAAC,SAAD,EACE,IAAI,CAAEc,OADR,CAEE,QAAQ,CAAC,IAFX,CAGE,OAAO,CAAE,CAAC,cAAD,CAHX,EADF,CAnDF,CA0DE,8BA1DF,CA4DE,mQA5DF,CA6DE,2BAAK,KAAK,CAAEd,MAAZ,EACE,oBAAC,SAAD,EACE,IAAI,CAAEe,SADR,CAEE,QAAQ,CAAC,IAFX,CAGE,OAAO,CAAE,CAAC,cAAD,CAHX,EADF,CA7DF,CADF,CADF,CANF,CADF,CA4FD,C,yBAlGuBvB,S,EAqG1B,cAAgBI,CAAAA,UAAU,CAACQ,MAAD,CAAV,CAAmBc,WAAnB,CAAhB","sourcesContent":["import React, { Component } from 'react';\nimport Prism from \"prismjs\"\nimport { Grid, Paper, withStyles, List } from \"@material-ui/core\";\n\nimport '../../ReactJs/styles.css'\nimport Sidebar from '../sidebar';\nimport PrismCode from '../../ReactJs/prismCode';\n\n\nconst titles = { backgroundColor: '#F0F8FF', padding: '1px', fontSize: '16px' }\n\nconst styles = theme => ({\n  paper: {\n    margin: theme.spacing(1),\n    padding: theme.spacing(1)\n  },\n  smMargin: {\n    margin: theme.spacing(1)\n  },\n  actionDiv: {\n    textAlign: \"center\"\n  }\n})\n\n\nconst cluster = `\nfrom sklearn import svm, datasets\niris = datasets.load_iris()\n\nimport pandas as pd\ndf = pd.DataFrame(iris.data,columns=iris.feature_names)\ndf['flower'] = iris.target\ndf['flower'] = df['flower'].apply(lambda x: iris.target_names[x])\ndf[47:150]\n`.trim();\n\nconst exactly = `\nfrom sklearn.model_selection import GridSearchCV\n\nclf = GridSearchCV(svm.SVC(gamma='auto'), {'C': [1,10,20], 'kernel': ['rbf','linear']}, cv=5, return_train_score=False)\nclf.fit(iris.data, iris.target)\nclf.cv_results_\n\ndf = pd.DataFrame(clf.cv_results_)\ndf[['param_C','param_kernel','mean_test_score']]\n\nclf.best_params_\nclf.best_score_\n\ndir(clf)\n`.trim();\n\nconst reduceNum = `\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn import svm\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\n\nrs = RandomizedSearchCV(svm.SVC(gamma='auto'), {'C': [1,10,20],'kernel': ['rbf','linear']}, \n    cv=5, return_train_score=False, n_iter=2)\n    \nrs.fit(iris.data, iris.target)\npd.DataFrame(rs.cv_results_)[['param_C','param_kernel','mean_test_score']]\n\n\n#How about different models with different hyperparameters?\n\nmodel_params = {\n    'svm': {\n        'model': svm.SVC(gamma='auto'),\n        'params' : {'C': [1,10,20],'kernel': ['rbf','linear']}  \n    },\n    'random_forest': {\n        'model': RandomForestClassifier(),\n        'params' : {'n_estimators': [1,5,10]}\n    },\n    'logistic_regression' : {\n        'model': LogisticRegression(solver='liblinear',multi_class='auto'),\n        'params': {'C': [1,5,10]}\n    }\n}\n\n\nscores = []\n\nfor model_name, mp in model_params.items():\n    clf =  GridSearchCV(mp['model'], mp['params'], cv=5, return_train_score=False)\n    clf.fit(iris.data, iris.target)\n    scores.append({'model': model_name,'best_score': clf.best_score_,'best_params': clf.best_params_})\n    \ndf = pd.DataFrame(scores,columns=['model','best_score','best_params'])\ndf\n`.trim();\n\nconst approachOne = `\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.3)\n\nmodel = svm.SVC(kernel='rbf',C=30,gamma='auto')\nmodel.fit(X_train,y_train)\nmodel.score(X_test, y_test)\n`.trim();\n\nconst parameters = `\ncross_val_score(svm.SVC(kernel='linear',C=10,gamma='auto'),iris.data, iris.target, cv=5)\ncross_val_score(svm.SVC(kernel='rbf',C=10,gamma='auto'),iris.data, iris.target, cv=5)\ncross_val_score(svm.SVC(kernel='rbf',C=20,gamma='auto'),iris.data, iris.target, cv=5)\n\n\n#Above approach is tiresome and very manual. We can use for loop as an alternative.\nkernels = ['rbf', 'linear']\nC = [1,10,20]\navg_scores = {}\nfor kval in kernels:\n    for cval in C:\n        cv_scores = cross_val_score(svm.SVC(kernel=kval,C=cval,gamma='auto'),iris.data, iris.target, cv=5)\n        avg_scores[kval + '_' + str(cval)] = np.average(cv_scores)\n\navg_scores\n\n`.trim();\n\n\nclass GreedSearch extends Component {\n  componentDidMount() {\n    setTimeout(() => Prism.highlightAll(), 0)\n  }\n  render() {\n    const { classes } = this.props;\n    return (\n      <Grid container>\n        <Grid item xs={2}>\n          <Paper className={classes.paper}>\n            <h4><Sidebar /></h4>\n          </Paper>\n        </Grid>\n        <Grid item xs={10}>\n          <Paper className={classes.paper}>\n            <List>\n              <h3>Greedy Algorithms</h3>\n              Greedy algorithms aim to make the optimal choice at given moment. Each step it chooses the optimal choice,\n              without knowing the future. It attempts to find the globally optimal way to solve the entire problem using this method.\n              <br />\n              <br />\n              <ul>\n                <li>Check which Model is best fit for given problem. So it's a Model selection technique. Also check which parameter good for model.</li>\n                <li>Greedy algorithms are greedy. They do not look into the future to decide the global optimal\n                  solution. They are only concerned with the optimal solution locally. This means that the overall\n                  optimal solution may differ from the solution the algorithm chooses.</li>\n                <li>Greedy algorithms don’t guarantee solutions, but are very time efficient.</li>\n                <li>Greedy algorithms are quick than <b>Divide & Conquer and Dynamic Programming</b>.</li>\n              </ul>\n              <br />\n\n              <h3>Finding best model and hyper parameter tunning using GridSearchCV</h3>\n              For iris flower dataset in sklearn library, we are going to find out best model and best hyper parameters using GridSearchCV.\n              <div style={titles}>\n                <PrismCode\n                  code={cluster}\n                  language=\"js\"\n                  plugins={[\"line-numbers\"]}\n                />\n              </div>\n              <br />\n\n              <h3>Approach 1</h3>\n              Use train_test_split and manually tune parameters by trial and error.\n              <div style={titles}>\n                <PrismCode\n                  code={approachOne}\n                  language=\"js\"\n                  plugins={[\"line-numbers\"]}\n                />\n              </div>\n              <br />\n\n              <h3>Approach 2: Use K Fold Cross validation.</h3>\n              Manually try suppling models with different parameters to cross_val_score function with 5 fold cross validation.\n              <div style={titles}>\n                <PrismCode\n                  code={parameters}\n                  language=\"js\"\n                  plugins={[\"line-numbers\"]}\n                />\n              </div>\n              <br />\n\n              <h3>Approach 3: Use GridSearchCV</h3>\n              GridSearchCV does exactly same thing as for loop above but in a single line of code.\n              <div style={titles}>\n                <PrismCode\n                  code={exactly}\n                  language=\"js\"\n                  plugins={[\"line-numbers\"]}\n                />\n              </div>\n              <br />\n\n              <h3>Use RandomizedSearchCV to reduce number of iterations and with random combination of parameters. This is useful when you have too many parameters to try and your training time is longer. It helps reduce the cost of computation</h3>\n              <div style={titles}>\n                <PrismCode\n                  code={reduceNum}\n                  language=\"js\"\n                  plugins={[\"line-numbers\"]}\n                />\n              </div>\n              {/* <br />\n\n              <h3></h3>\n              <div style={titles}>\n                <PrismCode\n                  code={stack}\n                  language=\"js\"\n                  plugins={[\"line-numbers\"]}\n                />\n              </div> */}\n            </List>\n          </Paper>\n        </Grid>\n      </Grid>\n    )\n  }\n}\n\nexport default (withStyles(styles)(GreedSearch));\n"]},"metadata":{},"sourceType":"module"}