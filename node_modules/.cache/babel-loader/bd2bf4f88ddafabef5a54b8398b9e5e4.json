{"ast":null,"code":"import _classCallCheck from\"/home/mukeshs/Projects/edurights/node_modules/@babel/runtime/helpers/esm/classCallCheck\";import _createClass from\"/home/mukeshs/Projects/edurights/node_modules/@babel/runtime/helpers/esm/createClass\";import _possibleConstructorReturn from\"/home/mukeshs/Projects/edurights/node_modules/@babel/runtime/helpers/esm/possibleConstructorReturn\";import _getPrototypeOf from\"/home/mukeshs/Projects/edurights/node_modules/@babel/runtime/helpers/esm/getPrototypeOf\";import _inherits from\"/home/mukeshs/Projects/edurights/node_modules/@babel/runtime/helpers/esm/inherits\";import React,{Component}from'react';import Prism from\"prismjs\";import{Grid,Paper,withStyles,List}from\"@material-ui/core\";import'../../ReactJs/styles.css';import Sidebar from'../sidebar';import PrismCode from'../../ReactJs/prismCode';import Sigmoid from'../../../assets/ML/knn2.png';import Knn3 from'../../../assets/ML/knn3.png';import Knn4 from'../../../assets/ML/knn4.png';import Knn5 from'../../../assets/ML/knn5.png';var titles={backgroundColor:'#F0F8FF',padding:'1px',fontSize:'16px'};var redesign={height:350,width:600};var styles=function styles(theme){return{paper:{margin:theme.spacing(1),padding:theme.spacing(1)},smMargin:{margin:theme.spacing(1)},actionDiv:{textAlign:\"center\"}};};var kFold=\"\\nimport numpy as np\\nfrom sklearn.datasets import load_digits\\nimport matplotlib.pyplot as plt\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.svm import SVC\\nfrom sklearn.ensemble import RandomForestClassifier\\n\\ndigits = load_digits()\\n\\nX_train, X_test, y_train, y_test = train_test_split(digits.data,digits.target,test_size=0.3)\\nlr = LogisticRegression(solver='liblinear',multi_class='ovr')                                     #Logistic Regression\\nlr.fit(X_train, y_train)\\nlr.score(X_test, y_test)\\n\\nsvm = SVC(gamma='auto')                                                                           #SVM\\nsvm.fit(X_train, y_train)\\nsvm.score(X_test, y_test)\\n\\n\\nrf = RandomForestClassifier(n_estimators=40)                                                      #Random Forest\\nrf.fit(X_train, y_train)\\nrf.score(X_test, y_test)\\n\".trim();var kFoldVal=\"\\nfrom sklearn.model_selection import KFold\\nkf = KFold(n_splits=3)\\n\\nfor train_index, test_index in kf.split([1,2,3,4,5,6,7,8,9]):\\n    print(train_index, test_index)\\n    \\ndef get_score(model, X_train, X_test, y_train, y_test):\\n    model.fit(X_train, y_train)\\n    return model.score(X_test, y_test)\\n    \\n    from sklearn.model_selection import StratifiedKFold\\nfolds = StratifiedKFold(n_splits=3)\\n\\nscores_logistic = []\\nscores_svm = []\\nscores_rf = []\\n\\nfor train_index, test_index in folds.split(digits.data,digits.target):\\n    X_train, X_test, y_train, y_test = digits.data[train_index], digits.data[test_index],     \\n                                       digits.target[train_index], digits.target[test_index]\\n                                       \\n    scores_logistic.append(get_score(LogisticRegression(solver='liblinear',multi_class='ovr'), \\n                                      X_train, X_test, y_train, y_test))  \\n                                      \\n    scores_svm.append(get_score(SVC(gamma='auto'), X_train, X_test, y_train, y_test))\\n    scores_rf.append(get_score(RandomForestClassifier(n_estimators=40), X_train, X_test, y_train, y_test))\\n    \\nscores_logistic\\nscores_svm\\nscores_rf\\n\".trim();var valScor=\"\\nfrom sklearn.model_selection import cross_val_score\\n\\n#Logistic regression model performance using cross_val_score\\ncross_val_score(LogisticRegression(solver='liblinear',multi_class='ovr'), digits.data, digits.target,cv=3)\\n\\n#svm model performance using cross_val_score\\ncross_val_score(SVC(gamma='auto'), digits.data, digits.target,cv=3)\\n\\n#random forest performance using cross_val_score\\ncross_val_score(RandomForestClassifier(n_estimators=40),digits.data, digits.target,cv=3)\\n\".trim();var tunning=\"\\nscores1 = cross_val_score(RandomForestClassifier(n_estimators=5),digits.data, digits.target, cv=10)\\nnp.average(scores1)\\n\\nscores2 = cross_val_score(RandomForestClassifier(n_estimators=20),digits.data, digits.target, cv=10)\\nnp.average(scores2)\\n\\nscores3 = cross_val_score(RandomForestClassifier(n_estimators=30),digits.data, digits.target, cv=10)\\nnp.average(scores3)\\n\\nscores4 = cross_val_score(RandomForestClassifier(n_estimators=40),digits.data, digits.target, cv=10)\\nnp.average(scores4)\\n\".trim();var knnClassifications=\"\\nimport pandas as pd\\nimport seaborn as sn\\nfrom sklearn.datasets import load_iris\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.neighbors import KNeighborsClassifier\\nfrom sklearn.metrics import confusion_matrix\\nfrom sklearn.metrics import classification_report\\nimport matplotlib.pyplot as plt\\n%matplotlib inline\\n\\niris = load_iris()\\niris.feature_names\\niris.target_names\\n\\ndf = pd.DataFrame(iris.data,columns=iris.feature_names)\\ndf['target'] = iris.target\\ndf[df.target==1].head()\\ndf[df.target==2].head()\\n\\ndf['flower_name'] =df.target.apply(lambda x: iris.target_names[x])\\n\\ndf[45:55]\\ndf0 = df[:50]\\ndf1 = df[50:100]\\ndf2 = df[100:]\\n\\n#Sepal length vs Sepal Width (Setosa vs Versicolor)\\nplt.xlabel('Sepal Length')\\nplt.ylabel('Sepal Width')\\nplt.scatter(df0['sepal length (cm)'], df0['sepal width (cm)'],color=\\\"green\\\",marker='+')\\nplt.scatter(df1['sepal length (cm)'], df1['sepal width (cm)'],color=\\\"blue\\\",marker='.')\\n\\n#Petal length vs Pepal Width (Setosa vs Versicolor)\\nplt.xlabel('Petal Length')\\nplt.ylabel('Petal Width')\\nplt.scatter(df0['petal length (cm)'], df0['petal width (cm)'],color=\\\"green\\\",marker='+')\\nplt.scatter(df1['petal length (cm)'], df1['petal width (cm)'],color=\\\"blue\\\",marker='.')\\n\\nX = df.drop(['target','flower_name'], axis='columns')\\ny = df.target\\n\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\\nlen(X_train)\\nlen(X_test)\\n\\nknn = KNeighborsClassifier(n_neighbors=10)\\nknn.fit(X_train, y_train)\\nknn.score(X_test, y_test)\\nknn.predict([[4.8,3.0,1.5,0.3]])\\n\\ny_pred = knn.predict(X_test)\\ncm = confusion_matrix(y_test, y_pred)\\n\\nplt.figure(figsize=(7,5))\\nsn.heatmap(cm, annot=True)\\nplt.xlabel('Predicted')\\nplt.ylabel('Truth')\\n\\n#Print classification report for precesion, recall and f1-score for each classes\\nprint(classification_report(y_test, y_pred))\\n\".trim();var KnnPy=/*#__PURE__*/function(_Component){_inherits(KnnPy,_Component);function KnnPy(){_classCallCheck(this,KnnPy);return _possibleConstructorReturn(this,_getPrototypeOf(KnnPy).apply(this,arguments));}_createClass(KnnPy,[{key:\"componentDidMount\",value:function componentDidMount(){setTimeout(function(){return Prism.highlightAll();},0);}},{key:\"render\",value:function render(){var classes=this.props.classes;return React.createElement(Grid,{container:true},React.createElement(Grid,{item:true,xs:2},React.createElement(Paper,{className:classes.paper},React.createElement(\"h4\",null,React.createElement(Sidebar,null)))),React.createElement(Grid,{item:true,xs:10},React.createElement(Paper,{className:classes.paper},React.createElement(List,null,React.createElement(\"h3\",null,\"KNN(K-nearest neighbors) supervised ML algorithm\"),\"KNN algorithm is  used for both classification as well as regression predictive problems.\",React.createElement(\"br\",null),React.createElement(\"br\",null),\"Two properties define KNN well:\",React.createElement(\"ul\",null,React.createElement(\"li\",null,React.createElement(\"b\",null,\"1. Lazy learning algorithm: \"),\"KNN is a lazy learning algorithm because it does not have a specialized training phase and uses all the data for training while classification.\"),React.createElement(\"li\",null,React.createElement(\"b\",null,\"Non-parametric learning algorithm: \"),\"It doesn\\u2019t assume anything about the underlying data.\")),React.createElement(\"br\",null),React.createElement(\"b\",null,\"Working of KNN Algorithm: \"),\"KNN algorithm uses \\u2018feature similarity\\u2019 to predict the values of new data points which further means that the new data point will be assigned a value based on how closely it matches the points in the training set.\",React.createElement(\"br\",null),React.createElement(\"br\",null),React.createElement(\"ul\",null,React.createElement(\"li\",null,\"First step of KNN, we must load the training as well as test data.\"),React.createElement(\"li\",null,\"Next, we need to choose the value of K.\"),React.createElement(\"li\",null,React.createElement(\"i\",null,React.createElement(\"b\",null,\"For each point in the test data do the following \\u2212\"))),React.createElement(\"ul\",null,React.createElement(\"li\",null,\"Calculate the distance between test data and each row of training data with the help of any of the method namely: Euclidean(commonly), Manhattan or Hamming distance. \"),React.createElement(\"li\",null,\"Now, based on the distance value, sort them in ascending order.\"),React.createElement(\"li\",null,\"Next, it will choose the top K rows from the sorted array.\"),React.createElement(\"li\",null,\"Now, it will assign a class to the test point based on most frequent class of these rows.\")),React.createElement(\"br\",null),React.createElement(\"li\",null,\"End Ex: Suppose we have a dataset which can be plotted dot. Now, we need to classify new data point with black dot (at point 60,60) into blue/ red class. We are assuming K = 3 i.e. it would find three nearest data points.\")),React.createElement(\"br\",null),React.createElement(\"h3\",null,\"KFold Cross Validation\"),React.createElement(\"div\",{style:titles},React.createElement(PrismCode,{code:kFold,language:\"js\",plugins:[\"line-numbers\"]})),React.createElement(\"br\",null),React.createElement(\"h3\",null,\"KFold cross validation\"),React.createElement(\"div\",{style:titles},React.createElement(PrismCode,{code:kFoldVal,language:\"js\",plugins:[\"line-numbers\"]})),React.createElement(\"br\",null),React.createElement(\"h3\",null,\"cross_val_score function\"),React.createElement(\"div\",{style:titles},React.createElement(PrismCode,{code:valScor,language:\"js\",plugins:[\"line-numbers\"]})),React.createElement(\"br\",null),React.createElement(\"h3\",null,\"Parameter tunning using k fold cross validation\"),React.createElement(\"div\",{style:titles},React.createElement(PrismCode,{code:tunning,language:\"js\",plugins:[\"line-numbers\"]})),React.createElement(\"i\",null,\"Here we used cross_val_score to fine tune our random forest classifier and figured that having around 40 trees in random forest gives best result. \"),React.createElement(\"br\",null),React.createElement(\"h3\",null,\"KNN (K Nearest Neighbors) Classification: Using Python Sklearn\"),\"From sklearn.datasets load digits dataset and do following:\",React.createElement(\"ul\",null,React.createElement(\"li\",null,\"Classify digits (0 to 9) using KNN classifier. You can use different values for k neighbors and need to figure out a value of K that gives you a maximum score. You can manually try different values of K or use gridsearchcv.\"),React.createElement(\"li\",null,\"Plot confusion matrix.\"),React.createElement(\"li\",null,\"Plot classification report.\")),React.createElement(\"br\",null),React.createElement(\"b\",null,\"Why do we need a K-NN Algorithm?\"),React.createElement(\"br\",null),\"Suppose there are two categories, Category A and Category B, and we have a new data point x1, so this data point will lie in which of these categories. To solve this type of problem, we need a K-NN algorithm. With the help of K-NN, we can easily identify the category or class of a particular dataset.\",React.createElement(\"br\",null),React.createElement(\"img\",{src:Sigmoid,alt:\"Equations\",className:\"responsive\",style:redesign}),React.createElement(\"br\",null),React.createElement(\"b\",null,\"Steps\"),React.createElement(\"ul\",null,React.createElement(\"li\",null,\"Select the number K of the neighbors.\"),React.createElement(\"li\",null,\"Calculate the Euclidean distance of K number of neighbors.\"),React.createElement(\"li\",null,\"Take the K nearest neighbors as per the calculated Euclidean distance.\"),React.createElement(\"li\",null,\"Among these k neighbors, count the number of the data points in each category.\"),React.createElement(\"li\",null,\"Assign the new data points to that category for which the number of the neighbor is maximum.\"),React.createElement(\"li\",null,\"Our model is ready.\")),React.createElement(\"br\",null),\"Suppose we have a new data point and we need to put it in the required category.\",React.createElement(\"br\",null),React.createElement(\"img\",{src:Knn3,alt:\"Equations\",className:\"responsive\",style:redesign}),React.createElement(\"ul\",null,React.createElement(\"li\",null,\"Firstly, we will choose the number of neighbors, so we will choose the k=5.\"),React.createElement(\"li\",null,\"Next, we will calculate the Euclidean distance between the data points. It can be calculated as:\"),React.createElement(\"br\",null),React.createElement(\"img\",{src:Knn4,alt:\"Equations\",className:\"responsive\",style:redesign}),React.createElement(\"br\",null),React.createElement(\"li\",null,\"By calculating the Euclidean distance we got the nearest neighbors, as three nearest neighbors in category A and two nearest neighbors in category B.\"),React.createElement(\"br\",null),React.createElement(\"img\",{src:Knn5,alt:\"Equations\",className:\"responsive\",style:redesign}),React.createElement(\"br\",null),React.createElement(\"li\",null,\"As we can see the 3 nearest neighbors are from category A, hence this new data point must belong to category A.\")),React.createElement(\"br\",null),React.createElement(\"b\",null,\"How to select the value of K in the K-NN Algorithm?\"),React.createElement(\"br\",null),React.createElement(\"ul\",null,React.createElement(\"li\",null,\"There is no particular way to determine the best value for \\\"K\\\", so we need to try some values to find the best out of them. The most preferred value for K is 5.\"),React.createElement(\"li\",null,\"A very low value for K such as K=1 or K=2, can be noisy and lead to the effects of outliers in the model.\"),React.createElement(\"li\",null,\"Large values for K are good, but it may find some difficulties.\")),React.createElement(\"br\",null),React.createElement(\"b\",null,\"Advantages: \"),React.createElement(\"ul\",null,React.createElement(\"li\",null,\"It is simple to implement.\"),React.createElement(\"li\",null,\"It is robust to the noisy training data.\"),React.createElement(\"li\",null,\"It can be more effective if the training data is large.\")),React.createElement(\"br\",null),React.createElement(\"b\",null,\"Disadvantages: \"),React.createElement(\"ul\",null,React.createElement(\"li\",null,\"Always needs to determine the value of K which may be complex some time.\"),React.createElement(\"li\",null,\"The computation cost is high because of calculating the distance between the data points for all the training samples.\")),React.createElement(\"div\",{style:titles},React.createElement(PrismCode,{code:knnClassifications,language:\"js\",plugins:[\"line-numbers\"]}))))));}}]);return KnnPy;}(Component);export default withStyles(styles)(KnnPy);","map":{"version":3,"sources":["/home/mukeshs/Projects/edurights/src/components/ml/deepMl/knn.js"],"names":["React","Component","Prism","Grid","Paper","withStyles","List","Sidebar","PrismCode","Sigmoid","Knn3","Knn4","Knn5","titles","backgroundColor","padding","fontSize","redesign","height","width","styles","theme","paper","margin","spacing","smMargin","actionDiv","textAlign","kFold","trim","kFoldVal","valScor","tunning","knnClassifications","KnnPy","setTimeout","highlightAll","classes","props"],"mappings":"6kBAAA,MAAOA,CAAAA,KAAP,EAAgBC,SAAhB,KAAiC,OAAjC,CACA,MAAOC,CAAAA,KAAP,KAAkB,SAAlB,CACA,OAASC,IAAT,CAAeC,KAAf,CAAsBC,UAAtB,CAAkCC,IAAlC,KAA8C,mBAA9C,CAEA,MAAO,0BAAP,CACA,MAAOC,CAAAA,OAAP,KAAoB,YAApB,CACA,MAAOC,CAAAA,SAAP,KAAsB,yBAAtB,CAEA,MAAOC,CAAAA,OAAP,KAAoB,6BAApB,CACA,MAAOC,CAAAA,IAAP,KAAiB,6BAAjB,CACA,MAAOC,CAAAA,IAAP,KAAiB,6BAAjB,CACA,MAAOC,CAAAA,IAAP,KAAiB,6BAAjB,CAGA,GAAMC,CAAAA,MAAM,CAAG,CAAEC,eAAe,CAAE,SAAnB,CAA8BC,OAAO,CAAE,KAAvC,CAA8CC,QAAQ,CAAE,MAAxD,CAAf,CAEA,GAAMC,CAAAA,QAAQ,CAAG,CACfC,MAAM,CAAE,GADO,CAEfC,KAAK,CAAE,GAFQ,CAAjB,CAKA,GAAMC,CAAAA,MAAM,CAAG,QAATA,CAAAA,MAAS,CAAAC,KAAK,QAAK,CACvBC,KAAK,CAAE,CACLC,MAAM,CAAEF,KAAK,CAACG,OAAN,CAAc,CAAd,CADH,CAELT,OAAO,CAAEM,KAAK,CAACG,OAAN,CAAc,CAAd,CAFJ,CADgB,CAKvBC,QAAQ,CAAE,CACRF,MAAM,CAAEF,KAAK,CAACG,OAAN,CAAc,CAAd,CADA,CALa,CAQvBE,SAAS,CAAE,CACTC,SAAS,CAAE,QADF,CARY,CAAL,EAApB,CAaA,GAAMC,CAAAA,KAAK,CAAG,i5BAwBZC,IAxBY,EAAd,CA0BA,GAAMC,CAAAA,QAAQ,CAAG,ysCAgCfD,IAhCe,EAAjB,CAkCA,GAAME,CAAAA,OAAO,CAAG,yeAWdF,IAXc,EAAhB,CAaA,GAAMG,CAAAA,OAAO,CAAG,sfAYdH,IAZc,EAAhB,CAcA,GAAMI,CAAAA,kBAAkB,CAAG,61DA6DzBJ,IA7DyB,EAA3B,C,GAgEMK,CAAAA,K,iRACgB,CAClBC,UAAU,CAAC,iBAAMjC,CAAAA,KAAK,CAACkC,YAAN,EAAN,EAAD,CAA6B,CAA7B,CAAV,CACD,C,uCACQ,IACCC,CAAAA,OADD,CACa,KAAKC,KADlB,CACCD,OADD,CAEP,MACE,qBAAC,IAAD,EAAM,SAAS,KAAf,EACE,oBAAC,IAAD,EAAM,IAAI,KAAV,CAAW,EAAE,CAAE,CAAf,EACE,oBAAC,KAAD,EAAO,SAAS,CAAEA,OAAO,CAACf,KAA1B,EACE,8BAAI,oBAAC,OAAD,MAAJ,CADF,CADF,CADF,CAME,oBAAC,IAAD,EAAM,IAAI,KAAV,CAAW,EAAE,CAAE,EAAf,EACE,oBAAC,KAAD,EAAO,SAAS,CAAEe,OAAO,CAACf,KAA1B,EACE,oBAAC,IAAD,MACE,iFADF,6FAGE,8BAHF,CAIE,8BAJF,mCAME,8BACE,8BAAI,4DAAJ,mJADF,CAGE,8BAAI,mEAAJ,8DAHF,CANF,CAWE,8BAXF,CAaE,0DAbF,mOAgBE,8BAhBF,CAiBE,8BAjBF,CAmBE,8BACE,mGADF,CAEE,wEAFF,CAGE,8BAAI,6BAAG,uFAAH,CAAJ,CAHF,CAIE,8BACE,uMADF,CAGE,gGAHF,CAIE,2FAJF,CAKE,0HALF,CAJF,CAWE,8BAXF,CAYE,8PAZF,CAnBF,CAmCE,8BAnCF,CAqCE,uDArCF,CAsCE,2BAAK,KAAK,CAAET,MAAZ,EACE,oBAAC,SAAD,EACE,IAAI,CAAEe,KADR,CAEE,QAAQ,CAAC,IAFX,CAGE,OAAO,CAAE,CAAC,cAAD,CAHX,EADF,CAtCF,CA6CE,8BA7CF,CA+CE,uDA/CF,CAgDE,2BAAK,KAAK,CAAEf,MAAZ,EACE,oBAAC,SAAD,EACE,IAAI,CAAEiB,QADR,CAEE,QAAQ,CAAC,IAFX,CAGE,OAAO,CAAE,CAAC,cAAD,CAHX,EADF,CAhDF,CAuDE,8BAvDF,CAyDE,yDAzDF,CA0DE,2BAAK,KAAK,CAAEjB,MAAZ,EACE,oBAAC,SAAD,EACE,IAAI,CAAEkB,OADR,CAEE,QAAQ,CAAC,IAFX,CAGE,OAAO,CAAE,CAAC,cAAD,CAHX,EADF,CA1DF,CAkEE,8BAlEF,CAoEE,gFApEF,CAqEE,2BAAK,KAAK,CAAElB,MAAZ,EACE,oBAAC,SAAD,EACE,IAAI,CAAEmB,OADR,CAEE,QAAQ,CAAC,IAFX,CAGE,OAAO,CAAE,CAAC,cAAD,CAHX,EADF,CArEF,CA4EE,mLA5EF,CA8EE,8BA9EF,CAgFE,+FAhFF,+DAkFE,8BACE,gQADF,CAEE,uDAFF,CAGE,4DAHF,CAlFF,CAuFE,8BAvFF,CAyFE,gEAzFF,CAyFyC,8BAzFzC,iTA6FE,8BA7FF,CA8FE,2BAAK,GAAG,CAAEvB,OAAV,CAAmB,GAAG,CAAC,WAAvB,CAAmC,SAAS,CAAC,YAA7C,CAA0D,KAAK,CAAEQ,QAAjE,EA9FF,CA+FE,8BA/FF,CAiGE,qCAjGF,CAkGE,8BACE,sEADF,CAEE,2FAFF,CAGE,uGAHF,CAIE,+GAJF,CAKE,6HALF,CAME,oDANF,CAlGF,CA0GE,8BA1GF,oFA4GE,8BA5GF,CA6GE,2BAAK,GAAG,CAAEP,IAAV,CAAgB,GAAG,CAAC,WAApB,CAAgC,SAAS,CAAC,YAA1C,CAAuD,KAAK,CAAEO,QAA9D,EA7GF,CA8GE,8BACE,4GADF,CAEE,iIAFF,CAGE,8BAHF,CAIE,2BAAK,GAAG,CAAEN,IAAV,CAAgB,GAAG,CAAC,WAApB,CAAgC,SAAS,CAAC,YAA1C,CAAuD,KAAK,CAAEM,QAA9D,EAJF,CAKE,8BALF,CAME,sLANF,CAOE,8BAPF,CAQE,2BAAK,GAAG,CAAEL,IAAV,CAAgB,GAAG,CAAC,WAApB,CAAgC,SAAS,CAAC,YAA1C,CAAuD,KAAK,CAAEK,QAA9D,EARF,CASE,8BATF,CAUE,gJAVF,CA9GF,CA0HE,8BA1HF,CA2HE,mFA3HF,CA4HE,8BA5HF,CA6HE,8BACE,mMADF,CAGE,0IAHF,CAIE,gGAJF,CA7HF,CAmIE,8BAnIF,CAqIE,4CArIF,CAsIE,8BACE,2DADF,CAEE,yEAFF,CAGE,wFAHF,CAtIF,CA2IE,8BA3IF,CA6IE,+CA7IF,CA8IE,8BACE,yGADF,CAEE,uJAFF,CA9IF,CAoJE,2BAAK,KAAK,CAAEJ,MAAZ,EACE,oBAAC,SAAD,EACE,IAAI,CAAEoB,kBADR,CAEE,QAAQ,CAAC,IAFX,CAGE,OAAO,CAAE,CAAC,cAAD,CAHX,EADF,CApJF,CADF,CADF,CANF,CADF,CAyKD,C,mBA/KiBhC,S,EAkLpB,cAAgBI,CAAAA,UAAU,CAACe,MAAD,CAAV,CAAmBc,KAAnB,CAAhB","sourcesContent":["import React, { Component } from 'react';\nimport Prism from \"prismjs\"\nimport { Grid, Paper, withStyles, List } from \"@material-ui/core\";\n\nimport '../../ReactJs/styles.css'\nimport Sidebar from '../sidebar';\nimport PrismCode from '../../ReactJs/prismCode';\n\nimport Sigmoid from '../../../assets/ML/knn2.png'\nimport Knn3 from '../../../assets/ML/knn3.png'\nimport Knn4 from '../../../assets/ML/knn4.png'\nimport Knn5 from '../../../assets/ML/knn5.png'\n\n\nconst titles = { backgroundColor: '#F0F8FF', padding: '1px', fontSize: '16px' }\n\nconst redesign = {\n  height: 350,\n  width: 600\n}\n\nconst styles = theme => ({\n  paper: {\n    margin: theme.spacing(1),\n    padding: theme.spacing(1)\n  },\n  smMargin: {\n    margin: theme.spacing(1)\n  },\n  actionDiv: {\n    textAlign: \"center\"\n  }\n})\n\nconst kFold = `\nimport numpy as np\nfrom sklearn.datasets import load_digits\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\n\ndigits = load_digits()\n\nX_train, X_test, y_train, y_test = train_test_split(digits.data,digits.target,test_size=0.3)\nlr = LogisticRegression(solver='liblinear',multi_class='ovr')                                     #Logistic Regression\nlr.fit(X_train, y_train)\nlr.score(X_test, y_test)\n\nsvm = SVC(gamma='auto')                                                                           #SVM\nsvm.fit(X_train, y_train)\nsvm.score(X_test, y_test)\n\n\nrf = RandomForestClassifier(n_estimators=40)                                                      #Random Forest\nrf.fit(X_train, y_train)\nrf.score(X_test, y_test)\n`.trim();\n\nconst kFoldVal = `\nfrom sklearn.model_selection import KFold\nkf = KFold(n_splits=3)\n\nfor train_index, test_index in kf.split([1,2,3,4,5,6,7,8,9]):\n    print(train_index, test_index)\n    \ndef get_score(model, X_train, X_test, y_train, y_test):\n    model.fit(X_train, y_train)\n    return model.score(X_test, y_test)\n    \n    from sklearn.model_selection import StratifiedKFold\nfolds = StratifiedKFold(n_splits=3)\n\nscores_logistic = []\nscores_svm = []\nscores_rf = []\n\nfor train_index, test_index in folds.split(digits.data,digits.target):\n    X_train, X_test, y_train, y_test = digits.data[train_index], digits.data[test_index], \\\n    \n                                       digits.target[train_index], digits.target[test_index]\n                                       \n    scores_logistic.append(get_score(LogisticRegression(solver='liblinear',multi_class='ovr'), \n                                      X_train, X_test, y_train, y_test))  \n                                      \n    scores_svm.append(get_score(SVC(gamma='auto'), X_train, X_test, y_train, y_test))\n    scores_rf.append(get_score(RandomForestClassifier(n_estimators=40), X_train, X_test, y_train, y_test))\n    \nscores_logistic\nscores_svm\nscores_rf\n`.trim();\n\nconst valScor = `\nfrom sklearn.model_selection import cross_val_score\n\n#Logistic regression model performance using cross_val_score\ncross_val_score(LogisticRegression(solver='liblinear',multi_class='ovr'), digits.data, digits.target,cv=3)\n\n#svm model performance using cross_val_score\ncross_val_score(SVC(gamma='auto'), digits.data, digits.target,cv=3)\n\n#random forest performance using cross_val_score\ncross_val_score(RandomForestClassifier(n_estimators=40),digits.data, digits.target,cv=3)\n`.trim();\n\nconst tunning = `\nscores1 = cross_val_score(RandomForestClassifier(n_estimators=5),digits.data, digits.target, cv=10)\nnp.average(scores1)\n\nscores2 = cross_val_score(RandomForestClassifier(n_estimators=20),digits.data, digits.target, cv=10)\nnp.average(scores2)\n\nscores3 = cross_val_score(RandomForestClassifier(n_estimators=30),digits.data, digits.target, cv=10)\nnp.average(scores3)\n\nscores4 = cross_val_score(RandomForestClassifier(n_estimators=40),digits.data, digits.target, cv=10)\nnp.average(scores4)\n`.trim();\n\nconst knnClassifications = `\nimport pandas as pd\nimport seaborn as sn\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\niris = load_iris()\niris.feature_names\niris.target_names\n\ndf = pd.DataFrame(iris.data,columns=iris.feature_names)\ndf['target'] = iris.target\ndf[df.target==1].head()\ndf[df.target==2].head()\n\ndf['flower_name'] =df.target.apply(lambda x: iris.target_names[x])\n\ndf[45:55]\ndf0 = df[:50]\ndf1 = df[50:100]\ndf2 = df[100:]\n\n#Sepal length vs Sepal Width (Setosa vs Versicolor)\nplt.xlabel('Sepal Length')\nplt.ylabel('Sepal Width')\nplt.scatter(df0['sepal length (cm)'], df0['sepal width (cm)'],color=\"green\",marker='+')\nplt.scatter(df1['sepal length (cm)'], df1['sepal width (cm)'],color=\"blue\",marker='.')\n\n#Petal length vs Pepal Width (Setosa vs Versicolor)\nplt.xlabel('Petal Length')\nplt.ylabel('Petal Width')\nplt.scatter(df0['petal length (cm)'], df0['petal width (cm)'],color=\"green\",marker='+')\nplt.scatter(df1['petal length (cm)'], df1['petal width (cm)'],color=\"blue\",marker='.')\n\nX = df.drop(['target','flower_name'], axis='columns')\ny = df.target\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\nlen(X_train)\nlen(X_test)\n\nknn = KNeighborsClassifier(n_neighbors=10)\nknn.fit(X_train, y_train)\nknn.score(X_test, y_test)\nknn.predict([[4.8,3.0,1.5,0.3]])\n\ny_pred = knn.predict(X_test)\ncm = confusion_matrix(y_test, y_pred)\n\nplt.figure(figsize=(7,5))\nsn.heatmap(cm, annot=True)\nplt.xlabel('Predicted')\nplt.ylabel('Truth')\n\n#Print classification report for precesion, recall and f1-score for each classes\nprint(classification_report(y_test, y_pred))\n`.trim();\n\n\nclass KnnPy extends Component {\n  componentDidMount() {\n    setTimeout(() => Prism.highlightAll(), 0)\n  }\n  render() {\n    const { classes } = this.props;\n    return (\n      <Grid container>\n        <Grid item xs={2}>\n          <Paper className={classes.paper}>\n            <h4><Sidebar /></h4>\n          </Paper>\n        </Grid>\n        <Grid item xs={10}>\n          <Paper className={classes.paper}>\n            <List>\n              <h3>KNN(K-nearest neighbors) supervised ML algorithm</h3>\n              KNN algorithm is  used for both classification as well as regression predictive problems.\n              <br />\n              <br />\n              Two properties define KNN well:\n              <ul>\n                <li><b>1. Lazy learning algorithm: </b>KNN is a lazy learning algorithm because it does not have a specialized training phase and uses all the data for\n                  training while classification.</li>\n                <li><b>Non-parametric learning algorithm: </b>It doesn’t assume anything about the underlying data.</li>\n              </ul>\n              <br />\n\n              <b>Working of KNN Algorithm: </b>\n              KNN algorithm uses ‘feature similarity’ to predict the values of new data points which further means that the new data\n              point will be assigned a value based on how closely it matches the points in the training set.\n              <br />\n              <br />\n\n              <ul>\n                <li>First step of KNN, we must load the training as well as test data.</li>\n                <li>Next, we need to choose the value of K.</li>\n                <li><i><b>For each point in the test data do the following −</b></i></li>\n                <ul>\n                  <li>Calculate the distance between test data and each row of training data with the help of any of the method\n                    namely: Euclidean(commonly), Manhattan or Hamming distance. </li>\n                  <li>Now, based on the distance value, sort them in ascending order.</li>\n                  <li>Next, it will choose the top K rows from the sorted array.</li>\n                  <li>Now, it will assign a class to the test point based on most frequent class of these rows.</li>\n                </ul>\n                <br />\n                <li>End Ex: Suppose we have a dataset which can be plotted dot. Now, we need to classify new data point with\n                  black dot (at point 60,60) into blue/ red class. We are assuming K = 3 i.e.\n                  it would find three nearest data points.</li>\n              </ul>\n              <br />\n\n              <h3>KFold Cross Validation</h3>\n              <div style={titles}>\n                <PrismCode\n                  code={kFold}\n                  language=\"js\"\n                  plugins={[\"line-numbers\"]}\n                />\n              </div>\n              <br />\n\n              <h3>KFold cross validation</h3>\n              <div style={titles}>\n                <PrismCode\n                  code={kFoldVal}\n                  language=\"js\"\n                  plugins={[\"line-numbers\"]}\n                />\n              </div>\n              <br />\n\n              <h3>cross_val_score function</h3>\n              <div style={titles}>\n                <PrismCode\n                  code={valScor}\n                  language=\"js\"\n                  plugins={[\"line-numbers\"]}\n                />\n              </div>\n\n              <br />\n\n              <h3>Parameter tunning using k fold cross validation</h3>\n              <div style={titles}>\n                <PrismCode\n                  code={tunning}\n                  language=\"js\"\n                  plugins={[\"line-numbers\"]}\n                />\n              </div>\n              <i>Here we used cross_val_score to\n                fine tune our random forest classifier and figured that having around 40 trees in random forest gives best result. </i>\n              <br />\n\n              <h3>KNN (K Nearest Neighbors) Classification: Using Python Sklearn</h3>\n              From sklearn.datasets load digits dataset and do following:\n              <ul>\n                <li>Classify digits (0 to 9) using KNN classifier. You can use different values for k neighbors and need to figure out a value of K that gives you a maximum score. You can manually try different values of K or use gridsearchcv.</li>\n                <li>Plot confusion matrix.</li>\n                <li>Plot classification report.</li>\n              </ul>\n              <br />\n\n              <b>Why do we need a K-NN Algorithm?</b><br />\n              Suppose there are two categories, Category A and Category B, and we have a new data point x1, so this\n              data point will lie in which of these categories. To solve this type of problem, we need a K-NN\n              algorithm. With the help of K-NN, we can easily identify the category or class of a particular dataset.\n              <br />\n              <img src={Sigmoid} alt=\"Equations\" className=\"responsive\" style={redesign} />\n              <br />\n\n              <b>Steps</b>\n              <ul>\n                <li>Select the number K of the neighbors.</li>\n                <li>Calculate the Euclidean distance of K number of neighbors.</li>\n                <li>Take the K nearest neighbors as per the calculated Euclidean distance.</li>\n                <li>Among these k neighbors, count the number of the data points in each category.</li>\n                <li>Assign the new data points to that category for which the number of the neighbor is maximum.</li>\n                <li>Our model is ready.</li>\n              </ul>\n              <br />\n              Suppose we have a new data point and we need to put it in the required category.\n              <br />\n              <img src={Knn3} alt=\"Equations\" className=\"responsive\" style={redesign} />\n              <ul>\n                <li>Firstly, we will choose the number of neighbors, so we will choose the k=5.</li>\n                <li>Next, we will calculate the Euclidean distance between the data points. It can be calculated as:</li>\n                <br />\n                <img src={Knn4} alt=\"Equations\" className=\"responsive\" style={redesign} />\n                <br />\n                <li>By calculating the Euclidean distance we got the nearest neighbors, as three nearest neighbors in category A and two nearest neighbors in category B.</li>\n                <br />\n                <img src={Knn5} alt=\"Equations\" className=\"responsive\" style={redesign} />\n                <br />\n                <li>As we can see the 3 nearest neighbors are from category A, hence this new data point must belong to category A.</li>\n              </ul>\n              <br />\n              <b>How to select the value of K in the K-NN Algorithm?</b>\n              <br />\n              <ul>\n                <li>There is no particular way to determine the best value for \"K\", so we need to try some values to\n                  find the best out of them. The most preferred value for K is 5.</li>\n                <li>A very low value for K such as K=1 or K=2, can be noisy and lead to the effects of outliers in the model.</li>\n                <li>Large values for K are good, but it may find some difficulties.</li>\n              </ul>\n              <br />\n\n              <b>Advantages: </b>\n              <ul>\n                <li>It is simple to implement.</li>\n                <li>It is robust to the noisy training data.</li>\n                <li>It can be more effective if the training data is large.</li>\n              </ul>\n              <br />\n\n              <b>Disadvantages: </b>\n              <ul>\n                <li>Always needs to determine the value of K which may be complex some time.</li>\n                <li>The computation cost is high because of calculating the distance between the data points for all\n                  the training samples.</li>\n              </ul>\n\n              <div style={titles}>\n                <PrismCode\n                  code={knnClassifications}\n                  language=\"js\"\n                  plugins={[\"line-numbers\"]}\n                />\n              </div>\n            </List>\n          </Paper>\n        </Grid>\n      </Grid>\n    )\n  }\n}\n\nexport default (withStyles(styles)(KnnPy));\n"]},"metadata":{},"sourceType":"module"}