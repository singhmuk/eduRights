{"ast":null,"code":"import _classCallCheck from\"/home/mukeshs/Projects/edurights/node_modules/@babel/runtime/helpers/esm/classCallCheck\";import _createClass from\"/home/mukeshs/Projects/edurights/node_modules/@babel/runtime/helpers/esm/createClass\";import _possibleConstructorReturn from\"/home/mukeshs/Projects/edurights/node_modules/@babel/runtime/helpers/esm/possibleConstructorReturn\";import _getPrototypeOf from\"/home/mukeshs/Projects/edurights/node_modules/@babel/runtime/helpers/esm/getPrototypeOf\";import _inherits from\"/home/mukeshs/Projects/edurights/node_modules/@babel/runtime/helpers/esm/inherits\";import React,{Component}from'react';import Prism from\"prismjs\";import{Grid,Paper,withStyles,List}from\"@material-ui/core\";import'../../ReactJs/styles.css';import Sidebar from'../sidebar';import PrismCode from'../../ReactJs/prismCode';import Activations from'../../../assets/AI/wordembedding.png';var titles={backgroundColor:'#F0F8FF',padding:'1px',fontSize:'16px'};var redesign={height:200,width:500};var styles=function styles(theme){return{paper:{margin:theme.spacing(1),padding:theme.spacing(1)},smMargin:{margin:theme.spacing(1)},actionDiv:{textAlign:\"center\"}};};var childsFile=\"\\nimport numpy as np\\nfrom tensorflow.keras.preprocessing.text import one_hot\\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\\nfrom tensorflow.keras.models import Sequential\\nfrom tensorflow.keras.layers import Dense\\nfrom tensorflow.keras.layers import Flatten\\nfrom tensorflow.keras.layers import Embedding\\n\\nreviews = ['nice food',\\n        'amazing restaurant',\\n        'too good',\\n        'just loved it!',\\n        'will go again',\\n        'horrible food',\\n        'never go there',\\n        'poor service',\\n        'poor quality',\\n        'needs improvement']\\n\\nsentiment = np.array([1,1,1,1,1,0,0,0,0,0])\\none_hot(\\\"amazing restaurant\\\",30)\\n\\nvocab_size = 30\\nencoded_reviews = [one_hot(d, vocab_size) for d in reviews]\\nprint(encoded_reviews)\\n\\nmax_length = 4\\npadded_reviews = pad_sequences(encoded_reviews, maxlen=max_length, padding='post')\\nprint(padded_reviews)\\n\\nembeded_vector_size = 5\\n\\nmodel = Sequential()\\nmodel.add(Embedding(vocab_size, embeded_vector_size, input_length=max_length,name=\\\"embedding\\\"))\\nmodel.add(Flatten())\\nmodel.add(Dense(1, activation='sigmoid'))\\n\\nX = padded_reviews\\ny = sentiment\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\\nprint(model.summary())\\n\\nmodel.fit(X, y, epochs=50, verbose=0)\\nweights = model.get_layer('embedding').get_weights()[0]\\n\\nweights[13]\\nweights[4]\\nweights[16]\\n\".trim();// const pipes = ``.trim();\n// const pipes = ``.trim();\n// const pipes = ``.trim();\nvar WordEmbedding=/*#__PURE__*/function(_Component){_inherits(WordEmbedding,_Component);function WordEmbedding(){_classCallCheck(this,WordEmbedding);return _possibleConstructorReturn(this,_getPrototypeOf(WordEmbedding).apply(this,arguments));}_createClass(WordEmbedding,[{key:\"componentDidMount\",value:function componentDidMount(){setTimeout(function(){return Prism.highlightAll();},0);}},{key:\"render\",value:function render(){var classes=this.props.classes;return React.createElement(Grid,{container:true},React.createElement(Grid,{item:true,xs:2},React.createElement(Paper,{className:classes.paper},React.createElement(\"h4\",null,React.createElement(Sidebar,null)))),React.createElement(Grid,{item:true,xs:10},React.createElement(Paper,{className:classes.paper},React.createElement(List,null,React.createElement(\"h3\",null,\"Word Embedding\"),React.createElement(\"ul\",null,React.createElement(\"li\",null,\"Computers break everything down to numbers. What happens when a software inside a computer (like a ML algorithm) has to operate/ process a word? Simple, this word needs to be given to the computer as the only thing it can understand: as numbers.\"),React.createElement(\"li\",null,\"In NLP, the most simple way to do this is by creating a vocabulary with a huge amount of words (100.000), and assigning a number to each word in the vocabulary.\"),React.createElement(\"li\",null,\"The first word in our vocabulary (\\u2018apple\\u2019 maybe) will be number 0. The second word (\\u2018banana\\u2019) will be number 1, and so on up to number 99.998, the previous to last word (\\u2018king\\u2019) and 999.999 being assigned to the last word (\\u2018queen\\u2019).\"),React.createElement(\"li\",null,\"Then we represent every word as a vector of length 100.000, where every single item is a zero except one of them, corresponding to the index of the number that the word is associated with.\"),React.createElement(\"br\",null),React.createElement(\"img\",{src:Activations,alt:\"Theata\",className:\"responsive2\",style:redesign}),React.createElement(\"br\",null),React.createElement(\"li\",null,\"This is called one-hot encoding for words.\"),React.createElement(\"li\",null,\"The main thing is that word embeddings are vectors that represent words, so that similar meaning words have similar vectors.\"),React.createElement(\"li\",null,\"The two most used Word embedding algorithms are Word2Vec and GloVe.\")),React.createElement(\"br\",null),React.createElement(\"div\",{style:titles},React.createElement(PrismCode,{code:childsFile,language:\"js\",plugins:[\"line-numbers\"]})),React.createElement(\"br\",null),React.createElement(\"h3\",null),React.createElement(\"div\",{style:titles},React.createElement(PrismCode,{code:childsFile,language:\"js\",plugins:[\"line-numbers\"]})),React.createElement(\"br\",null),React.createElement(\"h3\",null),React.createElement(\"div\",{style:titles},React.createElement(PrismCode,{code:childsFile,language:\"js\",plugins:[\"line-numbers\"]})),React.createElement(\"br\",null),React.createElement(\"h3\",null),React.createElement(\"div\",{style:titles},React.createElement(PrismCode,{code:childsFile,language:\"js\",plugins:[\"line-numbers\"]})),React.createElement(\"br\",null),React.createElement(\"h3\",null),React.createElement(\"div\",{style:titles},React.createElement(PrismCode,{code:childsFile,language:\"js\",plugins:[\"line-numbers\"]}))))));}}]);return WordEmbedding;}(Component);export default withStyles(styles)(WordEmbedding);","map":{"version":3,"sources":["/home/mukeshs/Projects/edurights/src/components/angularjs/deepAngularjs/word_embedding.js"],"names":["React","Component","Prism","Grid","Paper","withStyles","List","Sidebar","PrismCode","Activations","titles","backgroundColor","padding","fontSize","redesign","height","width","styles","theme","paper","margin","spacing","smMargin","actionDiv","textAlign","childsFile","trim","WordEmbedding","setTimeout","highlightAll","classes","props"],"mappings":"6kBAAA,MAAOA,CAAAA,KAAP,EAAgBC,SAAhB,KAAiC,OAAjC,CACA,MAAOC,CAAAA,KAAP,KAAkB,SAAlB,CACA,OAASC,IAAT,CAAeC,KAAf,CAAsBC,UAAtB,CAAkCC,IAAlC,KAA8C,mBAA9C,CAEA,MAAO,0BAAP,CACA,MAAOC,CAAAA,OAAP,KAAoB,YAApB,CACA,MAAOC,CAAAA,SAAP,KAAsB,yBAAtB,CAEA,MAAOC,CAAAA,WAAP,KAAwB,sCAAxB,CAGA,GAAMC,CAAAA,MAAM,CAAG,CAAEC,eAAe,CAAE,SAAnB,CAA8BC,OAAO,CAAE,KAAvC,CAA8CC,QAAQ,CAAE,MAAxD,CAAf,CAEA,GAAMC,CAAAA,QAAQ,CAAG,CACfC,MAAM,CAAE,GADO,CAEfC,KAAK,CAAE,GAFQ,CAAjB,CAKA,GAAMC,CAAAA,MAAM,CAAG,QAATA,CAAAA,MAAS,CAAAC,KAAK,QAAK,CACvBC,KAAK,CAAE,CACLC,MAAM,CAAEF,KAAK,CAACG,OAAN,CAAc,CAAd,CADH,CAELT,OAAO,CAAEM,KAAK,CAACG,OAAN,CAAc,CAAd,CAFJ,CADgB,CAKvBC,QAAQ,CAAE,CACRF,MAAM,CAAEF,KAAK,CAACG,OAAN,CAAc,CAAd,CADA,CALa,CAQvBE,SAAS,CAAE,CACTC,SAAS,CAAE,QADF,CARY,CAAL,EAApB,CAcA,GAAMC,CAAAA,UAAU,CAAG,g4CAkDjBC,IAlDiB,EAAnB,CAoDA;AAEA;AAEA;GAGMC,CAAAA,a,yTACgB,CAClBC,UAAU,CAAC,iBAAM1B,CAAAA,KAAK,CAAC2B,YAAN,EAAN,EAAD,CAA6B,CAA7B,CAAV,CACD,C,uCACQ,IACCC,CAAAA,OADD,CACa,KAAKC,KADlB,CACCD,OADD,CAEP,MACE,qBAAC,IAAD,EAAM,SAAS,KAAf,EACE,oBAAC,IAAD,EAAM,IAAI,KAAV,CAAW,EAAE,CAAE,CAAf,EACE,oBAAC,KAAD,EAAO,SAAS,CAAEA,OAAO,CAACX,KAA1B,EACE,8BAAI,oBAAC,OAAD,MAAJ,CADF,CADF,CADF,CAME,oBAAC,IAAD,EAAM,IAAI,KAAV,CAAW,EAAE,CAAE,EAAf,EACE,oBAAC,KAAD,EAAO,SAAS,CAAEW,OAAO,CAACX,KAA1B,EACE,oBAAC,IAAD,MACE,+CADF,CAEE,8BACE,sRADF,CAGE,iMAHF,CAKE,iTALF,CAOE,6NAPF,CASE,8BATF,CAUE,2BAAK,GAAG,CAAEV,WAAV,CAAuB,GAAG,CAAC,QAA3B,CAAoC,SAAS,CAAC,aAA9C,CAA4D,KAAK,CAAEK,QAAnE,EAVF,CAWE,8BAXF,CAYE,2EAZF,CAaE,6JAbF,CAcE,oGAdF,CAFF,CAkBE,8BAlBF,CAoBE,2BAAK,KAAK,CAAEJ,MAAZ,EACE,oBAAC,SAAD,EACE,IAAI,CAAEe,UADR,CAEE,QAAQ,CAAC,IAFX,CAGE,OAAO,CAAE,CAAC,cAAD,CAHX,EADF,CApBF,CA2BE,8BA3BF,CA6BE,8BA7BF,CA8BE,2BAAK,KAAK,CAAEf,MAAZ,EACE,oBAAC,SAAD,EACE,IAAI,CAAEe,UADR,CAEE,QAAQ,CAAC,IAFX,CAGE,OAAO,CAAE,CAAC,cAAD,CAHX,EADF,CA9BF,CAqCE,8BArCF,CAuCE,8BAvCF,CAwCE,2BAAK,KAAK,CAAEf,MAAZ,EACE,oBAAC,SAAD,EACE,IAAI,CAAEe,UADR,CAEE,QAAQ,CAAC,IAFX,CAGE,OAAO,CAAE,CAAC,cAAD,CAHX,EADF,CAxCF,CA+CE,8BA/CF,CAiDE,8BAjDF,CAkDE,2BAAK,KAAK,CAAEf,MAAZ,EACE,oBAAC,SAAD,EACE,IAAI,CAAEe,UADR,CAEE,QAAQ,CAAC,IAFX,CAGE,OAAO,CAAE,CAAC,cAAD,CAHX,EADF,CAlDF,CAyDE,8BAzDF,CA2DE,8BA3DF,CA4DE,2BAAK,KAAK,CAAEf,MAAZ,EACE,oBAAC,SAAD,EACE,IAAI,CAAEe,UADR,CAEE,QAAQ,CAAC,IAFX,CAGE,OAAO,CAAE,CAAC,cAAD,CAHX,EADF,CA5DF,CADF,CADF,CANF,CADF,CAiFD,C,2BAvFyBxB,S,EA2F5B,cAAgBI,CAAAA,UAAU,CAACY,MAAD,CAAV,CAAmBU,aAAnB,CAAhB","sourcesContent":["import React, { Component } from 'react';\nimport Prism from \"prismjs\"\nimport { Grid, Paper, withStyles, List } from \"@material-ui/core\";\n\nimport '../../ReactJs/styles.css'\nimport Sidebar from '../sidebar';\nimport PrismCode from '../../ReactJs/prismCode';\n\nimport Activations from '../../../assets/AI/wordembedding.png'\n\n\nconst titles = { backgroundColor: '#F0F8FF', padding: '1px', fontSize: '16px' }\n\nconst redesign = {\n  height: 200,\n  width: 500\n}\n\nconst styles = theme => ({\n  paper: {\n    margin: theme.spacing(1),\n    padding: theme.spacing(1)\n  },\n  smMargin: {\n    margin: theme.spacing(1)\n  },\n  actionDiv: {\n    textAlign: \"center\"\n  }\n})\n\n\nconst childsFile = `\nimport numpy as np\nfrom tensorflow.keras.preprocessing.text import one_hot\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import Flatten\nfrom tensorflow.keras.layers import Embedding\n\nreviews = ['nice food',\n        'amazing restaurant',\n        'too good',\n        'just loved it!',\n        'will go again',\n        'horrible food',\n        'never go there',\n        'poor service',\n        'poor quality',\n        'needs improvement']\n\nsentiment = np.array([1,1,1,1,1,0,0,0,0,0])\none_hot(\"amazing restaurant\",30)\n\nvocab_size = 30\nencoded_reviews = [one_hot(d, vocab_size) for d in reviews]\nprint(encoded_reviews)\n\nmax_length = 4\npadded_reviews = pad_sequences(encoded_reviews, maxlen=max_length, padding='post')\nprint(padded_reviews)\n\nembeded_vector_size = 5\n\nmodel = Sequential()\nmodel.add(Embedding(vocab_size, embeded_vector_size, input_length=max_length,name=\"embedding\"))\nmodel.add(Flatten())\nmodel.add(Dense(1, activation='sigmoid'))\n\nX = padded_reviews\ny = sentiment\n\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\nprint(model.summary())\n\nmodel.fit(X, y, epochs=50, verbose=0)\nweights = model.get_layer('embedding').get_weights()[0]\n\nweights[13]\nweights[4]\nweights[16]\n`.trim();\n\n// const pipes = ``.trim();\n\n// const pipes = ``.trim();\n\n// const pipes = ``.trim();\n\n\nclass WordEmbedding extends Component {\n  componentDidMount() {\n    setTimeout(() => Prism.highlightAll(), 0)\n  }\n  render() {\n    const { classes } = this.props;\n    return (\n      <Grid container>\n        <Grid item xs={2}>\n          <Paper className={classes.paper}>\n            <h4><Sidebar /></h4>\n          </Paper>\n        </Grid>\n        <Grid item xs={10}>\n          <Paper className={classes.paper}>\n            <List>\n              <h3>Word Embedding</h3>\n              <ul>\n                <li>Computers break everything down to numbers. What happens when a software inside a computer (like a ML algorithm) has to operate/ process\n                  a word? Simple, this word needs to be given to the computer as the only thing it can understand: as numbers.</li>\n                <li>In NLP, the most simple way to do this is by creating a vocabulary with a huge amount of words (100.000), and assigning a number to\n                  each word in the vocabulary.</li>\n                <li>The first word in our vocabulary (‘apple’ maybe) will be number 0. The second word (‘banana’) will be number 1, and so on up\n                  to number 99.998, the previous to last word (‘king’) and 999.999 being assigned to the last word (‘queen’).</li>\n                <li>Then we represent every word as a vector of length 100.000, where every single item is a zero except one of them, corresponding\n                  to the index of the number that the word is associated with.</li>\n                <br />\n                <img src={Activations} alt=\"Theata\" className=\"responsive2\" style={redesign} />\n                <br />\n                <li>This is called one-hot encoding for words.</li>\n                <li>The main thing is that word embeddings are vectors that represent words, so that similar meaning words have similar vectors.</li>\n                <li>The two most used Word embedding algorithms are Word2Vec and GloVe.</li>\n              </ul>\n              <br />\n\n              <div style={titles}>\n                <PrismCode\n                  code={childsFile}\n                  language=\"js\"\n                  plugins={[\"line-numbers\"]}\n                />\n              </div>\n              <br />\n\n              <h3></h3>\n              <div style={titles}>\n                <PrismCode\n                  code={childsFile}\n                  language=\"js\"\n                  plugins={[\"line-numbers\"]}\n                />\n              </div>\n              <br />\n\n              <h3></h3>\n              <div style={titles}>\n                <PrismCode\n                  code={childsFile}\n                  language=\"js\"\n                  plugins={[\"line-numbers\"]}\n                />\n              </div>\n              <br />\n\n              <h3></h3>\n              <div style={titles}>\n                <PrismCode\n                  code={childsFile}\n                  language=\"js\"\n                  plugins={[\"line-numbers\"]}\n                />\n              </div>\n              <br />\n\n              <h3></h3>\n              <div style={titles}>\n                <PrismCode\n                  code={childsFile}\n                  language=\"js\"\n                  plugins={[\"line-numbers\"]}\n                />\n              </div>\n            </List>\n          </Paper>\n        </Grid>\n      </Grid>\n    )\n  }\n}\n\n\nexport default (withStyles(styles)(WordEmbedding));\n"]},"metadata":{},"sourceType":"module"}