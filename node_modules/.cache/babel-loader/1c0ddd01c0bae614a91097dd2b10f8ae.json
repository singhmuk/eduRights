{"ast":null,"code":"import _classCallCheck from\"/home/mukeshs/Projects/edurights/node_modules/@babel/runtime/helpers/esm/classCallCheck\";import _createClass from\"/home/mukeshs/Projects/edurights/node_modules/@babel/runtime/helpers/esm/createClass\";import _possibleConstructorReturn from\"/home/mukeshs/Projects/edurights/node_modules/@babel/runtime/helpers/esm/possibleConstructorReturn\";import _getPrototypeOf from\"/home/mukeshs/Projects/edurights/node_modules/@babel/runtime/helpers/esm/getPrototypeOf\";import _inherits from\"/home/mukeshs/Projects/edurights/node_modules/@babel/runtime/helpers/esm/inherits\";import React,{Component}from'react';import Prism from\"prismjs\";import{Grid,Paper,withStyles,List}from\"@material-ui/core\";import'../../ReactJs/styles.css';import Sidebar from'../sidebar';import PrismCode from'../../ReactJs/prismCode';import NeuralKeras from'../../../assets/ML/perceptrons.png';var titles={backgroundColor:'#F0F8FF',padding:'1px',fontSize:'16px'};var redesign={height:200,width:500};var styles=function styles(theme){return{paper:{margin:theme.spacing(1),padding:theme.spacing(1)},smMargin:{margin:theme.spacing(1)},actionDiv:{textAlign:\"center\"}};};var tensorlow=\"\\nimport tensorflow as tf\\nimport pandas as pd\\n\\nCOLUMN_NAMES = [\\n        'SepalLength', \\n        'SepalWidth',\\n        'PetalLength', \\n        'PetalWidth', \\n        'Species'\\n        ]\\n\\n\\ntraining_dataset = pd.read_csv('iris_training.csv', names=COLUMN_NAMES, header=0)                 #Import training dataset\\ntrain_x = training_dataset.iloc[:, 0:4]\\ntrain_y = training_dataset.iloc[:, 4]\\n\\ntest_dataset = pd.read_csv('iris_test.csv', names=COLUMN_NAMES, header=0)\\ntest_x = test_dataset.iloc[:, 0:4]\\ntest_y = test_dataset.iloc[:, 4]\".trim();var eager=\"\\ntf.executing_eagerly()                                                                            #True\\n\\nx = [[2.]]\\ny = tf.matmul(x,x)\\nprint(y)\\n\".trim();var tensorObj=\"\\nx = tf.constant([[1, 2, 3, 4 ,5]])                                        \\ny = tf.ones((1,5))                                                       \\nz = tf.zeros((1,5))                                                       \\nq = tf.range(start=1, limit=6, delta=1)                                   \\n\\nprint(q)\\n\".trim();var rankDim=\"\\na=tf.constant(5)                                                          #Rank-0\\nb = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0])                                #Vector\\nc = tf.constant([[10,20],[30,40],[50,60],[70,80]])                        #Rank-2\\nd = tf.constant([[[10,20],[30,40],[50,60],[70,80]]])                      #Rank-3\\ne = tf.ones([1,2,3,4,5])                                                  #Rank-3\\n\".trim();var indexings=\"\\na = tf.constant([0,1,1,1,2,3,3,3,4,5,6,6])\\nb = a.numpy()                                                             #for indexing we need to convert tensor to numpy\\nb[2]\\n\".trim();var broadcasting=\"\\nx = tf.constant([1,2,3])\\ny = tf.constant(2)\\nz = tf.constant([2,2,2])\\n\\nprint(tf.multiply(x,2))\\nprint(x* y)\\nprint(x * z)\\n\\n\\n#2\\nx = tf.reshape(x,[3,1])\\ny = tf.range(1,5)\\nprint(x, y)\\nprint(tf.multiply(x,y))\\n\".trim();var specialTen=\"\\n#1 ragged_tensors\\nragged_list = [[1, 2, 3],[4, 5],[6]]\\nragged_tensor = tf.ragged.constant(ragged_list)\\nragged_tensor\\n\\n\\n#2 string_tensor\\nstring_tensor = tf.constant([\\\"With this\\\", \\\"code, I am\\\", \\\"creating a String Tensor\\\"])\\nstring_tensor\\n\\n\\n#3 sparse_tensor\\nsparse_tensor = tf.sparse.SparseTensor(indices=[[0, 0], [2, 2], [4, 4]], values=[25, 50, 100], dense_shape=[5, 5])\\nsparse_tensor\\n\\nctd = tf.sparse.to_dense(sparse_tensor)                                       #convert sparse tensors to dense\\nctd\\n\".trim();var graphs=\"\\nx = tf.constant(2)\\ny = tf.constant(5)\\nresult = x+y\\ntf.print(result)\\n\\nprint(tf.compat.v1.get_default_graph())                                                       #see generated graph\\n\\ng = tf.Graph()                                                                                #user define graph\\nuserdefault = tf.compat.v1.get_default_graph()                                                #user define default graph\\nprint(userdefault)\\n\".trim();var variables=\"\\n#1 constant\\nimport tensorflow as tf\\n\\nx = tf.constant([[1,2],[3,4]])\\ny = tf.add(x, 1)\\nprint(x * y)\\n\\nz = np.multiply(x, y)\\nprint(a.numpy)\\n\\n\\n#2 Variables \\nvar1 = tf.Variable([[1.2,2.1],[3.0,40.]])\\nt1 = tf.convert_to_tensor(var1)                        # varriable convert to tensor \\nprint(t1)    \\n\\n\\n#3 \\nvar1 = tf.Variable([12,30,40,50,60,70,80,90])\\n\\namax = tf.argmin(var1)\\namin = tf.argmax(var1)\\nrs = tf.reshape(var1, ([2,4]))\\n\\n\\n#4\\nmy_tensor = tf.random.uniform((5,5), 0,4)\\nvar1 = tf.Variable(initial_value = my_tensor)                                                 #create variable\\nprint(var1)\\n\\n\\n#5\\ntf.__version__\\nconst = tf.constant(10)\\nmat = tf.fill((5,5),10)\\nzeros = tf.zeros((5,5))\\nones = tf.ones((5,5))\\nrandm = tf.random.normal((4,4), mean=0, stddev=1.0)\\nrandu = tf.random.uniform((4,4), minval=0, maxval=1)\\nmyops = [const, mat, zeros, ones, randu]\\nprint(myops)\\n\\na = tf.constant([[2,3],[4,5]])\\na.get_shape()\\n\\n\\n#6\\nimport tensorflow.compat.v1 as tf\\ntf.disable_v2_behavior()\\n\\ntfph = tf.compat.v1.placeholder(tf.float32, shape=(None, 5))\\na = tf.compat.v1.placeholder(tf.float32, name='a')\\nb = tf.compat.v1.placeholder(tf.float32, name='b')\\nc = tf.add(a, b, name='c')\\nwith tf.Session() as sess:\\n    sess.run(c, feed_dict={a: 2.1, b: 1.9})\\n    print(tfph)\\n\".trim();var perceptropn=\"\\nnp.random.seed(101)\\ntf.random.set_seed(101)\\nrand_a = np.random.uniform(0, 100, (5,5))\\nrand_b = np.random.uniform(0, 100, (5,1))\\n\\n\\n#2\\nimport tensorflow.compat.v1 as tf\\ntf.disable_v2_behavior()\\n\\na = tf.compat.v1.placeholder(tf.float32)\\nb = tf.compat.v1.placeholder(tf.float32)\\nadd = a+b\\nmul = a*b\\n\\nwith tf.Session() as sess:\\n    result = sess.run(add, feed_dict={a:rand_a, b:rand_b})\\n    print(result)\\n\".trim();var neuralsnet=\"\\nnf = 1\\nndf = 3\\n\\nbis = tf.Variable(tf.zeros([ndf]))\\nwgt = tf.Variable(tf.random.normal([nf, ndf]))\\n\\n\\nimport tensorflow.compat.v1 as tf\\ntf.disable_v2_behavior()\\n\\nx = tf.compat.v1.placeholder(tf.float32,(None, nf))\\n\\nxw = tf.matmul(x, wgt)                                                                      #y = mx + b\\nz = tf.add(xw, b)\\n\\naf = tf.sigmoid(z)                                                                          #activation function\\n\\ninit = tf.compat.v1.global_variables_initializer()\\nwith tf.Session() as sess:\\n    sess.run(init)\\n    layer_out = sess.run(af, feed_dict={x:np.random.random([1,nf])})\\n\\nprint(layer_out)\\n\".trim();var regressions=\"\\nimport tensorflow.compat.v1 as tf\\n\\nnp.random.seed(101)\\ntf.random.set_seed(101)\\n\\n\\nx = np.linspace(0, 50, 50)                                                    # Generating random linear data\\ny = np.linspace(0, 50, 50)\\n  \\nx += np.random.uniform(-4, 4, 50)                                             # Adding noise to the random linear data\\ny += np.random.uniform(-4, 4, 50)\\n  \\nn = len(x) # Number of data points\\n\\nplt.scatter(x, y)\\nplt.xlabel('x')\\nplt.xlabel('y')\\nplt.title(\\\"Training Data\\\")\\nplt.show()\\n\\n\\ntf.disable_v2_behavior()\\n\\nX = tf.compat.v1.placeholder(\\\"float\\\")\\nY = tf.compat.v1.placeholder(\\\"float\\\")\\n\\nW = tf.Variable(np.random.randn(), name = \\\"W\\\")\\nb = tf.Variable(np.random.randn(), name = \\\"b\\\")\\n\\nlearning_rate = 0.01\\ntraining_epochs = 1000\\n\\n\\nwith tf.Session() as sess:                                                # Starting the Tensorflow Session\\n    sess.run(init)                                                        # Initializing the Variables\\n      \\n    for epoch in range(training_epochs):\\n        for (_x, _y) in zip(x, y):\\n            sess.run(optimizer, feed_dict = {X : _x, Y : _y})             # Feeding each data point into the optimizer\\n          \\n        \\n        if (epoch + 1) % 50 == 0:                                         # Displaying the result after every 50 epochs\\n            c = sess.run(cost, feed_dict = {X : x, Y : y})                # Calculating the cost a every epoch\\n            print((epoch + 1), c, sess.run(W), sess.run(b))\\n    \\n    training_cost = sess.run(cost, feed_dict ={X: x, Y: y})               # Storing values to be used outside the Session\\n    weight = sess.run(W)\\n    bias = sess.run(b)\\n\".trim();var TensorFlows=/*#__PURE__*/function(_Component){_inherits(TensorFlows,_Component);function TensorFlows(){_classCallCheck(this,TensorFlows);return _possibleConstructorReturn(this,_getPrototypeOf(TensorFlows).apply(this,arguments));}_createClass(TensorFlows,[{key:\"componentDidMount\",value:function componentDidMount(){setTimeout(function(){return Prism.highlightAll();},0);}},{key:\"render\",value:function render(){var classes=this.props.classes;return React.createElement(Grid,{container:true},React.createElement(Grid,{item:true,xs:2},React.createElement(Paper,{className:classes.paper},React.createElement(\"h4\",null,React.createElement(Sidebar,null)))),React.createElement(Grid,{item:true,xs:10},React.createElement(Paper,{className:classes.paper},React.createElement(List,null,React.createElement(\"h3\",null,\"TensorFlow\"),\"TensorFlow designed to implement ML and DL concepts in the easiest manner.\",React.createElement(\"br\",null),React.createElement(\"br\",null),\"TensorFlow uses for numerical computation and large-scale ML. TensorFlow bundles together a slew of ML and DL (neural networking) models algorithms and makes them useful by way of a common metaphor.\",React.createElement(\"br\",null),React.createElement(\"br\",null),React.createElement(\"b\",null,\"important features of TensorFlow:\"),React.createElement(\"ul\",null,React.createElement(\"li\",null,\"Defines, optimizes and calculates mathematical expressions easily with the help of multi-dimensional arrays (tensors).\"),React.createElement(\"li\",null,\"Support of deep neural networks and ML techniques.\"),React.createElement(\"li\",null,\"It includes a high scalable feature of computation with various data sets.\"),React.createElement(\"li\",null,\"Uses GPU computing, automating management. It also includes a unique feature of optimization of same memory and the data used.\")),React.createElement(\"br\",null),React.createElement(\"h3\",null,\"Eager Execution\"),\"Eager execution is an imperative, define-by-run interface where operations are executed immediately as they are called from Python. This makes it easier to get started with TensorFlow, and can make research and development more intuitive.\",React.createElement(\"div\",{style:titles},React.createElement(PrismCode,{code:eager,language:\"js\",plugins:[\"line-numbers\"]})),React.createElement(\"br\",null),React.createElement(\"h3\",null,\"Tensors\"),\"Tensors are TensorFlow\\u2019s multi-dimensional arrays with uniform type. They are very similar to NumPy arrays, and immutable.\",React.createElement(\"br\",null),React.createElement(\"br\",null),React.createElement(\"b\",null,\"Creating Tensor Objects:\"),React.createElement(\"ul\",null,React.createElement(\"li\",null,React.createElement(\"b\",null,\"tf.constant(): \")),React.createElement(\"li\",null,React.createElement(\"b\",null,\"tf.ones(): \"),\"Only consisting of 1s.\"),React.createElement(\"li\",null,React.createElement(\"b\",null,\"tf.zeros(): \"),\"Only consisting of 0s.\"),React.createElement(\"li\",null,React.createElement(\"b\",null,\"tf.range(): \"),\"To create Tensor objects.\")),React.createElement(\"div\",{style:titles},React.createElement(PrismCode,{code:tensorObj,language:\"js\",plugins:[\"line-numbers\"]})),React.createElement(\"i\",null,React.createElement(\"b\",null,\"N: \"),\"tf.ones and tf.zeros accepts the shape as the required argument since their element values are pre-determined.\"),React.createElement(\"br\",null),React.createElement(\"h3\",null,\"Rank System and Dimension\"),React.createElement(\"ul\",null,React.createElement(\"li\",null,React.createElement(\"b\",null,\"Rank-0 (Scalar) Tensor: \"),\"A tensor containing a single value and no axes.\"),React.createElement(\"li\",null,React.createElement(\"b\",null,\"Rank-1 (Vector) Tensor: \"),\"A tensor containing a list of values in a single axis.\"),React.createElement(\"li\",null,React.createElement(\"b\",null,\"Rank-2 Tensor: \"),\"A tensor containing 2-axes.\"),React.createElement(\"li\",null,React.createElement(\"b\",null,\"Rank-3 Tensor: \"),\"A tensor containing 3-axes.\"),React.createElement(\"li\",null,React.createElement(\"b\",null,\"Rank-4 Tensor: \"),\"4th dimension is space.\")),React.createElement(\"div\",{style:titles},React.createElement(PrismCode,{code:rankDim,language:\"js\",plugins:[\"line-numbers\"]})),React.createElement(\"br\",null),React.createElement(\"h3\",null,\"Operations with Tensors\"),React.createElement(\"ul\",null,React.createElement(\"li\",null,\"Indexing\"),React.createElement(\"li\",null,\"Addition\"),React.createElement(\"li\",null,\"Element-wise Multiplication\"),React.createElement(\"li\",null,\"Matrix Multiplication\"),React.createElement(\"li\",null,\"Finding the Maximum/ Minimum\"),React.createElement(\"li\",null,\"Finding the Index of the Max Element\"),React.createElement(\"li\",null,\"Computing Softmax Value\")),React.createElement(\"i\",null,\"Commas (,) are used to reach deeper levels.\"),React.createElement(\"div\",{style:titles},React.createElement(PrismCode,{code:indexings,language:\"js\",plugins:[\"line-numbers\"]})),React.createElement(\"br\",null),React.createElement(\"h3\",null,\"Broadcasting with Tensor\"),React.createElement(\"ul\",null,React.createElement(\"li\",null,\"Broadcasting concept is borrowed from Numpy broadcasting.\"),React.createElement(\"li\",null,\"Broadcasting is about bringing the tensors of different dimensions/ shape to the compatible shape such that arithmetic operations can be performed on them.\"),React.createElement(\"li\",null,\"In broadcasting, the smaller array is found, the new axes are added as per the larger array and data is added appropriately to the transformed array.\")),React.createElement(\"div\",{style:titles},React.createElement(PrismCode,{code:broadcasting,language:\"js\",plugins:[\"line-numbers\"]})),React.createElement(\"br\",null),React.createElement(\"h3\",null,\"Special Types of Tensors\"),\"We tend to generate Tensors in a rectangular shape and store numerical values as elements. However, TensorFlow also supports irregular, or specialized, Tensor types, which are:\",React.createElement(\"ul\",null,React.createElement(\"li\",null,React.createElement(\"b\",null,\"Ragged Tensors: \"),\"Are tensors with different numbers of elements along the size axis.\"),React.createElement(\"li\",null,React.createElement(\"b\",null,\"String Tensors: \"),\"Are stores string objects. We can build a String Tensor just as you create a regular Tensor object. But, we pass string objects as elements instead of numerical objects.\"),React.createElement(\"li\",null,React.createElement(\"b\",null,\"Sparse Tensors: \"),\"Are rectangular Tensors for sparse data. When we have holes (Null values) in our data, Sparse Tensors are to-go objects.\")),React.createElement(\"div\",{style:titles},React.createElement(PrismCode,{code:specialTen,language:\"js\",plugins:[\"line-numbers\"]})),React.createElement(\"h3\",null,\"Variables & Placeholders\"),\"Two way to initialize varriabble.\",React.createElement(\"ul\",null,React.createElement(\"li\",null,React.createElement(\"b\",null,\"constant: \"),\"initialize constant variable.\"),React.createElement(\"li\",null,React.createElement(\"b\",null,\"Variable: \"))),React.createElement(\"br\",null),React.createElement(\"b\",null,\"Methods:\"),React.createElement(\"ul\",null,React.createElement(\"li\",null,React.createElement(\"b\",null,\"argmin(): \"),\"Show smallest value index number.\"),React.createElement(\"li\",null,React.createElement(\"b\",null,\"argmax(): \"),\"Show large value index number.\")),React.createElement(\"div\",{style:titles},React.createElement(PrismCode,{code:variables,language:\"js\",plugins:[\"line-numbers\"]})),React.createElement(\"br\",null),React.createElement(\"h3\",null,\"Import training dataset\"),React.createElement(\"div\",{style:titles},React.createElement(PrismCode,{code:tensorlow,language:\"js\",plugins:[\"line-numbers\"]})),React.createElement(\"br\",null),React.createElement(\"h3\",null,\"Tensorflow Graph\"),React.createElement(\"div\",{style:titles},React.createElement(PrismCode,{code:graphs,language:\"js\",plugins:[\"line-numbers\"]})),React.createElement(\"br\",null),React.createElement(\"h3\",null,\"Perceptron\"),\"Perceptron is an algorithm that, given an inputs features, outputs either 1 or 0.\",React.createElement(\"br\",null),React.createElement(\"img\",{src:NeuralKeras,alt:\"Theata\",className:\"responsive2\",style:redesign}),React.createElement(\"div\",{style:titles},React.createElement(PrismCode,{code:perceptropn,language:\"js\",plugins:[\"line-numbers\"]})),React.createElement(\"br\",null),React.createElement(\"h3\",null,\"Neural Network\"),React.createElement(\"ul\",null,React.createElement(\"li\",null,\"Neural networks, a beautiful biologically-inspired programming paradigm which enables a computer to learn from observational data.\"),React.createElement(\"li\",null,\"DL, a powerful set of techniques for learning in neural networks.\"),React.createElement(\"li\",null,\"Neural networks and DL currently provide the best solutions to many problems in image recognition, speech recognition, and natural language processing.\")),React.createElement(\"br\",null),React.createElement(\"br\",null),React.createElement(\"div\",{style:titles},React.createElement(PrismCode,{code:neuralsnet,language:\"js\",plugins:[\"line-numbers\"]})),React.createElement(\"br\",null),React.createElement(\"h3\",null,\"Regression\"),React.createElement(\"div\",{style:titles},React.createElement(PrismCode,{code:regressions,language:\"js\",plugins:[\"line-numbers\"]})),React.createElement(\"br\",null)))));}}]);return TensorFlows;}(Component);export default withStyles(styles)(TensorFlows);","map":{"version":3,"sources":["/home/mukeshs/Projects/edurights/src/components/angularjs/deepAngularjs/tensorflow.js"],"names":["React","Component","Prism","Grid","Paper","withStyles","List","Sidebar","PrismCode","NeuralKeras","titles","backgroundColor","padding","fontSize","redesign","height","width","styles","theme","paper","margin","spacing","smMargin","actionDiv","textAlign","tensorlow","trim","eager","tensorObj","rankDim","indexings","broadcasting","specialTen","graphs","variables","perceptropn","neuralsnet","regressions","TensorFlows","setTimeout","highlightAll","classes","props"],"mappings":"6kBAAA,MAAOA,CAAAA,KAAP,EAAgBC,SAAhB,KAAiC,OAAjC,CACA,MAAOC,CAAAA,KAAP,KAAkB,SAAlB,CACA,OAASC,IAAT,CAAeC,KAAf,CAAsBC,UAAtB,CAAkCC,IAAlC,KAA8C,mBAA9C,CAEA,MAAO,0BAAP,CACA,MAAOC,CAAAA,OAAP,KAAoB,YAApB,CACA,MAAOC,CAAAA,SAAP,KAAsB,yBAAtB,CAEA,MAAOC,CAAAA,WAAP,KAAwB,oCAAxB,CAGA,GAAMC,CAAAA,MAAM,CAAG,CAAEC,eAAe,CAAE,SAAnB,CAA8BC,OAAO,CAAE,KAAvC,CAA8CC,QAAQ,CAAE,MAAxD,CAAf,CAEA,GAAMC,CAAAA,QAAQ,CAAG,CACfC,MAAM,CAAE,GADO,CAEfC,KAAK,CAAE,GAFQ,CAAjB,CAKA,GAAMC,CAAAA,MAAM,CAAG,QAATA,CAAAA,MAAS,CAAAC,KAAK,QAAK,CACvBC,KAAK,CAAE,CACLC,MAAM,CAAEF,KAAK,CAACG,OAAN,CAAc,CAAd,CADH,CAELT,OAAO,CAAEM,KAAK,CAACG,OAAN,CAAc,CAAd,CAFJ,CADgB,CAKvBC,QAAQ,CAAE,CACRF,MAAM,CAAEF,KAAK,CAACG,OAAN,CAAc,CAAd,CADA,CALa,CAQvBE,SAAS,CAAE,CACTC,SAAS,CAAE,QADF,CARY,CAAL,EAApB,CAaA,GAAMC,CAAAA,SAAS,CAAG,uiBAmBgBC,IAnBhB,EAAlB,CAqBA,GAAMC,CAAAA,KAAK,CAAG,0JAMZD,IANY,EAAd,CAQA,GAAME,CAAAA,SAAS,CAAG,gUAOhBF,IAPgB,EAAlB,CASA,GAAMG,CAAAA,OAAO,CAAG,oaAMdH,IANc,EAAhB,CAQA,GAAMI,CAAAA,SAAS,CAAG,mLAIhBJ,IAJgB,EAAlB,CAMA,GAAMK,CAAAA,YAAY,CAAG,6NAenBL,IAfmB,EAArB,CAiBA,GAAMM,CAAAA,UAAU,CAAG,+gBAkBjBN,IAlBiB,EAAnB,CAoBA,GAAMO,CAAAA,MAAM,CAAG,qcAWbP,IAXa,EAAf,CAaA,GAAMQ,CAAAA,SAAS,CAAG,qyCA0DhBR,IA1DgB,EAAlB,CA4DA,GAAMS,CAAAA,WAAW,CAAG,uaAmBlBT,IAnBkB,EAApB,CAqBA,GAAMU,CAAAA,UAAU,CAAG,spBAwBjBV,IAxBiB,EAAnB,CA0BA,GAAMW,CAAAA,WAAW,CAAG,6qDAiDlBX,IAjDkB,EAApB,C,GAoDMY,CAAAA,W,+SACgB,CAClBC,UAAU,CAAC,iBAAMrC,CAAAA,KAAK,CAACsC,YAAN,EAAN,EAAD,CAA6B,CAA7B,CAAV,CACD,C,uCACQ,IACCC,CAAAA,OADD,CACa,KAAKC,KADlB,CACCD,OADD,CAEP,MACE,qBAAC,IAAD,EAAM,SAAS,KAAf,EACE,oBAAC,IAAD,EAAM,IAAI,KAAV,CAAW,EAAE,CAAE,CAAf,EACE,oBAAC,KAAD,EAAO,SAAS,CAAEA,OAAO,CAACtB,KAA1B,EACE,8BAAI,oBAAC,OAAD,MAAJ,CADF,CADF,CADF,CAME,oBAAC,IAAD,EAAM,IAAI,KAAV,CAAW,EAAE,CAAE,EAAf,EACE,oBAAC,KAAD,EAAO,SAAS,CAAEsB,OAAO,CAACtB,KAA1B,EACE,oBAAC,IAAD,MACE,2CADF,8EAGE,8BAHF,CAIE,8BAJF,0MAQE,8BARF,CASE,8BATF,CAWE,iEAXF,CAYE,8BACE,uJADF,CAGE,mFAHF,CAIE,2GAJF,CAKE,+JALF,CAZF,CAoBE,8BApBF,CAsBE,gDAtBF,kPAyBE,2BAAK,KAAK,CAAET,MAAZ,EACE,oBAAC,SAAD,EACE,IAAI,CAAEiB,KADR,CAEE,QAAQ,CAAC,IAFX,CAGE,OAAO,CAAE,CAAC,cAAD,CAHX,EADF,CAzBF,CAgCE,8BAhCF,CAkCE,wCAlCF,mIAoCE,8BApCF,CAqCE,8BArCF,CAsCE,wDAtCF,CAuCE,8BACE,8BAAI,+CAAJ,CADF,CAEE,8BAAI,2CAAJ,0BAFF,CAGE,8BAAI,4CAAJ,0BAHF,CAIE,8BAAI,4CAAJ,6BAJF,CAvCF,CA6CE,2BAAK,KAAK,CAAEjB,MAAZ,EACE,oBAAC,SAAD,EACE,IAAI,CAAEkB,SADR,CAEE,QAAQ,CAAC,IAFX,CAGE,OAAO,CAAE,CAAC,cAAD,CAHX,EADF,CA7CF,CAoDE,6BAAG,mCAAH,kHApDF,CAqDE,8BArDF,CAuDE,0DAvDF,CAwDE,8BACE,8BAAI,wDAAJ,mDADF,CAEE,8BAAI,wDAAJ,0DAFF,CAGE,8BAAI,+CAAJ,+BAHF,CAIE,8BAAI,+CAAJ,+BAJF,CAKE,8BAAI,+CAAJ,2BALF,CAxDF,CA+DE,2BAAK,KAAK,CAAElB,MAAZ,EACE,oBAAC,SAAD,EACE,IAAI,CAAEmB,OADR,CAEE,QAAQ,CAAC,IAFX,CAGE,OAAO,CAAE,CAAC,cAAD,CAHX,EADF,CA/DF,CAsEE,8BAtEF,CAwEE,wDAxEF,CAyEE,8BACE,yCADF,CAEE,yCAFF,CAGE,4DAHF,CAIE,sDAJF,CAKE,6DALF,CAME,qEANF,CAOE,wDAPF,CAzEF,CAkFE,2EAlFF,CAmFE,2BAAK,KAAK,CAAEnB,MAAZ,EACE,oBAAC,SAAD,EACE,IAAI,CAAEoB,SADR,CAEE,QAAQ,CAAC,IAFX,CAGE,OAAO,CAAE,CAAC,cAAD,CAHX,EADF,CAnFF,CA0FE,8BA1FF,CA4FE,yDA5FF,CA6FE,8BACE,0FADF,CAEE,4LAFF,CAIE,sLAJF,CA7FF,CAqGE,2BAAK,KAAK,CAAEpB,MAAZ,EACE,oBAAC,SAAD,EACE,IAAI,CAAEqB,YADR,CAEE,QAAQ,CAAC,IAFX,CAGE,OAAO,CAAE,CAAC,cAAD,CAHX,EADF,CArGF,CA4GE,8BA5GF,CA8GE,yDA9GF,oLAiHE,8BACE,8BAAI,gDAAJ,uEADF,CAEE,8BAAI,gDAAJ,6KAFF,CAIE,8BAAI,gDAAJ,4HAJF,CAjHF,CAwHE,2BAAK,KAAK,CAAErB,MAAZ,EACE,oBAAC,SAAD,EACE,IAAI,CAAEsB,UADR,CAEE,QAAQ,CAAC,IAFX,CAGE,OAAO,CAAE,CAAC,cAAD,CAHX,EADF,CAxHF,CAgIE,yDAhIF,qCAkIE,8BACE,8BAAI,0CAAJ,iCADF,CAEE,8BAAI,0CAAJ,CAFF,CAlIF,CAsIE,8BAtIF,CAuIE,wCAvIF,CAwIE,8BACE,8BAAI,0CAAJ,qCADF,CAEE,8BAAI,0CAAJ,kCAFF,CAxIF,CA4IE,2BAAK,KAAK,CAAEtB,MAAZ,EACE,oBAAC,SAAD,EACE,IAAI,CAAEwB,SADR,CAEE,QAAQ,CAAC,IAFX,CAGE,OAAO,CAAE,CAAC,cAAD,CAHX,EADF,CA5IF,CAmJE,8BAnJF,CAqJE,wDArJF,CAsJE,2BAAK,KAAK,CAAExB,MAAZ,EACE,oBAAC,SAAD,EACE,IAAI,CAAEe,SADR,CAEE,QAAQ,CAAC,IAFX,CAGE,OAAO,CAAE,CAAC,cAAD,CAHX,EADF,CAtJF,CA6JE,8BA7JF,CA+JE,iDA/JF,CAgKE,2BAAK,KAAK,CAAEf,MAAZ,EACE,oBAAC,SAAD,EACE,IAAI,CAAEuB,MADR,CAEE,QAAQ,CAAC,IAFX,CAGE,OAAO,CAAE,CAAC,cAAD,CAHX,EADF,CAhKF,CAuKE,8BAvKF,CAyKE,2CAzKF,qFA2KE,8BA3KF,CA4KE,2BAAK,GAAG,CAAExB,WAAV,CAAuB,GAAG,CAAC,QAA3B,CAAoC,SAAS,CAAC,aAA9C,CAA4D,KAAK,CAAEK,QAAnE,EA5KF,CA6KE,2BAAK,KAAK,CAAEJ,MAAZ,EACE,oBAAC,SAAD,EACE,IAAI,CAAEyB,WADR,CAEE,QAAQ,CAAC,IAFX,CAGE,OAAO,CAAE,CAAC,cAAD,CAHX,EADF,CA7KF,CAoLE,8BApLF,CAsLE,+CAtLF,CAuLE,8BACE,mKADF,CAEE,kGAFF,CAGE,wLAHF,CAvLF,CA6LE,8BA7LF,CA8LE,8BA9LF,CAgME,2BAAK,KAAK,CAAEzB,MAAZ,EACE,oBAAC,SAAD,EACE,IAAI,CAAE0B,UADR,CAEE,QAAQ,CAAC,IAFX,CAGE,OAAO,CAAE,CAAC,cAAD,CAHX,EADF,CAhMF,CAuME,8BAvMF,CAyME,2CAzMF,CA0ME,2BAAK,KAAK,CAAE1B,MAAZ,EACE,oBAAC,SAAD,EACE,IAAI,CAAE2B,WADR,CAEE,QAAQ,CAAC,IAFX,CAGE,OAAO,CAAE,CAAC,cAAD,CAHX,EADF,CA1MF,CAiNE,8BAjNF,CADF,CADF,CANF,CADF,CAgOD,C,yBAtOuBpC,S,EA0O1B,cAAgBI,CAAAA,UAAU,CAACY,MAAD,CAAV,CAAmBqB,WAAnB,CAAhB","sourcesContent":["import React, { Component } from 'react';\nimport Prism from \"prismjs\"\nimport { Grid, Paper, withStyles, List } from \"@material-ui/core\";\n\nimport '../../ReactJs/styles.css'\nimport Sidebar from '../sidebar';\nimport PrismCode from '../../ReactJs/prismCode';\n\nimport NeuralKeras from '../../../assets/ML/perceptrons.png'\n\n\nconst titles = { backgroundColor: '#F0F8FF', padding: '1px', fontSize: '16px' }\n\nconst redesign = {\n  height: 200,\n  width: 500\n}\n\nconst styles = theme => ({\n  paper: {\n    margin: theme.spacing(1),\n    padding: theme.spacing(1)\n  },\n  smMargin: {\n    margin: theme.spacing(1)\n  },\n  actionDiv: {\n    textAlign: \"center\"\n  }\n})\n\nconst tensorlow = `\nimport tensorflow as tf\nimport pandas as pd\n\nCOLUMN_NAMES = [\n        'SepalLength', \n        'SepalWidth',\n        'PetalLength', \n        'PetalWidth', \n        'Species'\n        ]\n\n\ntraining_dataset = pd.read_csv('iris_training.csv', names=COLUMN_NAMES, header=0)                 #Import training dataset\ntrain_x = training_dataset.iloc[:, 0:4]\ntrain_y = training_dataset.iloc[:, 4]\n\ntest_dataset = pd.read_csv('iris_test.csv', names=COLUMN_NAMES, header=0)\ntest_x = test_dataset.iloc[:, 0:4]\ntest_y = test_dataset.iloc[:, 4]`.trim();\n\nconst eager = `\ntf.executing_eagerly()                                                                            #True\n\nx = [[2.]]\ny = tf.matmul(x,x)\nprint(y)\n`.trim();\n\nconst tensorObj = `\nx = tf.constant([[1, 2, 3, 4 ,5]])                                        \ny = tf.ones((1,5))                                                       \nz = tf.zeros((1,5))                                                       \nq = tf.range(start=1, limit=6, delta=1)                                   \n\nprint(q)\n`.trim();\n\nconst rankDim = `\na=tf.constant(5)                                                          #Rank-0\nb = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0])                                #Vector\nc = tf.constant([[10,20],[30,40],[50,60],[70,80]])                        #Rank-2\nd = tf.constant([[[10,20],[30,40],[50,60],[70,80]]])                      #Rank-3\ne = tf.ones([1,2,3,4,5])                                                  #Rank-3\n`.trim();\n\nconst indexings = `\na = tf.constant([0,1,1,1,2,3,3,3,4,5,6,6])\nb = a.numpy()                                                             #for indexing we need to convert tensor to numpy\nb[2]\n`.trim();\n\nconst broadcasting = `\nx = tf.constant([1,2,3])\ny = tf.constant(2)\nz = tf.constant([2,2,2])\n\nprint(tf.multiply(x,2))\nprint(x* y)\nprint(x * z)\n\n\n#2\nx = tf.reshape(x,[3,1])\ny = tf.range(1,5)\nprint(x, y)\nprint(tf.multiply(x,y))\n`.trim();\n\nconst specialTen = `\n#1 ragged_tensors\nragged_list = [[1, 2, 3],[4, 5],[6]]\nragged_tensor = tf.ragged.constant(ragged_list)\nragged_tensor\n\n\n#2 string_tensor\nstring_tensor = tf.constant([\"With this\", \"code, I am\", \"creating a String Tensor\"])\nstring_tensor\n\n\n#3 sparse_tensor\nsparse_tensor = tf.sparse.SparseTensor(indices=[[0, 0], [2, 2], [4, 4]], values=[25, 50, 100], dense_shape=[5, 5])\nsparse_tensor\n\nctd = tf.sparse.to_dense(sparse_tensor)                                       #convert sparse tensors to dense\nctd\n`.trim();\n\nconst graphs = `\nx = tf.constant(2)\ny = tf.constant(5)\nresult = x+y\ntf.print(result)\n\nprint(tf.compat.v1.get_default_graph())                                                       #see generated graph\n\ng = tf.Graph()                                                                                #user define graph\nuserdefault = tf.compat.v1.get_default_graph()                                                #user define default graph\nprint(userdefault)\n`.trim();\n\nconst variables = `\n#1 constant\nimport tensorflow as tf\n\nx = tf.constant([[1,2],[3,4]])\ny = tf.add(x, 1)\nprint(x * y)\n\nz = np.multiply(x, y)\nprint(a.numpy)\n\n\n#2 Variables \nvar1 = tf.Variable([[1.2,2.1],[3.0,40.]])\nt1 = tf.convert_to_tensor(var1)                        # varriable convert to tensor \nprint(t1)    \n\n\n#3 \nvar1 = tf.Variable([12,30,40,50,60,70,80,90])\n\namax = tf.argmin(var1)\namin = tf.argmax(var1)\nrs = tf.reshape(var1, ([2,4]))\n\n\n#4\nmy_tensor = tf.random.uniform((5,5), 0,4)\nvar1 = tf.Variable(initial_value = my_tensor)                                                 #create variable\nprint(var1)\n\n\n#5\ntf.__version__\nconst = tf.constant(10)\nmat = tf.fill((5,5),10)\nzeros = tf.zeros((5,5))\nones = tf.ones((5,5))\nrandm = tf.random.normal((4,4), mean=0, stddev=1.0)\nrandu = tf.random.uniform((4,4), minval=0, maxval=1)\nmyops = [const, mat, zeros, ones, randu]\nprint(myops)\n\na = tf.constant([[2,3],[4,5]])\na.get_shape()\n\n\n#6\nimport tensorflow.compat.v1 as tf\ntf.disable_v2_behavior()\n\ntfph = tf.compat.v1.placeholder(tf.float32, shape=(None, 5))\na = tf.compat.v1.placeholder(tf.float32, name='a')\nb = tf.compat.v1.placeholder(tf.float32, name='b')\nc = tf.add(a, b, name='c')\nwith tf.Session() as sess:\n    sess.run(c, feed_dict={a: 2.1, b: 1.9})\n    print(tfph)\n`.trim();\n\nconst perceptropn = `\nnp.random.seed(101)\ntf.random.set_seed(101)\nrand_a = np.random.uniform(0, 100, (5,5))\nrand_b = np.random.uniform(0, 100, (5,1))\n\n\n#2\nimport tensorflow.compat.v1 as tf\ntf.disable_v2_behavior()\n\na = tf.compat.v1.placeholder(tf.float32)\nb = tf.compat.v1.placeholder(tf.float32)\nadd = a+b\nmul = a*b\n\nwith tf.Session() as sess:\n    result = sess.run(add, feed_dict={a:rand_a, b:rand_b})\n    print(result)\n`.trim();\n\nconst neuralsnet = `\nnf = 1\nndf = 3\n\nbis = tf.Variable(tf.zeros([ndf]))\nwgt = tf.Variable(tf.random.normal([nf, ndf]))\n\n\nimport tensorflow.compat.v1 as tf\ntf.disable_v2_behavior()\n\nx = tf.compat.v1.placeholder(tf.float32,(None, nf))\n\nxw = tf.matmul(x, wgt)                                                                      #y = mx + b\nz = tf.add(xw, b)\n\naf = tf.sigmoid(z)                                                                          #activation function\n\ninit = tf.compat.v1.global_variables_initializer()\nwith tf.Session() as sess:\n    sess.run(init)\n    layer_out = sess.run(af, feed_dict={x:np.random.random([1,nf])})\n\nprint(layer_out)\n`.trim();\n\nconst regressions = `\nimport tensorflow.compat.v1 as tf\n\nnp.random.seed(101)\ntf.random.set_seed(101)\n\n\nx = np.linspace(0, 50, 50)                                                    # Generating random linear data\ny = np.linspace(0, 50, 50)\n  \nx += np.random.uniform(-4, 4, 50)                                             # Adding noise to the random linear data\ny += np.random.uniform(-4, 4, 50)\n  \nn = len(x) # Number of data points\n\nplt.scatter(x, y)\nplt.xlabel('x')\nplt.xlabel('y')\nplt.title(\"Training Data\")\nplt.show()\n\n\ntf.disable_v2_behavior()\n\nX = tf.compat.v1.placeholder(\"float\")\nY = tf.compat.v1.placeholder(\"float\")\n\nW = tf.Variable(np.random.randn(), name = \"W\")\nb = tf.Variable(np.random.randn(), name = \"b\")\n\nlearning_rate = 0.01\ntraining_epochs = 1000\n\n\nwith tf.Session() as sess:                                                # Starting the Tensorflow Session\n    sess.run(init)                                                        # Initializing the Variables\n      \n    for epoch in range(training_epochs):\n        for (_x, _y) in zip(x, y):\n            sess.run(optimizer, feed_dict = {X : _x, Y : _y})             # Feeding each data point into the optimizer\n          \n        \n        if (epoch + 1) % 50 == 0:                                         # Displaying the result after every 50 epochs\n            c = sess.run(cost, feed_dict = {X : x, Y : y})                # Calculating the cost a every epoch\n            print((epoch + 1), c, sess.run(W), sess.run(b))\n    \n    training_cost = sess.run(cost, feed_dict ={X: x, Y: y})               # Storing values to be used outside the Session\n    weight = sess.run(W)\n    bias = sess.run(b)\n`.trim();\n\n\nclass TensorFlows extends Component {\n  componentDidMount() {\n    setTimeout(() => Prism.highlightAll(), 0)\n  }\n  render() {\n    const { classes } = this.props;\n    return (\n      <Grid container>\n        <Grid item xs={2}>\n          <Paper className={classes.paper}>\n            <h4><Sidebar /></h4>\n          </Paper>\n        </Grid>\n        <Grid item xs={10}>\n          <Paper className={classes.paper}>\n            <List>\n              <h3>TensorFlow</h3>\n              TensorFlow designed to implement ML and DL concepts in the easiest manner.\n              <br />\n              <br />\n              TensorFlow uses for numerical computation and large-scale ML. TensorFlow bundles\n              together a slew of ML and DL (neural networking) models algorithms and makes them\n              useful by way of a common metaphor.\n              <br />\n              <br />\n\n              <b>important features of TensorFlow:</b>\n              <ul>\n                <li>Defines, optimizes and calculates mathematical expressions easily with the help of\n                  multi-dimensional arrays (tensors).</li>\n                <li>Support of deep neural networks and ML techniques.</li>\n                <li>It includes a high scalable feature of computation with various data sets.</li>\n                <li>Uses GPU computing, automating management. It also includes a unique feature of optimization of same\n                  memory and the data used.</li>\n              </ul>\n              <br />\n\n              <h3>Eager Execution</h3>\n              Eager execution is an imperative, define-by-run interface where operations are executed immediately as they are called from Python.\n              This makes it easier to get started with TensorFlow, and can make research and development more intuitive.\n              <div style={titles}>\n                <PrismCode\n                  code={eager}\n                  language=\"js\"\n                  plugins={[\"line-numbers\"]}\n                />\n              </div>\n              <br />\n\n              <h3>Tensors</h3>\n              Tensors are TensorFlowâ€™s multi-dimensional arrays with uniform type. They are very similar to NumPy arrays, and immutable.\n              <br />\n              <br />\n              <b>Creating Tensor Objects:</b>\n              <ul>\n                <li><b>tf.constant(): </b></li>\n                <li><b>tf.ones(): </b>Only consisting of 1s.</li>\n                <li><b>tf.zeros(): </b>Only consisting of 0s.</li>\n                <li><b>tf.range(): </b>To create Tensor objects.</li>\n              </ul>\n              <div style={titles}>\n                <PrismCode\n                  code={tensorObj}\n                  language=\"js\"\n                  plugins={[\"line-numbers\"]}\n                />\n              </div>\n              <i><b>N: </b>tf.ones and tf.zeros accepts the shape as the required argument since their element values are pre-determined.</i>\n              <br />\n\n              <h3>Rank System and Dimension</h3>\n              <ul>\n                <li><b>Rank-0 (Scalar) Tensor: </b>A tensor containing a single value and no axes.</li>\n                <li><b>Rank-1 (Vector) Tensor: </b>A tensor containing a list of values in a single axis.</li>\n                <li><b>Rank-2 Tensor: </b>A tensor containing 2-axes.</li>\n                <li><b>Rank-3 Tensor: </b>A tensor containing 3-axes.</li>\n                <li><b>Rank-4 Tensor: </b>4th dimension is space.</li>\n              </ul>\n              <div style={titles}>\n                <PrismCode\n                  code={rankDim}\n                  language=\"js\"\n                  plugins={[\"line-numbers\"]}\n                />\n              </div>\n              <br />\n\n              <h3>Operations with Tensors</h3>\n              <ul>\n                <li>Indexing</li>\n                <li>Addition</li>\n                <li>Element-wise Multiplication</li>\n                <li>Matrix Multiplication</li>\n                <li>Finding the Maximum/ Minimum</li>\n                <li>Finding the Index of the Max Element</li>\n                <li>Computing Softmax Value</li>\n              </ul>\n              <i>Commas (,) are used to reach deeper levels.</i>\n              <div style={titles}>\n                <PrismCode\n                  code={indexings}\n                  language=\"js\"\n                  plugins={[\"line-numbers\"]}\n                />\n              </div>\n              <br />\n\n              <h3>Broadcasting with Tensor</h3>\n              <ul>\n                <li>Broadcasting concept is borrowed from Numpy broadcasting.</li>\n                <li>Broadcasting is about bringing the tensors of different dimensions/ shape to the compatible shape such that arithmetic operations\n                  can be performed on them.</li>\n                <li>In broadcasting, the smaller array is found, the new axes are added as per the larger array and data is added appropriately to\n                  the transformed array.</li>\n              </ul>\n\n              <div style={titles}>\n                <PrismCode\n                  code={broadcasting}\n                  language=\"js\"\n                  plugins={[\"line-numbers\"]}\n                />\n              </div>\n              <br />\n\n              <h3>Special Types of Tensors</h3>\n              We tend to generate Tensors in a rectangular shape and store numerical values as elements. However, TensorFlow also supports irregular, or specialized,\n              Tensor types, which are:\n              <ul>\n                <li><b>Ragged Tensors: </b>Are tensors with different numbers of elements along the size axis.</li>\n                <li><b>String Tensors: </b>Are stores string objects. We can build a String Tensor just as you create\n                  a regular Tensor object. But, we pass string objects as elements instead of numerical objects.</li>\n                <li><b>Sparse Tensors: </b>Are rectangular Tensors for sparse data. When we have holes (Null values) in our data, Sparse Tensors\n                  are to-go objects.</li>\n              </ul>\n              <div style={titles}>\n                <PrismCode\n                  code={specialTen}\n                  language=\"js\"\n                  plugins={[\"line-numbers\"]}\n                />\n              </div>\n\n              <h3>Variables & Placeholders</h3>\n              Two way to initialize varriabble.\n              <ul>\n                <li><b>constant: </b>initialize constant variable.</li>\n                <li><b>Variable: </b></li>\n              </ul>\n              <br />\n              <b>Methods:</b>\n              <ul>\n                <li><b>argmin(): </b>Show smallest value index number.</li>\n                <li><b>argmax(): </b>Show large value index number.</li>\n              </ul>\n              <div style={titles}>\n                <PrismCode\n                  code={variables}\n                  language=\"js\"\n                  plugins={[\"line-numbers\"]}\n                />\n              </div>\n              <br />\n\n              <h3>Import training dataset</h3>\n              <div style={titles}>\n                <PrismCode\n                  code={tensorlow}\n                  language=\"js\"\n                  plugins={[\"line-numbers\"]}\n                />\n              </div>\n              <br />\n\n              <h3>Tensorflow Graph</h3>\n              <div style={titles}>\n                <PrismCode\n                  code={graphs}\n                  language=\"js\"\n                  plugins={[\"line-numbers\"]}\n                />\n              </div>\n              <br />\n\n              <h3>Perceptron</h3>\n              Perceptron is an algorithm that, given an inputs features, outputs either 1 or 0.\n              <br />\n              <img src={NeuralKeras} alt=\"Theata\" className=\"responsive2\" style={redesign} />\n              <div style={titles}>\n                <PrismCode\n                  code={perceptropn}\n                  language=\"js\"\n                  plugins={[\"line-numbers\"]}\n                />\n              </div>\n              <br />\n\n              <h3>Neural Network</h3>\n              <ul>\n                <li>Neural networks, a beautiful biologically-inspired programming paradigm which enables a computer to learn from observational data.</li>\n                <li>DL, a powerful set of techniques for learning in neural networks.</li>\n                <li>Neural networks and DL currently provide the best solutions to many problems in image recognition, speech recognition,\n                  and natural language processing.</li>\n              </ul>\n              <br />\n              <br />\n\n              <div style={titles}>\n                <PrismCode\n                  code={neuralsnet}\n                  language=\"js\"\n                  plugins={[\"line-numbers\"]}\n                />\n              </div>\n              <br />\n\n              <h3>Regression</h3>\n              <div style={titles}>\n                <PrismCode\n                  code={regressions}\n                  language=\"js\"\n                  plugins={[\"line-numbers\"]}\n                />\n              </div>\n              <br />\n            </List>\n          </Paper>\n        </Grid>\n      </Grid>\n    )\n  }\n}\n\n\nexport default (withStyles(styles)(TensorFlows));\n"]},"metadata":{},"sourceType":"module"}