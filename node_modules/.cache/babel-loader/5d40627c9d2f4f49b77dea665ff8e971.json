{"ast":null,"code":"import _classCallCheck from\"/home/mukeshs/Projects/edurights/node_modules/@babel/runtime/helpers/esm/classCallCheck\";import _createClass from\"/home/mukeshs/Projects/edurights/node_modules/@babel/runtime/helpers/esm/createClass\";import _possibleConstructorReturn from\"/home/mukeshs/Projects/edurights/node_modules/@babel/runtime/helpers/esm/possibleConstructorReturn\";import _getPrototypeOf from\"/home/mukeshs/Projects/edurights/node_modules/@babel/runtime/helpers/esm/getPrototypeOf\";import _inherits from\"/home/mukeshs/Projects/edurights/node_modules/@babel/runtime/helpers/esm/inherits\";import React,{Component}from'react';import Prism from\"prismjs\";import{Grid,Paper,withStyles,List}from\"@material-ui/core\";import'../../ReactJs/styles.css';import Sidebar from'../sidebar';import PrismCode from'../../ReactJs/prismCode';import Equations from'../../../assets/ML/oneHotEncodung.png';var titles={backgroundColor:'#F0F8FF',padding:'1px',fontSize:'16px'};var styles=function styles(theme){return{paper:{margin:theme.spacing(1),padding:theme.spacing(1)},smMargin:{margin:theme.spacing(1)},actionDiv:{textAlign:\"center\"}};};var redesign={height:350,width:600};var cluster=\"\\nimport pandas as pd\\nimport matplotlib.pyplot as plt\\n%matplotlib inline\\n\\ndf = pd.read_csv(\\\"carprices.csv\\\")\\n\\nnewPlt = plt.scatter(df['Mileage'],df['Sell Price($)'])                              \\nnewPlt = plt.scatter(df['Age(yrs)'],df['Sell Price($)'])\\n\\nX = df[['Mileage','Age(yrs)']]\\nY = df['Sell Price($)']  \\n\\ndf.shape\\ndf.head()\\ndf.isna().sum() \\ndf.describe()\\nnewPlt\\n\".trim();var reason=\"\\nfrom sklearn.model_selection import train_test_split\\n\\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3) \\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=10)             #random_state argument\\n \\nX_train\\nX_test\\n\".trim();var regrationModal=\"\\nfrom sklearn.linear_model import LinearRegression\\n\\nmodel = LinearRegression()\\nmodel.fit(X_train, Y_train)\\n\\nmodel.coef_\\nmodel.intercept_\\n\\nmodel.predict(X_test)\\nmodel.predict([[69000,6]])\\nmodel.score(X_test, Y_test)\\n\".trim();var pickle=\"\\nimport pickle\\n\\nwith open('model_pickle','wb') as file:\\n    pickle.dump(model,file)\\n    \\nwith open('model_pickle','rb') as file:                                                  # Load save modal.\\n    mp = pickle.load(file)\\n\".trim();var sklearn=\"\\nfrom sklearn.externals import joblib                                                     \\n\\njoblib.dump(model, 'model_joblib')\\n\\nmj = joblib.load('model_joblib')                                                         # Load save modal.\\n\".trim();var stack=\"\\nimport csv\\nimport numpy as np\\nimport pandas as pd\\n\\n# Download data from https://archive.ics.uci.edu/ml/datasets/spambase\\nFILE_NAME = \\\"spambase.data\\\"\\n\\nwith open(FILE_NAME, \\\"r\\\") as f:                                           # 1) load with csv file\\n    data = list(csv.reader(f, delimiter=\\\",\\\"))\\n    \\ndata = np.array(data, dtype=np.float32)\\ndata.shape\\ndata.dtype\\n\\n\\n# skiprows=1\\ndata = np.loadtxt(FILE_NAME, delimiter=\\\",\\\", dtype=np.float32)             # 2) load with np.loadtxt()\\n\\n\\n# skip_header=0, missing_values=\\\"---\\\", filling_values=0.0                 # 3) load with np.genfromtxt()\\ndata = np.genfromtxt(FILE_NAME, delimiter=\\\",\\\", dtype=np.float32)\\n\\n\\nn_samples, n_features = data.shape                                        # split into X and y\\nn_features -= 1\\n\\nX = data[:, 0:n_features]\\ny = data[:, n_features]\\n\\nX[0, 0:5]\\n\".trim();var clusters=\"\\nimport pandas as pd\\n\\ndf = pd.read_csv(\\\"homeprices.csv\\\")\\n\\ndummies = pd.get_dummies(df.town)                                                 # Using Pandas to create dummy varriables.\\nmerged = pd.concat([df,dummies],axis='columns')\\n\\nfinal = merged.drop(['town'], axis='columns')\\nfinal\\n\".trim();var dummyVar=\"\\nfinal = final.drop(['west windsor'], axis='columns')\\n\\nX = final.drop('price', axis='columns')\\ny = final.price\\n\\nfrom sklearn.linear_model import LinearRegression\\n\\nmodel = LinearRegression()\\nmodel.fit(X,y)\\nmodel.predict(X)                                                                   # 2600 sqr ft home in new jersey\\n\".trim();var sklearns=\"\\nfrom sklearn.preprocessing import LabelEncoder\\nfrom sklearn.preprocessing import OneHotEncoder\\nfrom sklearn.compose import ColumnTransformer\\n\\ndfle = df\\nle = LabelEncoder()\\ndfle.town = le.fit_transform(dfle.town)\\n\\nX = dfle[['town','area']].values\\nY = dfle.price.values\\n\\n#Use OHE to create dummy variables for each of the town.\\nct = ColumnTransformer([('town', OneHotEncoder(), [0])], remainder = 'passthrough')\\n\\nX = ct.fit_transform(X)\\nX = X[:,1:]\\n\\nmodel.fit(X,y)\\n\\nmodel.predict([[0,1,3400]])\\nmodel.predict([[1,0,2800]])\\n\".trim();var encoding=\"\\nimport numpy as np\\nfrom sklearn import preprocessing\\n\\ninput_labels = ['red','black','red','green','black','yellow','white']           #input labels.\\n\\nencoder = preprocessing.LabelEncoder()                                          #create the label encoder and train it\\nencoder.fit(input_labels)\\n\\n\\n#Check the performance by encoding the random ordered list\\n\\ntest_labels = ['green','red','black']\\nencoded_values = encoder.transform(test_labels)\\nprint(\\\"Labels =\\\", test_labels)\\nprint(\\\"Encoded values =\\\", list(encoded_values))\\n\\nencoded_values = [3,0,4,1]\\ndecoded_list = encoder.inverse_transform(encoded_values)\\nprint(\\\"Encoded values =\\\", encoded_values)\\nprint(\\\"Decoded labels =\\\", list(decoded_list))\\n\".trim();var Traning=/*#__PURE__*/function(_Component){_inherits(Traning,_Component);function Traning(){_classCallCheck(this,Traning);return _possibleConstructorReturn(this,_getPrototypeOf(Traning).apply(this,arguments));}_createClass(Traning,[{key:\"componentDidMount\",value:function componentDidMount(){setTimeout(function(){return Prism.highlightAll();},0);}},{key:\"render\",value:function render(){var classes=this.props.classes;return React.createElement(Grid,{container:true},React.createElement(Grid,{item:true,xs:2},React.createElement(Paper,{className:classes.paper},React.createElement(\"h4\",null,React.createElement(Sidebar,null)))),React.createElement(Grid,{item:true,xs:10},React.createElement(Paper,{className:classes.paper},React.createElement(List,null,React.createElement(\"h3\",null,\"1. Training And Testing Available Data\"),\"We have a dataset containing prices of used BMW cars. We are going to analyze this dataset and build a prediction function that can predict a price by taking mileage and age of the car as input. We will use sklearn \",React.createElement(\"b\",null,\"train_test_split\"),\" method to split training and testing dataset.\",React.createElement(\"br\",null),React.createElement(\"br\",null),React.createElement(\"div\",{style:titles},React.createElement(PrismCode,{code:cluster,language:\"js\",plugins:[\"line-numbers\"]})),React.createElement(\"br\",null),React.createElement(\"br\",null),React.createElement(\"b\",null,\"we are going to split available data in two sets.\"),React.createElement(\"ol\",null,React.createElement(\"li\",null,React.createElement(\"b\",null,\"Training: \"),\"We will train our model on this dataset.\"),React.createElement(\"li\",null,React.createElement(\"b\",null,\"Testing: \"),\"We will use this subset to make actual predictions using trained model.\")),React.createElement(\"br\",null),React.createElement(\"div\",{style:titles},React.createElement(PrismCode,{code:reason,language:\"js\",plugins:[\"line-numbers\"]})),React.createElement(\"br\",null),React.createElement(\"h3\",null,\"2. Run linear regression model\"),React.createElement(\"div\",{style:titles},React.createElement(PrismCode,{code:regrationModal,language:\"js\",plugins:[\"line-numbers\"]})),React.createElement(\"br\",null),React.createElement(\"h3\",null,\"3. Save Model\"),\"There are two ways we can save a model in scikit learn.\",React.createElement(\"ul\",null,React.createElement(\"li\",null,React.createElement(\"b\",null,\"1.Pickle string: \"),\"Algorithm for serializing and de-serializing a Python object structure. \"),React.createElement(\"ul\",null,React.createElement(\"li\",null,React.createElement(\"b\",null,\"pickle.dump: \"),\"Use to serialize an object hierarchy.\"),React.createElement(\"li\",null,React.createElement(\"b\",null,\"pickle.load : \"),\"Use to deserialize a data stream.\")),React.createElement(\"br\",null),React.createElement(\"li\",null,React.createElement(\"b\",null,\"2.Pickled model as a file using joblib: \"),\"It is more efficient on objects that carry large numpy arrays. These functions also accept file-like object instead of filenames.\"),React.createElement(\"ul\",null,React.createElement(\"li\",null,React.createElement(\"b\",null,\"joblib.dump: \"),\"To serialize an object hierarchy \"),React.createElement(\"li\",null,React.createElement(\"b\",null,\"joblib.load: \"),\"To deserialize a data stream\"))),React.createElement(\"br\",null),React.createElement(\"br\",null),React.createElement(\"b\",null,\"Save Trained Modal using Python Pickle\"),React.createElement(\"br\",null),React.createElement(\"br\",null),React.createElement(\"div\",{style:titles},React.createElement(PrismCode,{code:pickle,language:\"js\",plugins:[\"line-numbers\"]})),React.createElement(\"br\",null),React.createElement(\"br\",null),React.createElement(\"b\",null,\"Save Trained Modal using joblib\"),React.createElement(\"br\",null),React.createElement(\"br\",null),React.createElement(\"div\",{style:titles},React.createElement(PrismCode,{code:sklearn,language:\"js\",plugins:[\"line-numbers\"]})),React.createElement(\"br\",null),React.createElement(\"h3\",null,\"4. Diffrent way to load data\"),React.createElement(\"div\",{style:titles},React.createElement(PrismCode,{code:stack,language:\"js\",plugins:[\"line-numbers\"]})),React.createElement(\"br\",null),React.createElement(\"h3\",null,\"5. What is One Hot Encoding?\"),React.createElement(\"ul\",null,React.createElement(\"li\",null,\"OHE is a process of converting categorical data variables so they can be provided to ML algorithms to improve predictions.\"),React.createElement(\"br\",null),React.createElement(\"li\",null,\"Categorical data refers to variables that are made up of label values, for example, a \\u201Ccolor\\u201D variable could have the values \\u201Cred\\u201C, \\u201Cblue, and \\u201Cgreen\\u201D. Think of values like different categories that sometimes have a natural ordering to them.\"),React.createElement(\"br\",null),React.createElement(\"li\",null,\"Some ML algorithms can work directly with categorical data depending on implementation, such as a decision tree, but most require any i/p or o/p variables to be a numeric in value. This means that any categorical data must be mapped to integers.\"),React.createElement(\"br\",null),React.createElement(\"li\",null,\"OHE is one method of converting data to prepare it for an algorithm and get a better prediction. With one-hot, we convert each categorical value into a new categorical column and assign a binary value of 1 or 0 to those columns. Each integer value is represented as a binary vector. All the values are zero, and the index is marked with a 1.\")),React.createElement(\"br\",null),React.createElement(\"img\",{src:Equations,alt:\"Equations\",className:\"responsive\",style:redesign}),React.createElement(\"br\",null),React.createElement(\"b\",null,\"Why use OHE?\"),React.createElement(\"ul\",null,React.createElement(\"li\",null,\"OHE is useful for data that has no relationship to each other.\"),React.createElement(\"li\",null,\"ML algorithms read a higher number as better/ more important than a lower number.\"),React.createElement(\"li\",null,\"OHE makes our training data more useful and expressive, and it can be rescaled easily. By using numeric values, we more easily determine a probability for our values.\")),React.createElement(\"br\",null),React.createElement(\"b\",null,\"How to convert categorical data to numerical data\"),React.createElement(\"br\",null),\"Manually converting our data to numerical values includes two basic steps:\",React.createElement(\"ul\",null,React.createElement(\"li\",null,React.createElement(\"b\",null,\"Integer encoding:\"),\"We need to assign each category value with an integer, value. If we had the values red, yellow, and blue, we could assign them 1, 2, and 3 respectively.\"),React.createElement(\"li\",null,\"One hot encoding\")),React.createElement(\"br\",null),React.createElement(\"h3\",null,\"6. Categorical Variables and One Hot Encoding\"),React.createElement(\"div\",{style:titles},React.createElement(PrismCode,{code:clusters,language:\"js\",plugins:[\"line-numbers\"]})),React.createElement(\"br\",null),React.createElement(\"h3\",null,\"7. Dummy Varriable Trap\"),\"When you can derive one variable from other variables, they are known to be multi-colinear.\",React.createElement(\"br\",null),\"Here if you know values of california and georgia then you can easily infer value of new jersey state, i.e. california=0 and georgia=0. There for these state variables are called to be multi-colinear. In this situation linear regression won't work as expected. Hence you need to drop one column.\",React.createElement(\"br\",null),React.createElement(\"br\",null),React.createElement(\"b\",null,\"N: \"),\"sklearn library takes care of dummy variable trap hence even if you don't drop one of the state columns it is going to work.\",React.createElement(\"br\",null),React.createElement(\"br\",null),React.createElement(\"div\",{style:titles},React.createElement(PrismCode,{code:dummyVar,language:\"js\",plugins:[\"line-numbers\"]})),React.createElement(\"br\",null),React.createElement(\"h3\",null,\"8. Using sklearn OneHotEncoder\"),\"First step is to use label encoder to convert town names into numbers.\",React.createElement(\"div\",{style:titles},React.createElement(PrismCode,{code:sklearns,language:\"js\",plugins:[\"line-numbers\"]})),React.createElement(\"br\",null),React.createElement(\"h3\",null,\"9. What is Label Encoding?\"),\"Most of the sklearn functions expect that the data with number labels rather than word labels. Hence, we need to convert such labels into number labels. This process is called label encoding. We can perform label encoding of data with the help of \",React.createElement(\"b\",null,\"LabelEncoder()\"),\" function of scikit-learn Python library.\",React.createElement(\"br\",null),React.createElement(\"br\",null),React.createElement(\"div\",{style:titles},React.createElement(PrismCode,{code:encoding,language:\"js\",plugins:[\"line-numbers\"]}))))));}}]);return Traning;}(Component);export default withStyles(styles)(Traning);","map":{"version":3,"sources":["/home/mukeshs/Projects/edurights/src/components/ml/deepMl/training.js"],"names":["React","Component","Prism","Grid","Paper","withStyles","List","Sidebar","PrismCode","Equations","titles","backgroundColor","padding","fontSize","styles","theme","paper","margin","spacing","smMargin","actionDiv","textAlign","redesign","height","width","cluster","trim","reason","regrationModal","pickle","sklearn","stack","clusters","dummyVar","sklearns","encoding","Traning","setTimeout","highlightAll","classes","props"],"mappings":"6kBAAA,MAAOA,CAAAA,KAAP,EAAgBC,SAAhB,KAAiC,OAAjC,CACA,MAAOC,CAAAA,KAAP,KAAkB,SAAlB,CACA,OAASC,IAAT,CAAeC,KAAf,CAAsBC,UAAtB,CAAkCC,IAAlC,KAA8C,mBAA9C,CAEA,MAAO,0BAAP,CACA,MAAOC,CAAAA,OAAP,KAAoB,YAApB,CACA,MAAOC,CAAAA,SAAP,KAAsB,yBAAtB,CAEA,MAAOC,CAAAA,SAAP,KAAsB,uCAAtB,CAGA,GAAMC,CAAAA,MAAM,CAAG,CAAEC,eAAe,CAAE,SAAnB,CAA8BC,OAAO,CAAE,KAAvC,CAA8CC,QAAQ,CAAE,MAAxD,CAAf,CAEA,GAAMC,CAAAA,MAAM,CAAG,QAATA,CAAAA,MAAS,CAAAC,KAAK,QAAK,CACvBC,KAAK,CAAE,CACLC,MAAM,CAAEF,KAAK,CAACG,OAAN,CAAc,CAAd,CADH,CAELN,OAAO,CAAEG,KAAK,CAACG,OAAN,CAAc,CAAd,CAFJ,CADgB,CAKvBC,QAAQ,CAAE,CACRF,MAAM,CAAEF,KAAK,CAACG,OAAN,CAAc,CAAd,CADA,CALa,CAQvBE,SAAS,CAAE,CACTC,SAAS,CAAE,QADF,CARY,CAAL,EAApB,CAaA,GAAMC,CAAAA,QAAQ,CAAG,CACfC,MAAM,CAAE,GADO,CAEfC,KAAK,CAAE,GAFQ,CAAjB,CAMA,GAAMC,CAAAA,OAAO,CAAG,sYAkBdC,IAlBc,EAAhB,CAoBA,GAAMC,CAAAA,MAAM,CAAG,0RAQbD,IARa,EAAf,CAUA,GAAME,CAAAA,cAAc,CAAG,sOAYrBF,IAZqB,EAAvB,CAcA,GAAMG,CAAAA,MAAM,CAAG,2OAQbH,IARa,EAAf,CAUA,GAAMI,CAAAA,OAAO,CAAG,qPAMdJ,IANc,EAAhB,CAQA,GAAMK,CAAAA,KAAK,CAAG,w2BA+BZL,IA/BY,EAAd,CAiCA,GAAMM,CAAAA,QAAQ,CAAG,2SAUfN,IAVe,EAAjB,CAYA,GAAMO,CAAAA,QAAQ,CAAG,+UAWfP,IAXe,EAAjB,CAaA,GAAMQ,CAAAA,QAAQ,CAAG,kiBAsBfR,IAtBe,EAAjB,CAwBA,GAAMS,CAAAA,QAAQ,CAAG,wtBAqBfT,IArBe,EAAjB,C,GAwBMU,CAAAA,O,2RACgB,CAClBC,UAAU,CAAC,iBAAMnC,CAAAA,KAAK,CAACoC,YAAN,EAAN,EAAD,CAA6B,CAA7B,CAAV,CACD,C,uCACQ,IACCC,CAAAA,OADD,CACa,KAAKC,KADlB,CACCD,OADD,CAEP,MACE,qBAAC,IAAD,EAAM,SAAS,KAAf,EACE,oBAAC,IAAD,EAAM,IAAI,KAAV,CAAW,EAAE,CAAE,CAAf,EACE,oBAAC,KAAD,EAAO,SAAS,CAAEA,OAAO,CAACvB,KAA1B,EACE,8BAAI,oBAAC,OAAD,MAAJ,CADF,CADF,CADF,CAME,oBAAC,IAAD,EAAM,IAAI,KAAV,CAAW,EAAE,CAAE,EAAf,EACE,oBAAC,KAAD,EAAO,SAAS,CAAEuB,OAAO,CAACvB,KAA1B,EACE,oBAAC,IAAD,MACE,uEADF,2NAIgC,gDAJhC,kDAKE,8BALF,CAME,8BANF,CAOE,2BAAK,KAAK,CAAEN,MAAZ,EACE,oBAAC,SAAD,EACE,IAAI,CAAEe,OADR,CAEE,QAAQ,CAAC,IAFX,CAGE,OAAO,CAAE,CAAC,cAAD,CAHX,EADF,CAPF,CAcE,8BAdF,CAeE,8BAfF,CAiBE,iFAjBF,CAkBE,8BACE,8BAAI,0CAAJ,4CADF,CAEE,8BAAI,yCAAJ,2EAFF,CAlBF,CAsBE,8BAtBF,CAwBE,2BAAK,KAAK,CAAEf,MAAZ,EACE,oBAAC,SAAD,EACE,IAAI,CAAEiB,MADR,CAEE,QAAQ,CAAC,IAFX,CAGE,OAAO,CAAE,CAAC,cAAD,CAHX,EADF,CAxBF,CA+BE,8BA/BF,CAiCE,+DAjCF,CAkCE,2BAAK,KAAK,CAAEjB,MAAZ,EACE,oBAAC,SAAD,EACE,IAAI,CAAEkB,cADR,CAEE,QAAQ,CAAC,IAFX,CAGE,OAAO,CAAE,CAAC,cAAD,CAHX,EADF,CAlCF,CAyCE,8BAzCF,CA2CE,8CA3CF,2DA6CE,8BACE,8BAAI,iDAAJ,4EADF,CAEE,8BACE,8BAAI,6CAAJ,yCADF,CAEE,8BAAI,8CAAJ,qCAFF,CAFF,CAME,8BANF,CAOE,8BAAI,wEAAJ,qIAPF,CASE,8BACE,8BAAI,6CAAJ,qCADF,CAEE,8BAAI,6CAAJ,gCAFF,CATF,CA7CF,CA2DE,8BA3DF,CA4DE,8BA5DF,CA8DE,sEA9DF,CA+DE,8BA/DF,CAgEE,8BAhEF,CAiEE,2BAAK,KAAK,CAAElB,MAAZ,EACE,oBAAC,SAAD,EACE,IAAI,CAAEmB,MADR,CAEE,QAAQ,CAAC,IAFX,CAGE,OAAO,CAAE,CAAC,cAAD,CAHX,EADF,CAjEF,CAwEE,8BAxEF,CAyEE,8BAzEF,CA2EE,+DA3EF,CA4EE,8BA5EF,CA6EE,8BA7EF,CA8EE,2BAAK,KAAK,CAAEnB,MAAZ,EACE,oBAAC,SAAD,EACE,IAAI,CAAEoB,OADR,CAEE,QAAQ,CAAC,IAFX,CAGE,OAAO,CAAE,CAAC,cAAD,CAHX,EADF,CA9EF,CAqFE,8BArFF,CAuFE,6DAvFF,CAwFE,2BAAK,KAAK,CAAEpB,MAAZ,EACE,oBAAC,SAAD,EACE,IAAI,CAAEqB,KADR,CAEE,QAAQ,CAAC,IAFX,CAGE,OAAO,CAAE,CAAC,cAAD,CAHX,EADF,CAxFF,CA+FE,8BA/FF,CAiGE,6DAjGF,CAkGE,8BACE,2JADF,CAGE,8BAHF,CAKE,qTALF,CAQE,8BARF,CAUE,sRAVF,CAaE,8BAbF,CAeE,sXAfF,CAlGF,CAsHE,8BAtHF,CAwHE,2BAAK,GAAG,CAAEtB,SAAV,CAAqB,GAAG,CAAC,WAAzB,CAAqC,SAAS,CAAC,YAA/C,CAA4D,KAAK,CAAEa,QAAnE,EAxHF,CA0HE,8BA1HF,CA2HE,4CA3HF,CA4HE,8BACE,+FADF,CAEE,kHAFF,CAGE,uMAHF,CA5HF,CAkIE,8BAlIF,CAmIE,iFAnIF,CAoIE,8BApIF,8EAsIE,8BACE,8BAAI,iDAAJ,4JADF,CAGE,iDAHF,CAtIF,CA2IE,8BA3IF,CA6IE,8EA7IF,CA8IE,2BAAK,KAAK,CAAEZ,MAAZ,EACE,oBAAC,SAAD,EACE,IAAI,CAAEsB,QADR,CAEE,QAAQ,CAAC,IAFX,CAGE,OAAO,CAAE,CAAC,cAAD,CAHX,EADF,CA9IF,CAqJE,8BArJF,CAuJE,wDAvJF,+FAyJE,8BAzJF,2SA6JE,8BA7JF,CA8JE,8BA9JF,CA+JE,mCA/JF,gIAiKE,8BAjKF,CAkKE,8BAlKF,CAmKE,2BAAK,KAAK,CAAEtB,MAAZ,EACE,oBAAC,SAAD,EACE,IAAI,CAAEuB,QADR,CAEE,QAAQ,CAAC,IAFX,CAGE,OAAO,CAAE,CAAC,cAAD,CAHX,EADF,CAnKF,CA0KE,8BA1KF,CA4KE,+DA5KF,0EA8KE,2BAAK,KAAK,CAAEvB,MAAZ,EACE,oBAAC,SAAD,EACE,IAAI,CAAEwB,QADR,CAEE,QAAQ,CAAC,IAFX,CAGE,OAAO,CAAE,CAAC,cAAD,CAHX,EADF,CA9KF,CAqLE,8BArLF,CAuLE,2DAvLF,2PA0LmB,8CA1LnB,6CA2LE,8BA3LF,CA4LE,8BA5LF,CA6LE,2BAAK,KAAK,CAAExB,MAAZ,EACE,oBAAC,SAAD,EACE,IAAI,CAAEyB,QADR,CAEE,QAAQ,CAAC,IAFX,CAGE,OAAO,CAAE,CAAC,cAAD,CAHX,EADF,CA7LF,CADF,CADF,CANF,CADF,CAkND,C,qBAxNmBlC,S,EA2NtB,cAAgBI,CAAAA,UAAU,CAACS,MAAD,CAAV,CAAmBsB,OAAnB,CAAhB","sourcesContent":["import React, { Component } from 'react';\nimport Prism from \"prismjs\"\nimport { Grid, Paper, withStyles, List } from \"@material-ui/core\";\n\nimport '../../ReactJs/styles.css'\nimport Sidebar from '../sidebar';\nimport PrismCode from '../../ReactJs/prismCode';\n\nimport Equations from '../../../assets/ML/oneHotEncodung.png'\n\n\nconst titles = { backgroundColor: '#F0F8FF', padding: '1px', fontSize: '16px' }\n\nconst styles = theme => ({\n  paper: {\n    margin: theme.spacing(1),\n    padding: theme.spacing(1)\n  },\n  smMargin: {\n    margin: theme.spacing(1)\n  },\n  actionDiv: {\n    textAlign: \"center\"\n  }\n})\n\nconst redesign = {\n  height: 350,\n  width: 600\n}\n\n\nconst cluster = `\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\ndf = pd.read_csv(\"carprices.csv\")\n\nnewPlt = plt.scatter(df['Mileage'],df['Sell Price($)'])                              \nnewPlt = plt.scatter(df['Age(yrs)'],df['Sell Price($)'])\n\nX = df[['Mileage','Age(yrs)']]\nY = df['Sell Price($)']  \n\ndf.shape\ndf.head()\ndf.isna().sum() \ndf.describe()\nnewPlt\n`.trim();\n\nconst reason = `\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3) \nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=10)             #random_state argument\n \nX_train\nX_test\n`.trim();\n\nconst regrationModal = `\nfrom sklearn.linear_model import LinearRegression\n\nmodel = LinearRegression()\nmodel.fit(X_train, Y_train)\n\nmodel.coef_\nmodel.intercept_\n\nmodel.predict(X_test)\nmodel.predict([[69000,6]])\nmodel.score(X_test, Y_test)\n`.trim();\n\nconst pickle = `\nimport pickle\n\nwith open('model_pickle','wb') as file:\n    pickle.dump(model,file)\n    \nwith open('model_pickle','rb') as file:                                                  # Load save modal.\n    mp = pickle.load(file)\n`.trim();\n\nconst sklearn = `\nfrom sklearn.externals import joblib                                                     \n\njoblib.dump(model, 'model_joblib')\n\nmj = joblib.load('model_joblib')                                                         # Load save modal.\n`.trim();\n\nconst stack = `\nimport csv\nimport numpy as np\nimport pandas as pd\n\n# Download data from https://archive.ics.uci.edu/ml/datasets/spambase\nFILE_NAME = \"spambase.data\"\n\nwith open(FILE_NAME, \"r\") as f:                                           # 1) load with csv file\n    data = list(csv.reader(f, delimiter=\",\"))\n    \ndata = np.array(data, dtype=np.float32)\ndata.shape\ndata.dtype\n\n\n# skiprows=1\ndata = np.loadtxt(FILE_NAME, delimiter=\",\", dtype=np.float32)             # 2) load with np.loadtxt()\n\n\n# skip_header=0, missing_values=\"---\", filling_values=0.0                 # 3) load with np.genfromtxt()\ndata = np.genfromtxt(FILE_NAME, delimiter=\",\", dtype=np.float32)\n\n\nn_samples, n_features = data.shape                                        # split into X and y\nn_features -= 1\n\nX = data[:, 0:n_features]\ny = data[:, n_features]\n\nX[0, 0:5]\n`.trim();\n\nconst clusters = `\nimport pandas as pd\n\ndf = pd.read_csv(\"homeprices.csv\")\n\ndummies = pd.get_dummies(df.town)                                                 # Using Pandas to create dummy varriables.\nmerged = pd.concat([df,dummies],axis='columns')\n\nfinal = merged.drop(['town'], axis='columns')\nfinal\n`.trim();\n\nconst dummyVar = `\nfinal = final.drop(['west windsor'], axis='columns')\n\nX = final.drop('price', axis='columns')\ny = final.price\n\nfrom sklearn.linear_model import LinearRegression\n\nmodel = LinearRegression()\nmodel.fit(X,y)\nmodel.predict(X)                                                                   # 2600 sqr ft home in new jersey\n`.trim();\n\nconst sklearns = `\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\n\ndfle = df\nle = LabelEncoder()\ndfle.town = le.fit_transform(dfle.town)\n\nX = dfle[['town','area']].values\nY = dfle.price.values\n\n#Use OHE to create dummy variables for each of the town.\nct = ColumnTransformer([('town', OneHotEncoder(), [0])], remainder = 'passthrough')\n\nX = ct.fit_transform(X)\nX = X[:,1:]\n\nmodel.fit(X,y)\n\nmodel.predict([[0,1,3400]])\nmodel.predict([[1,0,2800]])\n`.trim();\n\nconst encoding = `\nimport numpy as np\nfrom sklearn import preprocessing\n\ninput_labels = ['red','black','red','green','black','yellow','white']           #input labels.\n\nencoder = preprocessing.LabelEncoder()                                          #create the label encoder and train it\nencoder.fit(input_labels)\n\n\n#Check the performance by encoding the random ordered list\n\ntest_labels = ['green','red','black']\nencoded_values = encoder.transform(test_labels)\nprint(\"Labels =\", test_labels)\nprint(\"Encoded values =\", list(encoded_values))\n\nencoded_values = [3,0,4,1]\ndecoded_list = encoder.inverse_transform(encoded_values)\nprint(\"Encoded values =\", encoded_values)\nprint(\"Decoded labels =\", list(decoded_list))\n`.trim()\n\n\nclass Traning extends Component {\n  componentDidMount() {\n    setTimeout(() => Prism.highlightAll(), 0)\n  }\n  render() {\n    const { classes } = this.props;\n    return (\n      <Grid container>\n        <Grid item xs={2}>\n          <Paper className={classes.paper}>\n            <h4><Sidebar /></h4>\n          </Paper>\n        </Grid>\n        <Grid item xs={10}>\n          <Paper className={classes.paper}>\n            <List>\n              <h3>1. Training And Testing Available Data</h3>\n              We have a dataset containing prices of used BMW cars. We are going to analyze this dataset\n              and build a prediction function that can predict a price by taking mileage and age of the car\n              as input. We will use sklearn <b>train_test_split</b> method to split training and testing dataset.\n              <br />\n              <br />\n              <div style={titles}>\n                <PrismCode\n                  code={cluster}\n                  language=\"js\"\n                  plugins={[\"line-numbers\"]}\n                />\n              </div>\n              <br />\n              <br />\n\n              <b>we are going to split available data in two sets.</b>\n              <ol>\n                <li><b>Training: </b>We will train our model on this dataset.</li>\n                <li><b>Testing: </b>We will use this subset to make actual predictions using trained model.</li>\n              </ol>\n              <br />\n\n              <div style={titles}>\n                <PrismCode\n                  code={reason}\n                  language=\"js\"\n                  plugins={[\"line-numbers\"]}\n                />\n              </div>\n              <br />\n\n              <h3>2. Run linear regression model</h3>\n              <div style={titles}>\n                <PrismCode\n                  code={regrationModal}\n                  language=\"js\"\n                  plugins={[\"line-numbers\"]}\n                />\n              </div>\n              <br />\n\n              <h3>3. Save Model</h3>\n              There are two ways we can save a model in scikit learn.\n              <ul>\n                <li><b>1.Pickle string: </b>Algorithm for serializing and de-serializing a Python object structure. </li>\n                <ul>\n                  <li><b>pickle.dump: </b>Use to serialize an object hierarchy.</li>\n                  <li><b>pickle.load : </b>Use to deserialize a data stream.</li>\n                </ul>\n                <br />\n                <li><b>2.Pickled model as a file using joblib: </b>It is more efficient on objects that carry large numpy arrays. These functions\n                  also accept file-like object instead of filenames.</li>\n                <ul>\n                  <li><b>joblib.dump: </b>To serialize an object hierarchy </li>\n                  <li><b>joblib.load: </b>To deserialize a data stream</li>\n                </ul>\n              </ul>\n              <br />\n              <br />\n\n              <b>Save Trained Modal using Python Pickle</b>\n              <br />\n              <br />\n              <div style={titles}>\n                <PrismCode\n                  code={pickle}\n                  language=\"js\"\n                  plugins={[\"line-numbers\"]}\n                />\n              </div>\n              <br />\n              <br />\n\n              <b>Save Trained Modal using joblib</b>\n              <br />\n              <br />\n              <div style={titles}>\n                <PrismCode\n                  code={sklearn}\n                  language=\"js\"\n                  plugins={[\"line-numbers\"]}\n                />\n              </div>\n              <br />\n\n              <h3>4. Diffrent way to load data</h3>\n              <div style={titles}>\n                <PrismCode\n                  code={stack}\n                  language=\"js\"\n                  plugins={[\"line-numbers\"]}\n                />\n              </div>\n              <br />\n\n              <h3>5. What is One Hot Encoding?</h3>\n              <ul>\n                <li>OHE is a process of converting categorical data variables so they can be provided to ML algorithms\n                  to improve predictions.</li>\n                <br />\n\n                <li>Categorical data refers to variables that are made up of label values, for example, a “color” variable could have\n                  the values “red“, “blue, and “green”. Think of values like different categories that sometimes have a natural\n                  ordering to them.</li>\n                <br />\n\n                <li>Some ML algorithms can work directly with categorical data depending on implementation, such as a decision tree, but\n                  most require any i/p or o/p variables to be a numeric in value. This means that any categorical\n                  data must be mapped to integers.</li>\n                <br />\n\n                <li>OHE is one method of converting data to prepare it for an algorithm and get a better prediction. With\n                  one-hot, we convert each categorical value into a new categorical column and assign a binary value of 1 or 0 to\n                  those columns. Each integer value is represented as a binary vector. All the values are zero, and the index is\n                  marked with a 1.</li>\n              </ul>\n              <br />\n\n              <img src={Equations} alt=\"Equations\" className=\"responsive\" style={redesign} />\n\n              <br />\n              <b>Why use OHE?</b>\n              <ul>\n                <li>OHE is useful for data that has no relationship to each other.</li>\n                <li>ML algorithms read a higher number as better/ more important than a lower number.</li>\n                <li>OHE makes our training data more useful and expressive, and it can be rescaled\n                  easily. By using numeric values, we more easily determine a probability for our values.</li>\n              </ul>\n              <br />\n              <b>How to convert categorical data to numerical data</b>\n              <br />\n              Manually converting our data to numerical values includes two basic steps:\n              <ul>\n                <li><b>Integer encoding:</b>We need to assign each category value with an integer, value. If we had\n                  the values red, yellow, and blue, we could assign them 1, 2, and 3 respectively.</li>\n                <li>One hot encoding</li>\n              </ul>\n              <br />\n\n              <h3>6. Categorical Variables and One Hot Encoding</h3>\n              <div style={titles}>\n                <PrismCode\n                  code={clusters}\n                  language=\"js\"\n                  plugins={[\"line-numbers\"]}\n                />\n              </div>\n              <br />\n\n              <h3>7. Dummy Varriable Trap</h3>\n              When you can derive one variable from other variables, they are known to be multi-colinear.\n              <br />\n              Here if you know values of california and georgia then you can easily infer value of new jersey state, i.e.\n              california=0 and georgia=0. There for these state variables are called to be multi-colinear. In this\n              situation linear regression won't work as expected. Hence you need to drop one column.\n              <br />\n              <br />\n              <b>N: </b>sklearn library takes care of dummy variable trap hence even if you don't drop one of the\n              state columns it is going to work.\n              <br />\n              <br />\n              <div style={titles}>\n                <PrismCode\n                  code={dummyVar}\n                  language=\"js\"\n                  plugins={[\"line-numbers\"]}\n                />\n              </div>\n              <br />\n\n              <h3>8. Using sklearn OneHotEncoder</h3>\n              First step is to use label encoder to convert town names into numbers.\n              <div style={titles}>\n                <PrismCode\n                  code={sklearns}\n                  language=\"js\"\n                  plugins={[\"line-numbers\"]}\n                />\n              </div>\n              <br />\n\n              <h3>9. What is Label Encoding?</h3>\n              Most of the sklearn functions expect that the data with number labels rather than word labels. Hence, we need to\n              convert such labels into number labels. This process is called label encoding. We can perform label encoding of data\n              with the help of <b>LabelEncoder()</b> function of scikit-learn Python library.\n              <br />\n              <br />\n              <div style={titles}>\n                <PrismCode\n                  code={encoding}\n                  language=\"js\"\n                  plugins={[\"line-numbers\"]}\n                />\n              </div>\n            </List>\n          </Paper>\n        </Grid>\n      </Grid>\n    )\n  }\n}\n\nexport default (withStyles(styles)(Traning));\n"]},"metadata":{},"sourceType":"module"}