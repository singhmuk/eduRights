{"ast":null,"code":"import _classCallCheck from\"/home/mukeshs/Projects/edurights/node_modules/@babel/runtime/helpers/esm/classCallCheck\";import _createClass from\"/home/mukeshs/Projects/edurights/node_modules/@babel/runtime/helpers/esm/createClass\";import _possibleConstructorReturn from\"/home/mukeshs/Projects/edurights/node_modules/@babel/runtime/helpers/esm/possibleConstructorReturn\";import _getPrototypeOf from\"/home/mukeshs/Projects/edurights/node_modules/@babel/runtime/helpers/esm/getPrototypeOf\";import _inherits from\"/home/mukeshs/Projects/edurights/node_modules/@babel/runtime/helpers/esm/inherits\";import React,{Component}from'react';import Prism from\"prismjs\";import{Grid,Paper,withStyles,List}from\"@material-ui/core\";import'../../ReactJs/styles.css';import Sidebar from'../sidebar';import PrismCode from'../../ReactJs/prismCode';var titles={backgroundColor:'#F0F8FF',padding:'1px',fontSize:'16px'};var styles=function styles(theme){return{paper:{margin:theme.spacing(1),padding:theme.spacing(1)},smMargin:{margin:theme.spacing(1)},actionDiv:{textAlign:\"center\"}};};var childsFile=\"\\nimport pandas as pd\\nfrom matplotlib import pyplot as plt\\nimport numpy as np\\n%matplotlib inline\\nimport warnings\\nwarnings.filterwarnings('ignore')\\n\\ndf = pd.read_csv(\\\"customer_churn.csv\\\")\\n\\ndf.Churn.value_counts()\\n517400/ df.shape[0]\\n\".trim();var customerID=\"\\ndf.drop('customerID',axis='columns',inplace=True)\\ndf.TotalCharges.values\\n\\npd.to_numeric(df.TotalCharges,errors='coerce').isnull()\\ndf[pd.to_numeric(df.TotalCharges,errors='coerce').isnull()]\\n\\ndf.iloc[488].TotalCharges\\ndf[df.TotalCharges!=' '].shape\\ndf1 = df[df.TotalCharges!=' ']\\n\\ndf1.TotalCharges = pd.to_numeric(df1.TotalCharges)\\ndf1.TotalCharges.values\\ndf1[df1.Churn=='No']\\n\".trim();var visualization=\"\\ntenure_churn_no = df1[df1.Churn=='No'].tenure\\ntenure_churn_yes = df1[df1.Churn=='Yes'].tenure\\n\\nplt.xlabel(\\\"tenure\\\")\\nplt.ylabel(\\\"Number Of Customers\\\")\\nplt.title(\\\"Customer Churn Prediction Visualiztion\\\")\\n\\nblood_sugar_men = [113, 85, 90, 150, 149, 88, 93, 115, 135, 80, 77, 82, 129]\\nblood_sugar_women = [67, 98, 89, 120, 133, 150, 84, 69, 89, 79, 120, 112, 100]\\n\\nplt.hist([tenure_churn_yes, tenure_churn_no], rwidth=0.95, color=['green','red'],label=['Churn=Yes','Churn=No'])\\nplt.legend()\\n\\n\\nmc_churn_no = df1[df1.Churn=='No'].MonthlyCharges      \\nmc_churn_yes = df1[df1.Churn=='Yes'].MonthlyCharges      \\n\\nplt.xlabel(\\\"Monthly Charges\\\")\\nplt.ylabel(\\\"Number Of Customers\\\")\\nplt.title(\\\"Customer Churn Prediction Visualiztion\\\")\\n\\nblood_sugar_men = [113, 85, 90, 150, 149, 88, 93, 115, 135, 80, 77, 82, 129]\\nblood_sugar_women = [67, 98, 89, 120, 133, 150, 84, 69, 89, 79, 120, 112, 100]\\n\\nplt.hist([mc_churn_yes, mc_churn_no], rwidth=0.95, color=['green','red'],label=['Churn=Yes','Churn=No'])\\nplt.legend()\\n\\ndef print_unique_col_values(df):                                                        #Many of the columns are yes, no.\\n       for column in df:\\n            if df[column].dtypes=='object':\\n                print(f'{column}: {df[column].unique()}') \\n                \\nprint_unique_col_values(df1)\\n\".trim();var service=\"\\ndf1.replace('No internet service','No',inplace=True)\\ndf1.replace('No phone service','No',inplace=True)\\n\\nprint_unique_col_values(df1)\\n\\n#Convert Yes and No to 1 or 0.\\nyes_no_columns = ['Partner','Dependents','PhoneService','MultipleLines','OnlineSecurity','OnlineBackup',\\n                  'DeviceProtection','TechSupport','StreamingTV','StreamingMovies','PaperlessBilling','Churn']\\nfor col in yes_no_columns:\\n    df1[col].replace({'Yes': 1,'No': 0},inplace=True)\\n    \\n    \\nfor col in df1:\\n    print(f'{col}: {df1[col].unique()}') \\n    \\ndf1['gender'].replace({'Female':1,'Male':0},inplace=True)\\ndf1.gender.unique()\\n\".trim();var encoding=\"\\ndf2 = pd.get_dummies(data=df1, columns=['InternetService','Contract','PaymentMethod'])\\ndf2.columns\\n\\ncols_to_scale = ['tenure','MonthlyCharges','TotalCharges']\\n\\nfrom sklearn.preprocessing import MinMaxScaler\\nscaler = MinMaxScaler()\\ndf2[cols_to_scale] = scaler.fit_transform(df2[cols_to_scale])\\n\\nfor col in df2:\\n    print(f'{col}: {df2[col].unique()}')\\n\".trim();var split=\"\\nX = df2.drop('Churn',axis='columns')\\ny = testLabels = df2.Churn.astype(np.float32)\\n\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=15, stratify=y)\\n\\ny_train.value_counts()\\ny.value_counts()\\n5163/1869\\n\\nX_train.shape\\nlen(X_train.columns)\\n\".trim();var regression=\"\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.metrics import classification_report\\n\\ndef log_reg(X_train, y_train, X_test, y_test, weights):\\n    if weights==-1:\\n        model = LogisticRegression()\\n    else:\\n        model = LogisticRegression(class_weight={0:weights[0], 1:weights[1]})\\n\\n    model.fit(X_train, y_train)\\n    acc = model.score(X_test, y_test)\\n    print(\\\"Accuracy\\\", acc)\\n\\n    y_pred = model.predict(X_test)\\n    print(\\\"preds\\\", y_pred[:5])\\n\\n    cl_rep = classification_report(y_test,y_pred)\\n    print(cl_rep)\\n    \\nweights = -1                                                      # pass -1 to use Logistics Regression without weights.\\nlog_reg(X_train, y_train, X_test, y_test, weights)\\n\\nweights = [1, 1.5]                                                # pass -1 to use Logistics Regression without weights.\\nlog_reg(X_train, y_train, X_test, y_test, weights)\\n\".trim();var skewdness=\"\\n# Method1: Undersampling\\ncount_class_0, count_class_1 = df1.Churn.value_counts()\\n\\ndf_class_0 = df2[df2['Churn'] == 0]                               # Divide by class.\\ndf_class_1 = df2[df2['Churn'] == 1]\\n\\n\\n# Undersample 0-class and concat the DataFrames of both class.\\ndf_class_0_under = df_class_0.sample(count_class_1)\\ndf_test_under = pd.concat([df_class_0_under, df_class_1], axis=0)\\n\\nprint('Random under-sampling:')\\nprint(df_test_under.Churn.value_counts())\\n\\nX = df_test_under.drop('Churn',axis='columns')\\ny = df_test_under['Churn']\\n\\nfrom sklearn.model_selection import train_test_split\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=15, stratify=y)\\n\\n\\ny_train.value_counts()                                                        # Number of classes in training Data.\\n\".trim();var applying=\"\\nweights = -1                                                    # pass -1 to use Logistics Regression without weights\\nlog_reg(X_train, y_train, X_test, y_test, weights)\\n\\n\\n# Method2: Oversampling\\n# Oversample 1-class and concat the DataFrames of both classes\\ndf_class_1_over = df_class_1.sample(count_class_0, replace=True)\\ndf_test_over = pd.concat([df_class_0, df_class_1_over], axis=0)\\n\\nprint('Random over-sampling:', df_test_over.Churn.value_counts())\\n\\nX = df_test_over.drop('Churn',axis='columns')\\ny = df_test_over['Churn']\\n\\nfrom sklearn.model_selection import train_test_split\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=15, stratify=y)\\n\\n\\ny_train.value_counts()                                                        # Number of classes in training Data.\\n\".trim();var logistic=\"\\nweights = -1                                                # pass -1 to use Logistics Regression without weights.\\nlog_reg(X_train, y_train, X_test, y_test, weights)\\n\\n# Method3: SMOTE\\nX = df2.drop('Churn',axis='columns')\\ny = df2['Churn']\\n\\nfrom imblearn.over_sampling import SMOTE\\nsmote = SMOTE(sampling_strategy='minority')\\nX_sm, y_sm = smote.fit_sample(X, y)\\n\\ny_sm.value_counts()\\n\\n\\nfrom sklearn.model_selection import train_test_split\\nX_train, X_test, y_train, y_test = train_test_split(X_sm, y_sm, test_size=0.2, random_state=15, stratify=y_sm)\\n\\ny_train.value_counts()\\n\\n\\n# Logistic Regression\\nweights = -1                                                 # pass -1 to use Logistics Regression without weights.\\nlog_reg(X_train, y_train, X_test, y_test, weights)\\n\\n\\ndf2.Churn.value_counts()                                    # Method4: Use of Ensemble with undersampling.\\n\\nX = df2.drop('Churn',axis='columns')                        # Regain Original features and labels.\\ny = df2['Churn']\\n\\nfrom sklearn.model_selection import train_test_split\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=15, stratify=y)\\n\\ny_train.value_counts()\\nmodel = LogisticRegression()\\n\\ndf3 = X_train.copy()\\ndf3['Churn'] = y_train\\ndf3.head()\\n\\ndf3_class0 = df3[df3.Churn==0]\\ndf3_class1 = df3[df3.Churn==1]\\n\\ndef get_train_batch(df_majority, df_minority, start, end):\\n    df_train = pd.concat([df_majority[start:end], df_minority], axis=0)\\n    X_train = df_train.drop('Churn', axis='columns')\\n    y_train = df_train.Churn\\n    return X_train, y_train    \\n    \\nX_train, y_train = get_train_batch(df3_class0, df3_class1, 0, 1495)\\nmodel1 = LogisticRegression()\\nmodel1.fit(X_train, y_train)\\ny_pred1 = model1.predict(X_test)\\n\\nX_train, y_train = get_train_batch(df3_class0, df3_class1, 1495, 2990)\\nmodel2 = LogisticRegression()\\nmodel2.fit(X_train, y_train)\\ny_pred2 = model2.predict(X_test)\\n\\nX_train, y_train = get_train_batch(df3_class0, df3_class1, 2990, 4130)\\nmodel3 = LogisticRegression()\\nmodel3.fit(X_train, y_train)\\ny_pred3 = model3.predict(X_test)\\n\\nlen(y_pred1)\\n\\ny_pred_final = y_pred1.copy()\\nfor i in range(len(y_pred1)):\\n    n_ones = y_pred1[i] + y_pred2[i] + y_pred3[i]\\n    if n_ones>1:\\n        y_pred_final[i] = 1\\n    else:\\n        y_pred_final[i] = 0\\n        \\n        \\ncl_rep = classification_report(y_test, y_pred_final)\\nprint(cl_rep)\\n\".trim();var Imbalanced=/*#__PURE__*/function(_Component){_inherits(Imbalanced,_Component);function Imbalanced(){_classCallCheck(this,Imbalanced);return _possibleConstructorReturn(this,_getPrototypeOf(Imbalanced).apply(this,arguments));}_createClass(Imbalanced,[{key:\"componentDidMount\",value:function componentDidMount(){setTimeout(function(){return Prism.highlightAll();},0);}},{key:\"render\",value:function render(){var classes=this.props.classes;return React.createElement(Grid,{container:true},React.createElement(Grid,{item:true,xs:2},React.createElement(Paper,{className:classes.paper},React.createElement(\"h4\",null,React.createElement(Sidebar,null)))),React.createElement(Grid,{item:true,xs:10},React.createElement(Paper,{className:classes.paper},React.createElement(List,null,React.createElement(\"h3\",null,\"Handle imbalanced data in churn prediction. Logistic Regression\"),React.createElement(\"i\",null,\"Customer churn prediction is to measure why customers are leaving a business. Looking at customer churn in telecom business. We will build a deep learning model to predict the churn and use precision,recall, f1-score to measure performance of our model.\"),React.createElement(\"br\",null),React.createElement(\"br\",null),React.createElement(\"div\",{style:titles},React.createElement(PrismCode,{code:childsFile,language:\"js\",plugins:[\"line-numbers\"]})),React.createElement(\"br\",null),React.createElement(\"h3\",null,\"First of all, drop customerID column as it is of no use\"),React.createElement(\"div\",{style:titles},React.createElement(PrismCode,{code:customerID,language:\"js\",plugins:[\"line-numbers\"]})),React.createElement(\"br\",null),React.createElement(\"h3\",null,\"Data Visualization\"),React.createElement(\"div\",{style:titles},React.createElement(PrismCode,{code:visualization,language:\"js\",plugins:[\"line-numbers\"]})),React.createElement(\"br\",null),React.createElement(\"h3\",null,\"Some of the columns have no internet service or no phone service, that can be replaced with a simple No.\"),React.createElement(\"div\",{style:titles},React.createElement(PrismCode,{code:service,language:\"js\",plugins:[\"line-numbers\"]})),React.createElement(\"br\",null),React.createElement(\"h3\",null,\"One hot encoding for categorical columns\"),React.createElement(\"div\",{style:titles},React.createElement(PrismCode,{code:encoding,language:\"js\",plugins:[\"line-numbers\"]})),React.createElement(\"br\",null),React.createElement(\"h3\",null,\"Train test split\"),React.createElement(\"div\",{style:titles},React.createElement(PrismCode,{code:split,language:\"js\",plugins:[\"line-numbers\"]})),React.createElement(\"br\",null),React.createElement(\"h3\",null,\"Use logistic regression classifier\"),React.createElement(\"div\",{style:titles},React.createElement(PrismCode,{code:regression,language:\"js\",plugins:[\"line-numbers\"]})),React.createElement(\"br\",null),React.createElement(\"h3\",null,\"Mitigating Skewdness of Data\"),React.createElement(\"div\",{style:titles},React.createElement(PrismCode,{code:skewdness,language:\"js\",plugins:[\"line-numbers\"]})),React.createElement(\"br\",null),React.createElement(\"h3\",null,\"Applying Logistic Regression\"),React.createElement(\"div\",{style:titles},React.createElement(PrismCode,{code:applying,language:\"js\",plugins:[\"line-numbers\"]})),React.createElement(\"br\",null),React.createElement(\"h3\",null,\"Logistic Regression\"),React.createElement(\"div\",{style:titles},React.createElement(PrismCode,{code:logistic,language:\"js\",plugins:[\"line-numbers\"]}))))));}}]);return Imbalanced;}(Component);export default withStyles(styles)(Imbalanced);","map":{"version":3,"sources":["/home/mukeshs/Projects/edurights/src/components/angularjs/deepAngularjs/imbalanced.js"],"names":["React","Component","Prism","Grid","Paper","withStyles","List","Sidebar","PrismCode","titles","backgroundColor","padding","fontSize","styles","theme","paper","margin","spacing","smMargin","actionDiv","textAlign","childsFile","trim","customerID","visualization","service","encoding","split","regression","skewdness","applying","logistic","Imbalanced","setTimeout","highlightAll","classes","props"],"mappings":"6kBAAA,MAAOA,CAAAA,KAAP,EAAgBC,SAAhB,KAAiC,OAAjC,CACA,MAAOC,CAAAA,KAAP,KAAkB,SAAlB,CACA,OAASC,IAAT,CAAeC,KAAf,CAAsBC,UAAtB,CAAkCC,IAAlC,KAA8C,mBAA9C,CAEA,MAAO,0BAAP,CACA,MAAOC,CAAAA,OAAP,KAAoB,YAApB,CACA,MAAOC,CAAAA,SAAP,KAAsB,yBAAtB,CAGA,GAAMC,CAAAA,MAAM,CAAG,CAAEC,eAAe,CAAE,SAAnB,CAA8BC,OAAO,CAAE,KAAvC,CAA8CC,QAAQ,CAAE,MAAxD,CAAf,CAEA,GAAMC,CAAAA,MAAM,CAAG,QAATA,CAAAA,MAAS,CAAAC,KAAK,QAAK,CACvBC,KAAK,CAAE,CACLC,MAAM,CAAEF,KAAK,CAACG,OAAN,CAAc,CAAd,CADH,CAELN,OAAO,CAAEG,KAAK,CAACG,OAAN,CAAc,CAAd,CAFJ,CADgB,CAKvBC,QAAQ,CAAE,CACRF,MAAM,CAAEF,KAAK,CAACG,OAAN,CAAc,CAAd,CADA,CALa,CAQvBE,SAAS,CAAE,CACTC,SAAS,CAAE,QADF,CARY,CAAL,EAApB,CAcA,GAAMC,CAAAA,UAAU,CAAG,wPAYjBC,IAZiB,EAAnB,CAcA,GAAMC,CAAAA,UAAU,CAAG,0YAcjBD,IAdiB,EAAnB,CAgBA,GAAME,CAAAA,aAAa,CAAG,8zCAkCpBF,IAlCoB,EAAtB,CAoCA,GAAMG,CAAAA,OAAO,CAAG,2nBAkBdH,IAlBc,EAAhB,CAoBA,GAAMI,CAAAA,QAAQ,CAAG,+WAYfJ,IAZe,EAAjB,CAcA,GAAMK,CAAAA,KAAK,CAAG,iSAYZL,IAZY,EAAd,CAcA,GAAMM,CAAAA,UAAU,CAAG,y5BAyBjBN,IAzBiB,EAAnB,CA2BA,GAAMO,CAAAA,SAAS,CAAG,q0BAuBhBP,IAvBgB,EAAlB,CAyBA,GAAMQ,CAAAA,QAAQ,CAAG,yzBAoBfR,IApBe,EAAjB,CAsBA,GAAMS,CAAAA,QAAQ,CAAG,83EA8EfT,IA9Ee,EAAjB,C,GAkFMU,CAAAA,U,0SACgB,CAClBC,UAAU,CAAC,iBAAM/B,CAAAA,KAAK,CAACgC,YAAN,EAAN,EAAD,CAA6B,CAA7B,CAAV,CACD,C,uCACQ,IACCC,CAAAA,OADD,CACa,KAAKC,KADlB,CACCD,OADD,CAEP,MACE,qBAAC,IAAD,EAAM,SAAS,KAAf,EACE,oBAAC,IAAD,EAAM,IAAI,KAAV,CAAW,EAAE,CAAE,CAAf,EACE,oBAAC,KAAD,EAAO,SAAS,CAAEA,OAAO,CAACpB,KAA1B,EACE,8BAAI,oBAAC,OAAD,MAAJ,CADF,CADF,CADF,CAME,oBAAC,IAAD,EAAM,IAAI,KAAV,CAAW,EAAE,CAAE,EAAf,EACE,oBAAC,KAAD,EAAO,SAAS,CAAEoB,OAAO,CAACpB,KAA1B,EACE,oBAAC,IAAD,MACE,gGADF,CAEE,6RAFF,CAKE,8BALF,CAME,8BANF,CAOE,2BAAK,KAAK,CAAEN,MAAZ,EACE,oBAAC,SAAD,EACE,IAAI,CAAEY,UADR,CAEE,QAAQ,CAAC,IAFX,CAGE,OAAO,CAAE,CAAC,cAAD,CAHX,EADF,CAPF,CAcE,8BAdF,CAgBE,wFAhBF,CAkBE,2BAAK,KAAK,CAAEZ,MAAZ,EACE,oBAAC,SAAD,EACE,IAAI,CAAEc,UADR,CAEE,QAAQ,CAAC,IAFX,CAGE,OAAO,CAAE,CAAC,cAAD,CAHX,EADF,CAlBF,CAyBE,8BAzBF,CA2BE,mDA3BF,CA4BE,2BAAK,KAAK,CAAEd,MAAZ,EACE,oBAAC,SAAD,EACE,IAAI,CAAEe,aADR,CAEE,QAAQ,CAAC,IAFX,CAGE,OAAO,CAAE,CAAC,cAAD,CAHX,EADF,CA5BF,CAmCE,8BAnCF,CAqCE,yIArCF,CAsCE,2BAAK,KAAK,CAAEf,MAAZ,EACE,oBAAC,SAAD,EACE,IAAI,CAAEgB,OADR,CAEE,QAAQ,CAAC,IAFX,CAGE,OAAO,CAAE,CAAC,cAAD,CAHX,EADF,CAtCF,CA6CE,8BA7CF,CA+CE,yEA/CF,CAgDE,2BAAK,KAAK,CAAEhB,MAAZ,EACE,oBAAC,SAAD,EACE,IAAI,CAAEiB,QADR,CAEE,QAAQ,CAAC,IAFX,CAGE,OAAO,CAAE,CAAC,cAAD,CAHX,EADF,CAhDF,CAuDE,8BAvDF,CAyDE,iDAzDF,CA0DE,2BAAK,KAAK,CAAEjB,MAAZ,EACE,oBAAC,SAAD,EACE,IAAI,CAAEkB,KADR,CAEE,QAAQ,CAAC,IAFX,CAGE,OAAO,CAAE,CAAC,cAAD,CAHX,EADF,CA1DF,CAiEE,8BAjEF,CAmEE,mEAnEF,CAoEE,2BAAK,KAAK,CAAElB,MAAZ,EACE,oBAAC,SAAD,EACE,IAAI,CAAEmB,UADR,CAEE,QAAQ,CAAC,IAFX,CAGE,OAAO,CAAE,CAAC,cAAD,CAHX,EADF,CApEF,CA2EE,8BA3EF,CA6EE,6DA7EF,CA8EE,2BAAK,KAAK,CAAEnB,MAAZ,EACE,oBAAC,SAAD,EACE,IAAI,CAAEoB,SADR,CAEE,QAAQ,CAAC,IAFX,CAGE,OAAO,CAAE,CAAC,cAAD,CAHX,EADF,CA9EF,CAqFE,8BArFF,CAuFE,6DAvFF,CAwFE,2BAAK,KAAK,CAAEpB,MAAZ,EACE,oBAAC,SAAD,EACE,IAAI,CAAEqB,QADR,CAEE,QAAQ,CAAC,IAFX,CAGE,OAAO,CAAE,CAAC,cAAD,CAHX,EADF,CAxFF,CAgGE,8BAhGF,CAkGE,oDAlGF,CAmGE,2BAAK,KAAK,CAAErB,MAAZ,EACE,oBAAC,SAAD,EACE,IAAI,CAAEsB,QADR,CAEE,QAAQ,CAAC,IAFX,CAGE,OAAO,CAAE,CAAC,cAAD,CAHX,EADF,CAnGF,CADF,CADF,CANF,CADF,CAwHD,C,wBA9HsB9B,S,EAkIzB,cAAgBI,CAAAA,UAAU,CAACQ,MAAD,CAAV,CAAmBmB,UAAnB,CAAhB","sourcesContent":["import React, { Component } from 'react';\nimport Prism from \"prismjs\"\nimport { Grid, Paper, withStyles, List } from \"@material-ui/core\";\n\nimport '../../ReactJs/styles.css'\nimport Sidebar from '../sidebar';\nimport PrismCode from '../../ReactJs/prismCode';\n\n\nconst titles = { backgroundColor: '#F0F8FF', padding: '1px', fontSize: '16px' }\n\nconst styles = theme => ({\n  paper: {\n    margin: theme.spacing(1),\n    padding: theme.spacing(1)\n  },\n  smMargin: {\n    margin: theme.spacing(1)\n  },\n  actionDiv: {\n    textAlign: \"center\"\n  }\n})\n\n\nconst childsFile = `\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport numpy as np\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')\n\ndf = pd.read_csv(\"customer_churn.csv\")\n\ndf.Churn.value_counts()\n517400/ df.shape[0]\n`.trim();\n\nconst customerID = `\ndf.drop('customerID',axis='columns',inplace=True)\ndf.TotalCharges.values\n\npd.to_numeric(df.TotalCharges,errors='coerce').isnull()\ndf[pd.to_numeric(df.TotalCharges,errors='coerce').isnull()]\n\ndf.iloc[488].TotalCharges\ndf[df.TotalCharges!=' '].shape\ndf1 = df[df.TotalCharges!=' ']\n\ndf1.TotalCharges = pd.to_numeric(df1.TotalCharges)\ndf1.TotalCharges.values\ndf1[df1.Churn=='No']\n`.trim();\n\nconst visualization = `\ntenure_churn_no = df1[df1.Churn=='No'].tenure\ntenure_churn_yes = df1[df1.Churn=='Yes'].tenure\n\nplt.xlabel(\"tenure\")\nplt.ylabel(\"Number Of Customers\")\nplt.title(\"Customer Churn Prediction Visualiztion\")\n\nblood_sugar_men = [113, 85, 90, 150, 149, 88, 93, 115, 135, 80, 77, 82, 129]\nblood_sugar_women = [67, 98, 89, 120, 133, 150, 84, 69, 89, 79, 120, 112, 100]\n\nplt.hist([tenure_churn_yes, tenure_churn_no], rwidth=0.95, color=['green','red'],label=['Churn=Yes','Churn=No'])\nplt.legend()\n\n\nmc_churn_no = df1[df1.Churn=='No'].MonthlyCharges      \nmc_churn_yes = df1[df1.Churn=='Yes'].MonthlyCharges      \n\nplt.xlabel(\"Monthly Charges\")\nplt.ylabel(\"Number Of Customers\")\nplt.title(\"Customer Churn Prediction Visualiztion\")\n\nblood_sugar_men = [113, 85, 90, 150, 149, 88, 93, 115, 135, 80, 77, 82, 129]\nblood_sugar_women = [67, 98, 89, 120, 133, 150, 84, 69, 89, 79, 120, 112, 100]\n\nplt.hist([mc_churn_yes, mc_churn_no], rwidth=0.95, color=['green','red'],label=['Churn=Yes','Churn=No'])\nplt.legend()\n\ndef print_unique_col_values(df):                                                        #Many of the columns are yes, no.\n       for column in df:\n            if df[column].dtypes=='object':\n                print(f'{column}: {df[column].unique()}') \n                \nprint_unique_col_values(df1)\n`.trim();\n\nconst service = `\ndf1.replace('No internet service','No',inplace=True)\ndf1.replace('No phone service','No',inplace=True)\n\nprint_unique_col_values(df1)\n\n#Convert Yes and No to 1 or 0.\nyes_no_columns = ['Partner','Dependents','PhoneService','MultipleLines','OnlineSecurity','OnlineBackup',\n                  'DeviceProtection','TechSupport','StreamingTV','StreamingMovies','PaperlessBilling','Churn']\nfor col in yes_no_columns:\n    df1[col].replace({'Yes': 1,'No': 0},inplace=True)\n    \n    \nfor col in df1:\n    print(f'{col}: {df1[col].unique()}') \n    \ndf1['gender'].replace({'Female':1,'Male':0},inplace=True)\ndf1.gender.unique()\n`.trim();\n\nconst encoding = `\ndf2 = pd.get_dummies(data=df1, columns=['InternetService','Contract','PaymentMethod'])\ndf2.columns\n\ncols_to_scale = ['tenure','MonthlyCharges','TotalCharges']\n\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\ndf2[cols_to_scale] = scaler.fit_transform(df2[cols_to_scale])\n\nfor col in df2:\n    print(f'{col}: {df2[col].unique()}')\n`.trim();\n\nconst split = `\nX = df2.drop('Churn',axis='columns')\ny = testLabels = df2.Churn.astype(np.float32)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=15, stratify=y)\n\ny_train.value_counts()\ny.value_counts()\n5163/1869\n\nX_train.shape\nlen(X_train.columns)\n`.trim();\n\nconst regression = `\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report\n\ndef log_reg(X_train, y_train, X_test, y_test, weights):\n    if weights==-1:\n        model = LogisticRegression()\n    else:\n        model = LogisticRegression(class_weight={0:weights[0], 1:weights[1]})\n\n    model.fit(X_train, y_train)\n    acc = model.score(X_test, y_test)\n    print(\"Accuracy\", acc)\n\n    y_pred = model.predict(X_test)\n    print(\"preds\", y_pred[:5])\n\n    cl_rep = classification_report(y_test,y_pred)\n    print(cl_rep)\n    \nweights = -1                                                      # pass -1 to use Logistics Regression without weights.\nlog_reg(X_train, y_train, X_test, y_test, weights)\n\nweights = [1, 1.5]                                                # pass -1 to use Logistics Regression without weights.\nlog_reg(X_train, y_train, X_test, y_test, weights)\n`.trim();\n\nconst skewdness = `\n# Method1: Undersampling\ncount_class_0, count_class_1 = df1.Churn.value_counts()\n\ndf_class_0 = df2[df2['Churn'] == 0]                               # Divide by class.\ndf_class_1 = df2[df2['Churn'] == 1]\n\n\n# Undersample 0-class and concat the DataFrames of both class.\ndf_class_0_under = df_class_0.sample(count_class_1)\ndf_test_under = pd.concat([df_class_0_under, df_class_1], axis=0)\n\nprint('Random under-sampling:')\nprint(df_test_under.Churn.value_counts())\n\nX = df_test_under.drop('Churn',axis='columns')\ny = df_test_under['Churn']\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=15, stratify=y)\n\n\ny_train.value_counts()                                                        # Number of classes in training Data.\n`.trim();\n\nconst applying = `\nweights = -1                                                    # pass -1 to use Logistics Regression without weights\nlog_reg(X_train, y_train, X_test, y_test, weights)\n\n\n# Method2: Oversampling\n# Oversample 1-class and concat the DataFrames of both classes\ndf_class_1_over = df_class_1.sample(count_class_0, replace=True)\ndf_test_over = pd.concat([df_class_0, df_class_1_over], axis=0)\n\nprint('Random over-sampling:', df_test_over.Churn.value_counts())\n\nX = df_test_over.drop('Churn',axis='columns')\ny = df_test_over['Churn']\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=15, stratify=y)\n\n\ny_train.value_counts()                                                        # Number of classes in training Data.\n`.trim();\n\nconst logistic = `\nweights = -1                                                # pass -1 to use Logistics Regression without weights.\nlog_reg(X_train, y_train, X_test, y_test, weights)\n\n# Method3: SMOTE\nX = df2.drop('Churn',axis='columns')\ny = df2['Churn']\n\nfrom imblearn.over_sampling import SMOTE\nsmote = SMOTE(sampling_strategy='minority')\nX_sm, y_sm = smote.fit_sample(X, y)\n\ny_sm.value_counts()\n\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_sm, y_sm, test_size=0.2, random_state=15, stratify=y_sm)\n\ny_train.value_counts()\n\n\n# Logistic Regression\nweights = -1                                                 # pass -1 to use Logistics Regression without weights.\nlog_reg(X_train, y_train, X_test, y_test, weights)\n\n\ndf2.Churn.value_counts()                                    # Method4: Use of Ensemble with undersampling.\n\nX = df2.drop('Churn',axis='columns')                        # Regain Original features and labels.\ny = df2['Churn']\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=15, stratify=y)\n\ny_train.value_counts()\nmodel = LogisticRegression()\n\ndf3 = X_train.copy()\ndf3['Churn'] = y_train\ndf3.head()\n\ndf3_class0 = df3[df3.Churn==0]\ndf3_class1 = df3[df3.Churn==1]\n\ndef get_train_batch(df_majority, df_minority, start, end):\n    df_train = pd.concat([df_majority[start:end], df_minority], axis=0)\n    X_train = df_train.drop('Churn', axis='columns')\n    y_train = df_train.Churn\n    return X_train, y_train    \n    \nX_train, y_train = get_train_batch(df3_class0, df3_class1, 0, 1495)\nmodel1 = LogisticRegression()\nmodel1.fit(X_train, y_train)\ny_pred1 = model1.predict(X_test)\n\nX_train, y_train = get_train_batch(df3_class0, df3_class1, 1495, 2990)\nmodel2 = LogisticRegression()\nmodel2.fit(X_train, y_train)\ny_pred2 = model2.predict(X_test)\n\nX_train, y_train = get_train_batch(df3_class0, df3_class1, 2990, 4130)\nmodel3 = LogisticRegression()\nmodel3.fit(X_train, y_train)\ny_pred3 = model3.predict(X_test)\n\nlen(y_pred1)\n\ny_pred_final = y_pred1.copy()\nfor i in range(len(y_pred1)):\n    n_ones = y_pred1[i] + y_pred2[i] + y_pred3[i]\n    if n_ones>1:\n        y_pred_final[i] = 1\n    else:\n        y_pred_final[i] = 0\n        \n        \ncl_rep = classification_report(y_test, y_pred_final)\nprint(cl_rep)\n`.trim();\n\n\n\nclass Imbalanced extends Component {\n  componentDidMount() {\n    setTimeout(() => Prism.highlightAll(), 0)\n  }\n  render() {\n    const { classes } = this.props;\n    return (\n      <Grid container>\n        <Grid item xs={2}>\n          <Paper className={classes.paper}>\n            <h4><Sidebar /></h4>\n          </Paper>\n        </Grid>\n        <Grid item xs={10}>\n          <Paper className={classes.paper}>\n            <List>\n              <h3>Handle imbalanced data in churn prediction. Logistic Regression</h3>\n              <i>Customer churn prediction is to measure why customers are leaving a business.\n                Looking at customer churn in telecom business. We will build a deep learning model to predict\n                the churn and use precision,recall, f1-score to measure performance of our model.</i>\n              <br />\n              <br />\n              <div style={titles}>\n                <PrismCode\n                  code={childsFile}\n                  language=\"js\"\n                  plugins={[\"line-numbers\"]}\n                />\n              </div>\n              <br />\n\n              <h3>First of all, drop customerID column as it is of no use</h3>\n\n              <div style={titles}>\n                <PrismCode\n                  code={customerID}\n                  language=\"js\"\n                  plugins={[\"line-numbers\"]}\n                />\n              </div>\n              <br />\n\n              <h3>Data Visualization</h3>\n              <div style={titles}>\n                <PrismCode\n                  code={visualization}\n                  language=\"js\"\n                  plugins={[\"line-numbers\"]}\n                />\n              </div>\n              <br />\n\n              <h3>Some of the columns have no internet service or no phone service, that can be replaced with a simple No.</h3>\n              <div style={titles}>\n                <PrismCode\n                  code={service}\n                  language=\"js\"\n                  plugins={[\"line-numbers\"]}\n                />\n              </div>\n              <br />\n\n              <h3>One hot encoding for categorical columns</h3>\n              <div style={titles}>\n                <PrismCode\n                  code={encoding}\n                  language=\"js\"\n                  plugins={[\"line-numbers\"]}\n                />\n              </div>\n              <br />\n\n              <h3>Train test split</h3>\n              <div style={titles}>\n                <PrismCode\n                  code={split}\n                  language=\"js\"\n                  plugins={[\"line-numbers\"]}\n                />\n              </div>\n              <br />\n\n              <h3>Use logistic regression classifier</h3>\n              <div style={titles}>\n                <PrismCode\n                  code={regression}\n                  language=\"js\"\n                  plugins={[\"line-numbers\"]}\n                />\n              </div>\n              <br />\n\n              <h3>Mitigating Skewdness of Data</h3>\n              <div style={titles}>\n                <PrismCode\n                  code={skewdness}\n                  language=\"js\"\n                  plugins={[\"line-numbers\"]}\n                />\n              </div>\n              <br />\n\n              <h3>Applying Logistic Regression</h3>\n              <div style={titles}>\n                <PrismCode\n                  code={applying}\n                  language=\"js\"\n                  plugins={[\"line-numbers\"]}\n                />\n              </div>\n\n              <br />\n\n              <h3>Logistic Regression</h3>\n              <div style={titles}>\n                <PrismCode\n                  code={logistic}\n                  language=\"js\"\n                  plugins={[\"line-numbers\"]}\n                />\n              </div>\n            </List>\n          </Paper>\n        </Grid>\n      </Grid>\n    )\n  }\n}\n\n\nexport default (withStyles(styles)(Imbalanced));\n"]},"metadata":{},"sourceType":"module"}