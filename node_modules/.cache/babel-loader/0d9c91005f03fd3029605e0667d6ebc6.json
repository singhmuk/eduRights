{"ast":null,"code":"import _classCallCheck from\"/home/mukeshs/Projects/edurights/node_modules/@babel/runtime/helpers/esm/classCallCheck\";import _createClass from\"/home/mukeshs/Projects/edurights/node_modules/@babel/runtime/helpers/esm/createClass\";import _possibleConstructorReturn from\"/home/mukeshs/Projects/edurights/node_modules/@babel/runtime/helpers/esm/possibleConstructorReturn\";import _getPrototypeOf from\"/home/mukeshs/Projects/edurights/node_modules/@babel/runtime/helpers/esm/getPrototypeOf\";import _inherits from\"/home/mukeshs/Projects/edurights/node_modules/@babel/runtime/helpers/esm/inherits\";import React,{Component}from'react';import Prism from\"prismjs\";import{Grid,Paper,withStyles,List}from\"@material-ui/core\";import'../../ReactJs/styles.css';import Sidebar from'../sidebar';import PrismCode from'../../ReactJs/prismCode';import Sigmoid from'../../../assets/ML/primer.PNG';import PCAS from'../../../assets/ML/pca1.png';import PCAS2 from'../../../assets/ML/pca2.png';import PCAS3 from'../../../assets/ML/pca3.png';import PCAS4 from'../../../assets/ML/pca4.png';var titles={backgroundColor:'#F0F8FF',padding:'1px',fontSize:'16px'};var redesign={height:350,width:600};var styles=function styles(theme){return{paper:{margin:theme.spacing(1),padding:theme.spacing(1)},smMargin:{margin:theme.spacing(1)},actionDiv:{textAlign:\"center\"}};};var stack=\"\\nimport numpy as np\\n\\nclass PCA:\\n    def __init__(self, n_components):\\n        self.n_components = n_components\\n        self.components = None\\n        self.mean = None\\n\\n    def fit(self, X):\\n        self.mean = np.mean(X, axis=0)\\n        X = X - self.mean\\n        \\n        cov = np.cov(X.T)                                                 # covariance, function needs samples as columns\\n        eigenvalues, eigenvectors = np.linalg.eig(cov)                                      # eigenvalues, eigenvectors                                           \\n\\n                                               # -> eigenvector v = [:,i] column vector, transpose for easier calculations\\n                                                                                            # sort eigenvectors\\n        eigenvectors = eigenvectors.T\\n        idxs = np.argsort(eigenvalues)[::-1]\\n        eigenvalues = eigenvalues[idxs]\\n        eigenvectors = eigenvectors[idxs]\\n        \\n        self.components = eigenvectors[0 : self.n_components]                               # store first n eigenvectors\\n\\n    def transform(self, X):\\n        X = X - self.mean\\n        return np.dot(X, self.components.T)\\n\".trim();var testings=\"\\nif __name__ == \\\"__main__\\\":\\n    import matplotlib.pyplot as plt\\n    from sklearn import datasets\\n\\n    # data = datasets.load_digits()\\n    data = datasets.load_iris()\\n    X = data.data\\n    y = data.target\\n\\n    pca = PCA(2)                                                # Project the data onto the 2 primary principal components\\n    pca.fit(X)\\n    X_projected = pca.transform(X)\\n\\n    print(\\\"Shape of X:\\\", X.shape)\\n    print(\\\"Shape of transformed X:\\\", X_projected.shape)\\n\\n    x1 = X_projected[:, 0]\\n    x2 = X_projected[:, 1]\\n\\n    plt.scatter(x1, x2, c=y, edgecolor=\\\"none\\\", alpha=0.8, cmap=plt.cm.get_cmap(\\\"viridis\\\", 3))\\n\\n    plt.xlabel(\\\"Principal Component 1\\\")\\n    plt.ylabel(\\\"Principal Component 2\\\")\\n    plt.colorbar()\\n    plt.show()\\n\".trim();var pcaps=\"\\nimport pandas as pd\\nfrom sklearn.datasets import load_digits\\nfrom sklearn.preprocessing import StandardScaler\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.linear_model import LogisticRegression\\nfrom matplotlib import pyplot as plt\\n%matplotlib inline\\n\\n\\ndataset = load_digits()\\ndataset.keys()\\ndataset.data.shape\\ndataset.data[0]\\ndataset.data[0].reshape(8,8)\\n\\nplt.gray()\\nplt.matshow(dataset.data[0].reshape(8,8))\\nplt.matshow(dataset.data[9].reshape(8,8))\\ndataset.target[:5]\\n\\ndf = pd.DataFrame(dataset.data, columns=dataset.feature_names)\\ndf.head()\\ndataset.target\\ndf.describe()\\n\\nX = df\\ny = dataset.target\\n\\n\\nscaler = StandardScaler()\\nX_scaled = scaler.fit_transform(X)\\nX_scaled\\n\\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=30)\\n\\nmodel = LogisticRegression()\\nmodel.fit(X_train, y_train)\\nmodel.score(X_test, y_test)\\n\".trim();var reduceDim=\"\\nfrom sklearn.decomposition import PCA\\nfrom sklearn.linear_model import LogisticRegression\\n\\npca = PCA(0.95)\\nX_pca = pca.fit_transform(X)\\npca.explained_variance_ratio_\\npca.n_components_\\nX_pca\\n\\nX_train_pca, X_test_pca, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=30)\\n\\nmodel = LogisticRegression(max_iter=1000)\\nmodel.fit(X_train_pca, y_train)\\nmodel.score(X_test_pca, y_test)\\n\\npca = PCA(n_components=2)                                              #Let's select only two components.\\nX_pca = pca.fit_transform(X)\\nX_pca.shape\\n\\npca.explained_variance_ratio_\\n\\n#see both combined retains 0.14+0.13=0.27 or 27% of important feature information.\\nX_train_pca, X_test_pca, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=30)\\n\\nmodel = LogisticRegression(max_iter=1000)\\nmodel.fit(X_train_pca, y_train)\\nmodel.score(X_test_pca, y_test)\\n\".trim();// const stack = ``.trim();\nvar PcaPy=/*#__PURE__*/function(_Component){_inherits(PcaPy,_Component);function PcaPy(){_classCallCheck(this,PcaPy);return _possibleConstructorReturn(this,_getPrototypeOf(PcaPy).apply(this,arguments));}_createClass(PcaPy,[{key:\"componentDidMount\",value:function componentDidMount(){setTimeout(function(){return Prism.highlightAll();},0);}},{key:\"render\",value:function render(){var classes=this.props.classes;return React.createElement(Grid,{container:true},React.createElement(Grid,{item:true,xs:2},React.createElement(Paper,{className:classes.paper},React.createElement(\"h4\",null,React.createElement(Sidebar,null)))),React.createElement(Grid,{item:true,xs:10},React.createElement(Paper,{className:classes.paper},React.createElement(List,null,React.createElement(\"h3\",null,\"Principal Component Analysis (Unsupervised)\"),\"Primarily used for dimensionality reduction in ML.\",React.createElement(\"br\",null),\"PCA is a process of figuring out most important features/ principle components that has the most impact on the target variable.\",React.createElement(\"br\",null),React.createElement(\"br\",null),React.createElement(\"b\",null,\"Few things to keep ikn mind before using PCA:\"),React.createElement(\"ul\",null,React.createElement(\"li\",null,\"Scale Features befor applying PCA.\"),React.createElement(\"li\",null,\"Accuracy might drop.\")),React.createElement(\"br\",null),React.createElement(\"br\",null),React.createElement(\"b\",null,\"Steps:\"),React.createElement(\"ul\",null,React.createElement(\"li\",null,\"Load heart disease dataset in pandas dataframe.\"),React.createElement(\"li\",null,\"Remove outliers using Z score. Usual guideline is to remove anything that has Z score greater than 3 formula or Z score less-than -3,\"),React.createElement(\"li\",null,\"Convert text columns to numbers using label encoding and one hot encoding.\"),React.createElement(\"li\",null,\"Apply scaling.\"),React.createElement(\"li\",null,\"Build a classification model using various methods (SVM, logistic regression, random forest) and check which model gives you the best accuracy.\"),React.createElement(\"li\",null,\"Now use PCA to reduce dimensions, retrain your model and see what impact it has on your model in terms of accuracy. Keep in mind that many times doing PCA reduces the accuracy but computation is much lighter and that's the trade off you need to consider while building models in real life.\")),React.createElement(\"br\",null),React.createElement(\"div\",{style:titles},React.createElement(PrismCode,{code:pcaps,language:\"js\",plugins:[\"line-numbers\"]})),React.createElement(\"br\",null),React.createElement(\"h3\",null,\"Use PCA to reduce dimensions\"),\"Use components such that 95% of variance is retained\",React.createElement(\"div\",{style:titles},React.createElement(PrismCode,{code:reduceDim,language:\"js\",plugins:[\"line-numbers\"]})),React.createElement(\"br\",null),React.createElement(\"i\",null,\"We get less accuancy (~60%) as using only 2 components did not retain much of the feature information. However in real life you will find many cases where using 2 or few PCA components can still give you a pretty good accuracy.\"),React.createElement(\"br\",null),React.createElement(\"h3\",null,\"PCA\"),\"High dimensionality means that the dataset has a large number of features. The primary problem associated with high-dimensionality in the machine learning field is model overfitting.\",React.createElement(\"br\",null),React.createElement(\"br\",null),\"Many algorithms that work fine in low dimensions become intractable when the input is high-dimensional.\",React.createElement(\"br\",null),React.createElement(\"br\",null),\"The ability to generalize correctly becomes exponentially harder as the dimensionality of the training dataset grows, as the training set covers a dwindling fraction of the input space. Models also become more efficient as the reduced feature set boosts learning rates and diminishes computation costs by removing redundant features.\",React.createElement(\"br\",null),React.createElement(\"br\",null),\"PCA can also be used to filter noisy datasets, such as image compression.\",React.createElement(\"br\",null),React.createElement(\"br\",null),React.createElement(\"b\",null,\"Primer:\"),React.createElement(\"br\",null),\"PCA makes maximum variability in the dataset more visible by rotating the axes. PCA identifies a list of the principal axes to describe the underlying dataset before ranking them according to the amount of variance captured by each.\",React.createElement(\"img\",{src:Sigmoid,alt:\"Equations\",className:\"responsive\",style:redesign}),React.createElement(\"br\",null),React.createElement(\"i\",null,\"the first of the principal components (PC1) is a synthetic variable constructed as a linear combination to determine the magnitude and the direction of the maximum variance in the dataset. This component has the highest variability of all the components and therefore the most information. The PC2 is also a synthetic linear combination which captures the remaining variance in the data set and is not correlated with PC1. The following principal components similarly capture the remaining variation without being correlated with the previous component.\"),React.createElement(\"br\",null),React.createElement(\"br\",null),\"PCA is an unsupervised learning algorithm as the directions of these components is calculated purely from the explanatory feature set without any reference to response variables.\",React.createElement(\"br\",null),React.createElement(\"br\",null),\"The number of feature combinations is equal to the number of dimensions of the dataset and in general set the maximum number of PCAs which can be constructed.\",React.createElement(\"img\",{src:PCAS,alt:\"Equations\",className:\"responsive\",style:redesign}),React.createElement(\"br\",null),React.createElement(\"img\",{src:PCAS2,alt:\"Equations\",className:\"responsive\",style:redesign}),React.createElement(\"br\",null),React.createElement(\"br\",null),React.createElement(\"i\",null,\"Each blue point corresponds to an observation, and each principal component reduces the three dimensions to two. The algorithm finds a pair of orthogonal vectors (red arrows) that define a lower-dimensional space (grey plane) to capture as much variance as possible from the original dataset.\"),React.createElement(\"br\",null),React.createElement(\"br\",null),React.createElement(\"b\",null,\"Measurement:\"),\"Eigenvectors and eigenvalues are measures used to quantify the direction and the magnitude of the variation captured by each axis. Eigenvector describes the angle or direction of the axis through the data space, and the eigenvalue quantifies the magnitude of the variance of the data on the axis.\",React.createElement(\"img\",{src:PCAS3,alt:\"Equations\",className:\"responsive\",style:redesign}),React.createElement(\"ul\",null,React.createElement(\"li\",null,React.createElement(\"i\",null,\"A is an x n matrix, \\u019B is the eigenvalue, and the X is the eigenvector.\")),React.createElement(\"li\",null,React.createElement(\"i\",null,\"The number of feature combinations is equal to the number of dimensions of the dataset.\",React.createElement(\"b\",null,\"Ex.\"),\" A dataset with ten features will have ten eigenvalues/eigenvector combinations.\"))),React.createElement(\"br\",null),\"The correlation between each principal component should be zero as subsequent components capture the remaining variance. Correlation between any pair of eigenvalue/eigenvector is zero so that the axes are orthogonal(perpendicular), to each other in the data space.\",React.createElement(\"br\",null),React.createElement(\"br\",null),\"The line which maximizes the variance of the data once it is projected into the data space is equivalent to finding the path which minimizes the least-squares distance of the projection.\",React.createElement(\"img\",{src:PCAS4,alt:\"Equations\",className:\"responsive\",style:redesign}),React.createElement(\"br\",null),React.createElement(\"b\",null,\"Assumptions:\"),React.createElement(\"ul\",null,React.createElement(\"li\",null,React.createElement(\"b\",null,\"Sample size: \"),\"Minimum of 150 observations and ideally a 5:1 ratio of observation to features.\"),React.createElement(\"li\",null,React.createElement(\"b\",null,\"Correlations: \"),\"The feature set is correlated, so the reduced feature set effectively represents the original data space.\"),React.createElement(\"li\",null,React.createElement(\"b\",null,\"Linearity: \"),\"All variables exhibit a constant multivariate normal relationship, and principal components are a linear combination of the original features.\"),React.createElement(\"li\",null,React.createElement(\"b\",null,\"Outliers: \"),\"No significant outliers in the data as these can have a disproportionate influence on the results.\"),React.createElement(\"li\",null,React.createElement(\"b\",null,\"Large variance implies more structure: \"),\"High variance axes are treated as principal components, while low variance axes are treated as noise and discarded.\")),React.createElement(\"br\",null),React.createElement(\"b\",null,\"PCA Limitations :\"),React.createElement(\"br\",null),React.createElement(\"ul\",null,React.createElement(\"li\",null,React.createElement(\"b\",null,\"Model performance: \"),\"PCA can lead to a reduction in model performance on datasets with no or low feature correlation or does not meet the assumptions of linearity.\"),React.createElement(\"li\",null,React.createElement(\"b\",null,\"Classification accuracy: \"),\"Variance based PCA framework does not consider the differentiating characteristics of the classes. Also, the information that distinguishes one class from another might be in the low variance components and may be discarded.\"),React.createElement(\"li\",null,React.createElement(\"b\",null,\"Outliers: \"),\"PCA is also affected by outliers, and normalization of the data needs to be an essential component of any workflow.\"),React.createElement(\"li\",null,React.createElement(\"b\",null,\"Interpretability: \"),\"Each principal component is a combination of original features and does not allow for the individual feature importance to be recognized.\")),React.createElement(\"br\",null),React.createElement(\"div\",{style:titles},React.createElement(PrismCode,{code:stack,language:\"js\",plugins:[\"line-numbers\"]})),React.createElement(\"br\",null),React.createElement(\"b\",null,\"Testing\"),React.createElement(\"div\",{style:titles},React.createElement(PrismCode,{code:testings,language:\"js\",plugins:[\"line-numbers\"]}))))));}}]);return PcaPy;}(Component);export default withStyles(styles)(PcaPy);","map":{"version":3,"sources":["/home/mukeshs/Projects/edurights/src/components/ml/deepMl/pcaPy.js"],"names":["React","Component","Prism","Grid","Paper","withStyles","List","Sidebar","PrismCode","Sigmoid","PCAS","PCAS2","PCAS3","PCAS4","titles","backgroundColor","padding","fontSize","redesign","height","width","styles","theme","paper","margin","spacing","smMargin","actionDiv","textAlign","stack","trim","testings","pcaps","reduceDim","PcaPy","setTimeout","highlightAll","classes","props"],"mappings":"6kBAAA,MAAOA,CAAAA,KAAP,EAAgBC,SAAhB,KAAiC,OAAjC,CACA,MAAOC,CAAAA,KAAP,KAAkB,SAAlB,CACA,OAASC,IAAT,CAAeC,KAAf,CAAsBC,UAAtB,CAAkCC,IAAlC,KAA8C,mBAA9C,CAEA,MAAO,0BAAP,CACA,MAAOC,CAAAA,OAAP,KAAoB,YAApB,CACA,MAAOC,CAAAA,SAAP,KAAsB,yBAAtB,CAEA,MAAOC,CAAAA,OAAP,KAAoB,+BAApB,CACA,MAAOC,CAAAA,IAAP,KAAiB,6BAAjB,CACA,MAAOC,CAAAA,KAAP,KAAkB,6BAAlB,CACA,MAAOC,CAAAA,KAAP,KAAkB,6BAAlB,CACA,MAAOC,CAAAA,KAAP,KAAkB,6BAAlB,CAGA,GAAMC,CAAAA,MAAM,CAAG,CAAEC,eAAe,CAAE,SAAnB,CAA8BC,OAAO,CAAE,KAAvC,CAA8CC,QAAQ,CAAE,MAAxD,CAAf,CAEA,GAAMC,CAAAA,QAAQ,CAAG,CACfC,MAAM,CAAE,GADO,CAEfC,KAAK,CAAE,GAFQ,CAAjB,CAKA,GAAMC,CAAAA,MAAM,CAAG,QAATA,CAAAA,MAAS,CAAAC,KAAK,QAAK,CACvBC,KAAK,CAAE,CACLC,MAAM,CAAEF,KAAK,CAACG,OAAN,CAAc,CAAd,CADH,CAELT,OAAO,CAAEM,KAAK,CAACG,OAAN,CAAc,CAAd,CAFJ,CADgB,CAKvBC,QAAQ,CAAE,CACRF,MAAM,CAAEF,KAAK,CAACG,OAAN,CAAc,CAAd,CADA,CALa,CAQvBE,SAAS,CAAE,CACTC,SAAS,CAAE,QADF,CARY,CAAL,EAApB,CAcA,GAAMC,CAAAA,KAAK,CAAG,0rCA4BZC,IA5BY,EAAd,CA8BA,GAAMC,CAAAA,QAAQ,CAAG,ywBA0BfD,IA1Be,EAAjB,CA4BA,GAAME,CAAAA,KAAK,CAAG,w5BAuCZF,IAvCY,EAAd,CAyCA,GAAMG,CAAAA,SAAS,CAAG,w4BA4BhBH,IA5BgB,EAAlB,CA8BA;GAGMI,CAAAA,K,iRACgB,CAClBC,UAAU,CAAC,iBAAMjC,CAAAA,KAAK,CAACkC,YAAN,EAAN,EAAD,CAA6B,CAA7B,CAAV,CACD,C,uCACQ,IACCC,CAAAA,OADD,CACa,KAAKC,KADlB,CACCD,OADD,CAEP,MACE,qBAAC,IAAD,EAAM,SAAS,KAAf,EACE,oBAAC,IAAD,EAAM,IAAI,KAAV,CAAW,EAAE,CAAE,CAAf,EACE,oBAAC,KAAD,EAAO,SAAS,CAAEA,OAAO,CAACd,KAA1B,EACE,8BAAI,oBAAC,OAAD,MAAJ,CADF,CADF,CADF,CAME,oBAAC,IAAD,EAAM,IAAI,KAAV,CAAW,EAAE,CAAE,EAAf,EACE,oBAAC,KAAD,EAAO,SAAS,CAAEc,OAAO,CAACd,KAA1B,EACE,oBAAC,IAAD,MAGE,4EAHF,sDAKE,8BALF,mIAOE,8BAPF,CAQE,8BARF,CASE,6EATF,CAUE,8BACE,mEADF,CAEE,qDAFF,CAVF,CAcE,8BAdF,CAeE,8BAfF,CAgBE,sCAhBF,CAiBE,8BACE,gFADF,CAEE,sKAFF,CAGE,2GAHF,CAIE,+CAJF,CAKE,gLALF,CAME,kUANF,CAjBF,CAyBE,8BAzBF,CA0BE,2BAAK,KAAK,CAAET,MAAZ,EACE,oBAAC,SAAD,EACE,IAAI,CAAEkB,KADR,CAEE,QAAQ,CAAC,IAFX,CAGE,OAAO,CAAE,CAAC,cAAD,CAHX,EADF,CA1BF,CAiCE,8BAjCF,CAmCE,6DAnCF,wDAqCE,2BAAK,KAAK,CAAElB,MAAZ,EACE,oBAAC,SAAD,EACE,IAAI,CAAEmB,SADR,CAEE,QAAQ,CAAC,IAFX,CAGE,OAAO,CAAE,CAAC,cAAD,CAHX,EADF,CArCF,CA4CE,8BA5CF,CA6CE,mQA7CF,CA8CE,8BA9CF,CAgDE,oCAhDF,0LAmDE,8BAnDF,CAoDE,8BApDF,2GAsDE,8BAtDF,CAuDE,8BAvDF,iVA2DE,8BA3DF,CA4DE,8BA5DF,6EA8DE,8BA9DF,CA+DE,8BA/DF,CAgEE,uCAhEF,CAiEE,8BAjEF,4OAqEE,2BAAK,GAAG,CAAExB,OAAV,CAAmB,GAAG,CAAC,WAAvB,CAAmC,SAAS,CAAC,YAA7C,CAA0D,KAAK,CAAES,QAAjE,EArEF,CAsEE,8BAtEF,CAwEE,ykBAxEF,CA8EE,8BA9EF,CA+EE,8BA/EF,sLAkFE,8BAlFF,CAmFE,8BAnFF,kKAsFE,2BAAK,GAAG,CAAER,IAAV,CAAgB,GAAG,CAAC,WAApB,CAAgC,SAAS,CAAC,YAA1C,CAAuD,KAAK,CAAEQ,QAA9D,EAtFF,CAuFE,8BAvFF,CAyFE,2BAAK,GAAG,CAAEP,KAAV,CAAiB,GAAG,CAAC,WAArB,CAAiC,SAAS,CAAC,YAA3C,CAAwD,KAAK,CAAEO,QAA/D,EAzFF,CA0FE,8BA1FF,CA2FE,8BA3FF,CA4FE,oUA5FF,CAgGE,8BAhGF,CAiGE,8BAjGF,CAkGE,4CAlGF,4SAsGE,2BAAK,GAAG,CAAEN,KAAV,CAAiB,GAAG,CAAC,WAArB,CAAiC,SAAS,CAAC,YAA3C,CAAwD,KAAK,CAAEM,QAA/D,EAtGF,CAuGE,8BACE,8BAAI,2GAAJ,CADF,CAEE,8BAAI,uHACF,mCADE,oFAAJ,CAFF,CAvGF,CA4GE,8BA5GF,4QAgHE,8BAhHF,CAiHE,8BAjHF,8LAoHE,2BAAK,GAAG,CAAEL,KAAV,CAAiB,GAAG,CAAC,WAArB,CAAiC,SAAS,CAAC,YAA3C,CAAwD,KAAK,CAAEK,QAA/D,EApHF,CAqHE,8BArHF,CAuHE,4CAvHF,CAwHE,8BACE,8BAAI,6CAAJ,mFADF,CAEE,8BAAI,8CAAJ,6GAFF,CAIE,8BAAI,2CAAJ,kJAJF,CAME,8BAAI,0CAAJ,sGANF,CAQE,8BAAI,uEAAJ,uHARF,CAxHF,CAoIE,8BApIF,CAqIE,iDArIF,CAsIE,8BAtIF,CAuIE,8BACE,8BAAI,mDAAJ,kJADF,CAGE,8BAAI,yDAAJ,oOAHF,CAME,8BAAI,0CAAJ,uHANF,CAQE,8BAAI,kDAAJ,6IARF,CAvIF,CAkJE,8BAlJF,CAoJE,2BAAK,KAAK,CAAEJ,MAAZ,EACE,oBAAC,SAAD,EACE,IAAI,CAAEe,KADR,CAEE,QAAQ,CAAC,IAFX,CAGE,OAAO,CAAE,CAAC,cAAD,CAHX,EADF,CApJF,CA2JE,8BA3JF,CA6JE,uCA7JF,CA8JE,2BAAK,KAAK,CAAEf,MAAZ,EACE,oBAAC,SAAD,EACE,IAAI,CAAEiB,QADR,CAEE,QAAQ,CAAC,IAFX,CAGE,OAAO,CAAE,CAAC,cAAD,CAHX,EADF,CA9JF,CADF,CADF,CANF,CADF,CAmLD,C,mBAzLiB9B,S,EA4LpB,cAAgBI,CAAAA,UAAU,CAACgB,MAAD,CAAV,CAAmBa,KAAnB,CAAhB","sourcesContent":["import React, { Component } from 'react';\nimport Prism from \"prismjs\"\nimport { Grid, Paper, withStyles, List } from \"@material-ui/core\";\n\nimport '../../ReactJs/styles.css'\nimport Sidebar from '../sidebar';\nimport PrismCode from '../../ReactJs/prismCode';\n\nimport Sigmoid from '../../../assets/ML/primer.PNG'\nimport PCAS from '../../../assets/ML/pca1.png'\nimport PCAS2 from '../../../assets/ML/pca2.png'\nimport PCAS3 from '../../../assets/ML/pca3.png'\nimport PCAS4 from '../../../assets/ML/pca4.png'\n\n\nconst titles = { backgroundColor: '#F0F8FF', padding: '1px', fontSize: '16px' }\n\nconst redesign = {\n  height: 350,\n  width: 600\n}\n\nconst styles = theme => ({\n  paper: {\n    margin: theme.spacing(1),\n    padding: theme.spacing(1)\n  },\n  smMargin: {\n    margin: theme.spacing(1)\n  },\n  actionDiv: {\n    textAlign: \"center\"\n  }\n})\n\n\nconst stack = `\nimport numpy as np\n\nclass PCA:\n    def __init__(self, n_components):\n        self.n_components = n_components\n        self.components = None\n        self.mean = None\n\n    def fit(self, X):\n        self.mean = np.mean(X, axis=0)\n        X = X - self.mean\n        \n        cov = np.cov(X.T)                                                 # covariance, function needs samples as columns\n        eigenvalues, eigenvectors = np.linalg.eig(cov)                                      # eigenvalues, eigenvectors                                           \n\n                                               # -> eigenvector v = [:,i] column vector, transpose for easier calculations\n                                                                                            # sort eigenvectors\n        eigenvectors = eigenvectors.T\n        idxs = np.argsort(eigenvalues)[::-1]\n        eigenvalues = eigenvalues[idxs]\n        eigenvectors = eigenvectors[idxs]\n        \n        self.components = eigenvectors[0 : self.n_components]                               # store first n eigenvectors\n\n    def transform(self, X):\n        X = X - self.mean\n        return np.dot(X, self.components.T)\n`.trim();\n\nconst testings = `\nif __name__ == \"__main__\":\n    import matplotlib.pyplot as plt\n    from sklearn import datasets\n\n    # data = datasets.load_digits()\n    data = datasets.load_iris()\n    X = data.data\n    y = data.target\n\n    pca = PCA(2)                                                # Project the data onto the 2 primary principal components\n    pca.fit(X)\n    X_projected = pca.transform(X)\n\n    print(\"Shape of X:\", X.shape)\n    print(\"Shape of transformed X:\", X_projected.shape)\n\n    x1 = X_projected[:, 0]\n    x2 = X_projected[:, 1]\n\n    plt.scatter(x1, x2, c=y, edgecolor=\"none\", alpha=0.8, cmap=plt.cm.get_cmap(\"viridis\", 3))\n\n    plt.xlabel(\"Principal Component 1\")\n    plt.ylabel(\"Principal Component 2\")\n    plt.colorbar()\n    plt.show()\n`.trim();\n\nconst pcaps = `\nimport pandas as pd\nfrom sklearn.datasets import load_digits\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom matplotlib import pyplot as plt\n%matplotlib inline\n\n\ndataset = load_digits()\ndataset.keys()\ndataset.data.shape\ndataset.data[0]\ndataset.data[0].reshape(8,8)\n\nplt.gray()\nplt.matshow(dataset.data[0].reshape(8,8))\nplt.matshow(dataset.data[9].reshape(8,8))\ndataset.target[:5]\n\ndf = pd.DataFrame(dataset.data, columns=dataset.feature_names)\ndf.head()\ndataset.target\ndf.describe()\n\nX = df\ny = dataset.target\n\n\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\nX_scaled\n\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=30)\n\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train)\nmodel.score(X_test, y_test)\n`.trim();\n\nconst reduceDim = `\nfrom sklearn.decomposition import PCA\nfrom sklearn.linear_model import LogisticRegression\n\npca = PCA(0.95)\nX_pca = pca.fit_transform(X)\npca.explained_variance_ratio_\npca.n_components_\nX_pca\n\nX_train_pca, X_test_pca, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=30)\n\nmodel = LogisticRegression(max_iter=1000)\nmodel.fit(X_train_pca, y_train)\nmodel.score(X_test_pca, y_test)\n\npca = PCA(n_components=2)                                              #Let's select only two components.\nX_pca = pca.fit_transform(X)\nX_pca.shape\n\npca.explained_variance_ratio_\n\n#see both combined retains 0.14+0.13=0.27 or 27% of important feature information.\nX_train_pca, X_test_pca, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=30)\n\nmodel = LogisticRegression(max_iter=1000)\nmodel.fit(X_train_pca, y_train)\nmodel.score(X_test_pca, y_test)\n`.trim();\n\n// const stack = ``.trim();\n\n\nclass PcaPy extends Component {\n  componentDidMount() {\n    setTimeout(() => Prism.highlightAll(), 0)\n  }\n  render() {\n    const { classes } = this.props;\n    return (\n      <Grid container>\n        <Grid item xs={2}>\n          <Paper className={classes.paper}>\n            <h4><Sidebar /></h4>\n          </Paper>\n        </Grid>\n        <Grid item xs={10}>\n          <Paper className={classes.paper}>\n            <List>\n\n\n              <h3>Principal Component Analysis (Unsupervised)</h3>\n              Primarily used for dimensionality reduction in ML.\n              <br />\n              PCA is a process of figuring out most important features/ principle components that has the most impact on the target variable.\n              <br />\n              <br />\n              <b>Few things to keep ikn mind before using PCA:</b>\n              <ul>\n                <li>Scale Features befor applying PCA.</li>\n                <li>Accuracy might drop.</li>\n              </ul>\n              <br />\n              <br />\n              <b>Steps:</b>\n              <ul>\n                <li>Load heart disease dataset in pandas dataframe.</li>\n                <li>Remove outliers using Z score. Usual guideline is to remove anything that has Z score greater than 3 formula or Z score less-than -3,</li>\n                <li>Convert text columns to numbers using label encoding and one hot encoding.</li>\n                <li>Apply scaling.</li>\n                <li>Build a classification model using various methods (SVM, logistic regression, random forest) and check which model gives you the best accuracy.</li>\n                <li>Now use PCA to reduce dimensions, retrain your model and see what impact it has on your model in terms of accuracy. Keep in mind that many times doing PCA reduces the accuracy but computation is much lighter and that's the trade off you need to consider while building models in real life.</li>\n              </ul>\n              <br />\n              <div style={titles}>\n                <PrismCode\n                  code={pcaps}\n                  language=\"js\"\n                  plugins={[\"line-numbers\"]}\n                />\n              </div>\n              <br />\n\n              <h3>Use PCA to reduce dimensions</h3>\n              Use components such that 95% of variance is retained\n              <div style={titles}>\n                <PrismCode\n                  code={reduceDim}\n                  language=\"js\"\n                  plugins={[\"line-numbers\"]}\n                />\n              </div>\n              <br />\n              <i>We get less accuancy (~60%) as using only 2 components did not retain much of the feature information. However in real life you will find many cases where using 2 or few PCA components can still give you a pretty good accuracy.</i>\n              <br />\n\n              <h3>PCA</h3>\n              High dimensionality means that the dataset has a large number of features. The primary problem associated with\n              high-dimensionality in the machine learning field is model overfitting.\n              <br />\n              <br />\n              Many algorithms that work fine in low dimensions become intractable when the input is high-dimensional.\n              <br />\n              <br />\n              The ability to generalize correctly becomes exponentially harder as the dimensionality of the training dataset\n              grows, as the training set covers a dwindling fraction of the input space. Models also become more efficient as\n              the reduced feature set boosts learning rates and diminishes computation costs by removing redundant features.\n              <br />\n              <br />\n              PCA can also be used to filter noisy datasets, such as image compression.\n              <br />\n              <br />\n              <b>Primer:</b>\n              <br />\n              PCA makes maximum variability in the dataset more visible by rotating the axes. PCA identifies a list of the\n              principal axes to describe the underlying dataset before ranking them according to the amount of variance\n              captured by each.\n              <img src={Sigmoid} alt=\"Equations\" className=\"responsive\" style={redesign} />\n              <br />\n\n              <i>the first of the principal components (PC1) is a synthetic variable constructed as a linear combination to\n                determine the magnitude and the direction of the maximum variance in the dataset. This component has the\n                highest variability of all the components and therefore the most information. The PC2 is also a synthetic\n                linear combination which captures the remaining variance in the data set and is not correlated with PC1.\n                The following principal components similarly capture the remaining variation without being correlated with the\n                previous component.</i>\n              <br />\n              <br />\n              PCA is an unsupervised learning algorithm as the directions of these components is calculated purely from the\n              explanatory feature set without any reference to response variables.\n              <br />\n              <br />\n              The number of feature combinations is equal to the number of dimensions of the dataset and in general set the\n              maximum number of PCAs which can be constructed.\n              <img src={PCAS} alt=\"Equations\" className=\"responsive\" style={redesign} />\n              <br />\n\n              <img src={PCAS2} alt=\"Equations\" className=\"responsive\" style={redesign} />\n              <br />\n              <br />\n              <i>Each blue point corresponds to an observation, and each principal component reduces the three dimensions to\n                two. The algorithm finds a pair of orthogonal vectors (red arrows) that define a lower-dimensional space\n                (grey plane) to capture as much variance as possible from the original dataset.</i>\n\n              <br />\n              <br />\n              <b>Measurement:</b>\n              Eigenvectors and eigenvalues are measures used to quantify the direction and the magnitude of the\n              variation captured by each axis. Eigenvector describes the angle or direction of the axis through the\n              data space, and the eigenvalue quantifies the magnitude of the variance of the data on the axis.\n              <img src={PCAS3} alt=\"Equations\" className=\"responsive\" style={redesign} />\n              <ul>\n                <li><i>A is an x n matrix, Æ› is the eigenvalue, and the X is the eigenvector.</i></li>\n                <li><i>The number of feature combinations is equal to the number of dimensions of the dataset.\n                  <b>Ex.</b> A dataset with ten features will have ten eigenvalues/eigenvector combinations.</i></li>\n              </ul>\n              <br />\n              The correlation between each principal component should be zero as subsequent components capture the\n              remaining variance. Correlation between any pair of eigenvalue/eigenvector is zero so that the axes are\n              orthogonal(perpendicular), to each other in the data space.\n              <br />\n              <br />\n              The line which maximizes the variance of the data once it is projected into the data space is equivalent to\n              finding the path which minimizes the least-squares distance of the projection.\n              <img src={PCAS4} alt=\"Equations\" className=\"responsive\" style={redesign} />\n              <br />\n\n              <b>Assumptions:</b>\n              <ul>\n                <li><b>Sample size: </b>Minimum of 150 observations and ideally a 5:1 ratio of observation to features.</li>\n                <li><b>Correlations: </b>The feature set is correlated, so the reduced feature set effectively\n                  represents the original data space.</li>\n                <li><b>Linearity: </b>All variables exhibit a constant multivariate normal relationship, and\n                  principal components are a linear combination of the original features.</li>\n                <li><b>Outliers: </b>No significant outliers in the data as these can have a disproportionate\n                  influence on the results.</li>\n                <li><b>Large variance implies more structure: </b>High variance axes are treated as principal components,\n                  while low variance axes are treated as noise and discarded.</li>\n              </ul>\n\n              <br />\n              <b>PCA Limitations :</b>\n              <br />\n              <ul>\n                <li><b>Model performance: </b>PCA can lead to a reduction in model performance on datasets with no or\n                  low feature correlation or does not meet the assumptions of linearity.</li>\n                <li><b>Classification accuracy: </b>Variance based PCA framework does not consider the\n                  differentiating characteristics of the classes. Also, the information that distinguishes one class\n                  from another might be in the low variance components and may be discarded.</li>\n                <li><b>Outliers: </b>PCA is also affected by outliers, and normalization of the data needs to be an\n                  essential component of any workflow.</li>\n                <li><b>Interpretability: </b>Each principal component is a combination of original features and does\n                  not allow for the individual feature importance to be recognized.</li>\n              </ul>\n              <br />\n\n              <div style={titles}>\n                <PrismCode\n                  code={stack}\n                  language=\"js\"\n                  plugins={[\"line-numbers\"]}\n                />\n              </div>\n              <br />\n\n              <b>Testing</b>\n              <div style={titles}>\n                <PrismCode\n                  code={testings}\n                  language=\"js\"\n                  plugins={[\"line-numbers\"]}\n                />\n              </div>\n            </List>\n          </Paper>\n        </Grid>\n      </Grid>\n    )\n  }\n}\n\nexport default (withStyles(styles)(PcaPy));\n"]},"metadata":{},"sourceType":"module"}