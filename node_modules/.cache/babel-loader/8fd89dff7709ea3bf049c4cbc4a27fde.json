{"ast":null,"code":"import _classCallCheck from\"/home/mukeshs/Projects/edurights/node_modules/@babel/runtime/helpers/esm/classCallCheck\";import _createClass from\"/home/mukeshs/Projects/edurights/node_modules/@babel/runtime/helpers/esm/createClass\";import _possibleConstructorReturn from\"/home/mukeshs/Projects/edurights/node_modules/@babel/runtime/helpers/esm/possibleConstructorReturn\";import _getPrototypeOf from\"/home/mukeshs/Projects/edurights/node_modules/@babel/runtime/helpers/esm/getPrototypeOf\";import _inherits from\"/home/mukeshs/Projects/edurights/node_modules/@babel/runtime/helpers/esm/inherits\";import React,{Component}from'react';import Prism from\"prismjs\";import{Grid,Paper,withStyles,List}from\"@material-ui/core\";import'../../ReactJs/styles.css';import Sidebar from'../sidebar';import PrismCode from'../../ReactJs/prismCode';var titles={backgroundColor:'#F0F8FF',padding:'1px',fontSize:'16px'};var styles=function styles(theme){return{paper:{margin:theme.spacing(1),padding:theme.spacing(1)},smMargin:{margin:theme.spacing(1)},actionDiv:{textAlign:\"center\"}};};var kmeansClucs=\"\\nfrom sklearn.cluster import KMeans\\nimport pandas as pd\\nfrom sklearn.preprocessing import MinMaxScaler\\nfrom matplotlib import pyplot as plt\\n%matplotlib inline\\n\\ndf = pd.read_csv(\\\"income.csv\\\")\\n\\nplt.scatter(df.Age,df['Income($)'])\\nplt.xlabel('Age')\\nplt.ylabel('Income($)')\\n\\nkm = KMeans(n_clusters=3)\\ny_predicted = km.fit_predict(df[['Age','Income($)']])\\n\\ndf['cluster']=y_predicted\\n\\nkm.cluster_centers_\\n\\ndf1 = df[df.cluster==0]\\ndf2 = df[df.cluster==1]\\ndf3 = df[df.cluster==2]\\n\\nplt.scatter(df1.Age,df1['Income($)'],color='green')\\nplt.scatter(df2.Age,df2['Income($)'],color='red')\\nplt.scatter(df3.Age,df3['Income($)'],color='black')\\nplt.scatter(km.cluster_centers_[:,0],km.cluster_centers_[:,1],color='purple',marker='*',label='centroid')\\n\\nplt.xlabel('Age')\\nplt.ylabel('Income ($)')\\nplt.legend()\\n\".trim();var preprocessing=\"\\nscaler = MinMaxScaler()\\n\\nscaler.fit(df[['Income($)']])\\ndf['Income($)'] = scaler.transform(df[['Income($)']])\\n\\nscaler.fit(df[['Age']])\\ndf['Age'] = scaler.transform(df[['Age']])\\n\\nplt.scatter(df.Age,df['Income($)'])\\n\\nkm = KMeans(n_clusters=3)\\ny_predicted = km.fit_predict(df[['Age','Income($)']])\\n\\ndf['cluster'] = y_predicted\\nkm.cluster_centers_\\n\\ndf1 = df[df.cluster==0]\\ndf2 = df[df.cluster==1]\\n\\nplt.scatter(df1.Age,df1['Income($)'],color='green')\\nplt.scatter(df2.Age,df2['Income($)'],color='red')\\nplt.scatter(df3.Age,df3['Income($)'],color='black')\\nplt.scatter(km.cluster_centers_[:,0],km.cluster_centers_[:,1],color='purple',marker='*',label='centroid')\\nplt.legend()\\n\".trim();var elbo=\"\\nsse = []\\nk_rng = range(1,10)\\nfor k in k_rng:\\n    km = KMeans(n_clusters=k)\\n    km.fit(df[['Age','Income($)']])\\n    sse.append(km.inertia_)\\n    \\nplt.xlabel('K')\\nplt.ylabel('Sum of squared error')\\nplt.plot(k_rng,sse)\\n\".trim();var cluster=\"      %matplotlib inline\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns; sns.set()\\nimport numpy as np\\nfrom sklearn.cluster import KMeans\\nfrom sklearn.datasets.samples_generator import make_blobs\\nX, y_true = make_blobs(n_samples = 400, centers = 4, cluster_std = 0.60, random_state = 0)\\nplt.scatter(X[:, 0], X[:, 1], s = 20);\\nplt.show()\\n\".trim();var cluster_2=\"kmeans = KMeans(n_clusters = 4)\\nkmeans.fit(X)\\ny_kmeans = kmeans.predict(X)\\n\".trim();var cluster_3=\"plt.scatter(X[:, 0], X[:, 1], c = y_kmeans, s = 20, cmap = 'summer')\\ncenters = kmeans.cluster_centers_\\nplt.scatter(centers[:, 0], centers[:, 1], c = 'blue', s = 100, alpha = 0.9);\\nplt.show()\\n\".trim();var cluster_4=\"%matplotlib inline\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns; sns.set()\\nimport numpy as np\\nfrom sklearn.cluster import KMeans\\nfrom sklearn.datasets import load_digits\\ndigits = load_digits()\\ndigits.data.shape\\n\".trim();var K_Mean=/*#__PURE__*/function(_Component){_inherits(K_Mean,_Component);function K_Mean(){_classCallCheck(this,K_Mean);return _possibleConstructorReturn(this,_getPrototypeOf(K_Mean).apply(this,arguments));}_createClass(K_Mean,[{key:\"componentDidMount\",value:function componentDidMount(){setTimeout(function(){return Prism.highlightAll();},0);}},{key:\"render\",value:function render(){var classes=this.props.classes;return React.createElement(Grid,{container:true},React.createElement(Grid,{item:true,xs:2},React.createElement(Paper,{className:classes.paper},React.createElement(\"h4\",null,React.createElement(Sidebar,null)))),React.createElement(Grid,{item:true,xs:10},React.createElement(Paper,{className:classes.paper},React.createElement(List,null,React.createElement(\"h3\",null,\"Clustering With K Means\"),\"K Means is an Unsuperwised Learning.\",React.createElement(\"br\",null),React.createElement(\"br\",null),React.createElement(\"div\",{style:titles},React.createElement(PrismCode,{code:kmeansClucs,language:\"js\",plugins:[\"line-numbers\"]})),React.createElement(\"br\",null),React.createElement(\"h3\",null,\"Preprocessing using min max scaler\"),React.createElement(\"div\",{style:titles},React.createElement(PrismCode,{code:preprocessing,language:\"js\",plugins:[\"line-numbers\"]})),React.createElement(\"br\",null),React.createElement(\"h3\",null,\"Elbow Plot\"),React.createElement(\"div\",{style:titles},React.createElement(PrismCode,{code:elbo,language:\"js\",plugins:[\"line-numbers\"]})),React.createElement(\"br\",null),React.createElement(\"h3\",null,\"K-Means clustering:\"),\"K-means clustering algorithm computes the centroids and iterates until we it finds optimal centroid. It assumes that the number of clusters are already known. It is also called flat clustering algorithm. The number of clusters identified from data by algorithm is represented by \\u2018K\\u2019 in K-means.\",React.createElement(\"br\",null),React.createElement(\"br\",null),\"In this, the data points are assigned to a cluster in such a manner that the sum of the squared distance between the data points and centroid would be minimum. It is to be understood that less variation within the clusters will lead to more similar data points within same cluster.\",React.createElement(\"br\",null),React.createElement(\"br\",null),React.createElement(\"b\",null,\"Working of K-Means Algorithm: \"),React.createElement(\"ul\",null,React.createElement(\"li\",null,\"1. We need to specify the number of clusters, K, need to be generated by this algorithm.\"),React.createElement(\"li\",null,\"2. Randomly select K data points and assign each data point to a cluster. In simple words, classify the data based on the number of data points.\"),React.createElement(\"li\",null,\"3. Now it will compute the cluster centroids.\"),React.createElement(\"li\",null,\"4. Keep iterating the following until we find optimal centroid which is the assignment of data points to the clusters that are not changing any more\"),React.createElement(\"ul\",null,React.createElement(\"li\",null,\"1. The sum of squared distance between data points and centroids would be computed.\"),React.createElement(\"li\",null,\"2. Now, we have to assign each data point to the cluster that is closer than other cluster (centroid).\"),React.createElement(\"li\",null,\"3. At last compute the centroids for the clusters by taking the average of all data points of that cluster.\"))),React.createElement(\"br\",null),\"K-means follows Expectation-Maximization approach to solve the problem. The Expectation-step is used for assigning the data points to the closest cluster and the Maximization-step is used for computing the centroid of each cluster.\",React.createElement(\"br\",null),React.createElement(\"br\",null),\"While working with K-means algorithm we need to take care of the following things \\u2212\",React.createElement(\"ul\",null,React.createElement(\"li\",null,\"It is recommended to standardize the data because such algorithms use distance-based measurement to determine the similarity between data points.\"),React.createElement(\"li\",null,\"Due to the iterative nature of K-Means and random initialization of centroids, K-Means may stick in a local optimum and may not converge to global optimum. That is why it is recommended to use different initializations of centroids.\")),React.createElement(\"br\",null),React.createElement(\"b\",null,\"Ex.\"),\" We are going to first generate 2D dataset containing 4 different blobs and after that will apply k-means algorithm to see the result.\",React.createElement(\"div\",{style:titles},React.createElement(PrismCode,{code:cluster,language:\"js\",plugins:[\"line-numbers\"]})),React.createElement(\"br\",null),React.createElement(\"br\",null),React.createElement(\"i\",null,\"Next, make an object of KMeans along with providing number of clusters, train the model and do the prediction as follows \\u2212\"),React.createElement(\"div\",{style:titles},React.createElement(PrismCode,{code:cluster_2,language:\"js\",plugins:[\"line-numbers\"]})),React.createElement(\"br\",null),\"Now, with the help of following code we can plot and visualize the cluster\\u2019s centers picked by k-means Python estimator \\u2212\",React.createElement(\"br\",null),\"from sklearn.datasets.samples_generator import make_blobs\",React.createElement(\"br\",null),\"X, y_true = make_blobs(n_samples = 400, centers = 4, cluster_std = 0.60, random_state = 0)\",React.createElement(\"br\",null),\"Next, the following code will help us to visualize the dataset \\u2212\",React.createElement(\"div\",{style:titles},React.createElement(PrismCode,{code:cluster_3,language:\"js\",plugins:[\"line-numbers\"]})),React.createElement(\"h3\",null,\"Let us move to another example in which we are going to apply K-means clustering on simple digits dataset. K-means will try to identify similar digits without using the original label information.\"),React.createElement(\"div\",{style:titles},React.createElement(PrismCode,{code:cluster_4,language:\"js\",plugins:[\"line-numbers\"]})),React.createElement(\"h3\",null,\"Advantages and Disadvantages:\"),React.createElement(\"b\",null,\"Advantages:\"),React.createElement(\"ul\",null,React.createElement(\"li\",null,\"It is very easy to understand and implement.\"),React.createElement(\"li\",null,\"If we have large number of variables then, K-means would be faster than Hierarchical clustering.\"),React.createElement(\"li\",null,\"On re-computation of centroids, an instance can change the cluster.\"),React.createElement(\"li\",null,\"Tighter clusters are formed with K-means as compared to Hierarchical clustering\")),React.createElement(\"br\",null),React.createElement(\"b\",null,\"Disadvantages:\"),React.createElement(\"ul\",null,React.createElement(\"li\",null,\"It is a bit difficult to predict the number of clusters i.e. the value of k.\"),React.createElement(\"li\",null,\"Output is strongly impacted by initial inputs like number of clusters (value of k) Order of data will have strong impact on the final output.\"),React.createElement(\"li\",null,\"It is very sensitive to rescaling. If we will rescale our data by means of normalization or standardization, then the output will completely change.\"),React.createElement(\"li\",null,\"It is not good in doing clustering job if the clusters have a complicated geometric shape.\")),React.createElement(\"br\",null),React.createElement(\"b\",null,\"Applications:\"),\"To get a meaningful intuition from the data we are working with. Cluster-then-predict where different models will be built for different subgroups.\"))));}}]);return K_Mean;}(Component);export default withStyles(styles)(K_Mean);","map":{"version":3,"sources":["/home/mukeshs/Projects/edurights/src/components/ml/deepMl/k_meanClustring.js"],"names":["React","Component","Prism","Grid","Paper","withStyles","List","Sidebar","PrismCode","titles","backgroundColor","padding","fontSize","styles","theme","paper","margin","spacing","smMargin","actionDiv","textAlign","kmeansClucs","trim","preprocessing","elbo","cluster","cluster_2","cluster_3","cluster_4","K_Mean","setTimeout","highlightAll","classes","props"],"mappings":"6kBAAA,MAAOA,CAAAA,KAAP,EAAgBC,SAAhB,KAAiC,OAAjC,CACA,MAAOC,CAAAA,KAAP,KAAkB,SAAlB,CACA,OAASC,IAAT,CAAeC,KAAf,CAAsBC,UAAtB,CAAkCC,IAAlC,KAA8C,mBAA9C,CAEA,MAAO,0BAAP,CACA,MAAOC,CAAAA,OAAP,KAAoB,YAApB,CACA,MAAOC,CAAAA,SAAP,KAAsB,yBAAtB,CAGA,GAAMC,CAAAA,MAAM,CAAG,CAAEC,eAAe,CAAE,SAAnB,CAA8BC,OAAO,CAAE,KAAvC,CAA8CC,QAAQ,CAAE,MAAxD,CAAf,CAEA,GAAMC,CAAAA,MAAM,CAAG,QAATA,CAAAA,MAAS,CAAAC,KAAK,QAAK,CACvBC,KAAK,CAAE,CACLC,MAAM,CAAEF,KAAK,CAACG,OAAN,CAAc,CAAd,CADH,CAELN,OAAO,CAAEG,KAAK,CAACG,OAAN,CAAc,CAAd,CAFJ,CADgB,CAKvBC,QAAQ,CAAE,CACRF,MAAM,CAAEF,KAAK,CAACG,OAAN,CAAc,CAAd,CADA,CALa,CAQvBE,SAAS,CAAE,CACTC,SAAS,CAAE,QADF,CARY,CAAL,EAApB,CAaA,GAAMC,CAAAA,WAAW,CAAG,2zBAgClBC,IAhCkB,EAApB,CAkCA,GAAMC,CAAAA,aAAa,CAAG,urBAyBpBD,IAzBoB,EAAtB,CA2BA,GAAME,CAAAA,IAAI,CAAG,sOAWXF,IAXW,EAAb,CAaA,GAAMG,CAAAA,OAAO,CAAG,mWASdH,IATc,EAAhB,CAWA,GAAMI,CAAAA,SAAS,CAAG,iFAGhBJ,IAHgB,EAAlB,CAKA,GAAMK,CAAAA,SAAS,CAAG,sMAIhBL,IAJgB,EAAlB,CAMA,GAAMM,CAAAA,SAAS,CAAG,uOAQhBN,IARgB,EAAlB,C,GAWMO,CAAAA,M,sRACgB,CAClBC,UAAU,CAAC,iBAAM5B,CAAAA,KAAK,CAAC6B,YAAN,EAAN,EAAD,CAA6B,CAA7B,CAAV,CACD,C,uCACQ,IACCC,CAAAA,OADD,CACa,KAAKC,KADlB,CACCD,OADD,CAEP,MACE,qBAAC,IAAD,EAAM,SAAS,KAAf,EACE,oBAAC,IAAD,EAAM,IAAI,KAAV,CAAW,EAAE,CAAE,CAAf,EACE,oBAAC,KAAD,EAAO,SAAS,CAAEA,OAAO,CAACjB,KAA1B,EACE,8BAAI,oBAAC,OAAD,MAAJ,CADF,CADF,CADF,CAME,oBAAC,IAAD,EAAM,IAAI,KAAV,CAAW,EAAE,CAAE,EAAf,EACE,oBAAC,KAAD,EAAO,SAAS,CAAEiB,OAAO,CAACjB,KAA1B,EACE,oBAAC,IAAD,MACE,wDADF,wCAGE,8BAHF,CAIE,8BAJF,CAKE,2BAAK,KAAK,CAAEN,MAAZ,EACE,oBAAC,SAAD,EACE,IAAI,CAAEY,WADR,CAEE,QAAQ,CAAC,IAFX,CAGE,OAAO,CAAE,CAAC,cAAD,CAHX,EADF,CALF,CAYE,8BAZF,CAcE,mEAdF,CAeE,2BAAK,KAAK,CAAEZ,MAAZ,EACE,oBAAC,SAAD,EACE,IAAI,CAAEc,aADR,CAEE,QAAQ,CAAC,IAFX,CAGE,OAAO,CAAE,CAAC,cAAD,CAHX,EADF,CAfF,CAsBE,8BAtBF,CAwBE,2CAxBF,CAyBE,2BAAK,KAAK,CAAEd,MAAZ,EACE,oBAAC,SAAD,EACE,IAAI,CAAEe,IADR,CAEE,QAAQ,CAAC,IAFX,CAGE,OAAO,CAAE,CAAC,cAAD,CAHX,EADF,CAzBF,CAgCE,8BAhCF,CAkCE,oDAlCF,oTAsCE,8BAtCF,CAuCE,8BAvCF,6RA2CE,8BA3CF,CA4CE,8BA5CF,CA6CE,8DA7CF,CA8CE,8BACE,yHADF,CAEE,iLAFF,CAIE,8EAJF,CAKE,qLALF,CAOE,8BACE,oHADF,CAEE,uIAFF,CAGE,4IAHF,CAPF,CA9CF,CA2DE,8BA3DF,2OA+DE,8BA/DF,CAgEE,8BAhEF,4FAkEE,8BACE,kLADF,CAGE,yQAHF,CAlEF,CAwEE,8BAxEF,CAyEE,mCAzEF,0IA4EE,2BAAK,KAAK,CAAEf,MAAZ,EACE,oBAAC,SAAD,EACE,IAAI,CAAEgB,OADR,CAEE,QAAQ,CAAC,IAFX,CAGE,OAAO,CAAE,CAAC,cAAD,CAHX,EADF,CA5EF,CAmFE,8BAnFF,CAoFE,8BApFF,CAqFE,+JArFF,CAwFE,2BAAK,KAAK,CAAEhB,MAAZ,EACE,oBAAC,SAAD,EACE,IAAI,CAAEiB,SADR,CAEE,QAAQ,CAAC,IAFX,CAGE,OAAO,CAAE,CAAC,cAAD,CAHX,EADF,CAxFF,CA+FE,8BA/FF,uIAkGE,8BAlGF,6DAmG2D,8BAnG3D,8FAqGE,8BArGF,yEAuGE,2BAAK,KAAK,CAAEjB,MAAZ,EACE,oBAAC,SAAD,EACE,IAAI,CAAEkB,SADR,CAEE,QAAQ,CAAC,IAFX,CAGE,OAAO,CAAE,CAAC,cAAD,CAHX,EADF,CAvGF,CA8GE,qOA9GF,CAiHE,2BAAK,KAAK,CAAElB,MAAZ,EACE,oBAAC,SAAD,EACE,IAAI,CAAEmB,SADR,CAEE,QAAQ,CAAC,IAFX,CAGE,OAAO,CAAE,CAAC,cAAD,CAHX,EADF,CAjHF,CAwHE,8DAxHF,CAyHE,2CAzHF,CA0HE,8BACE,6EADF,CAEE,iIAFF,CAGE,oGAHF,CAIE,gHAJF,CA1HF,CAgIE,8BAhIF,CAkIE,8CAlIF,CAmIE,8BACE,6GADF,CAEE,8KAFF,CAIE,qLAJF,CAME,2HANF,CAnIF,CA2IE,8BA3IF,CA6IE,6CA7IF,uJADF,CADF,CANF,CADF,CA8JD,C,oBApKkB3B,S,EAuKrB,cAAgBI,CAAAA,UAAU,CAACQ,MAAD,CAAV,CAAmBgB,MAAnB,CAAhB","sourcesContent":["import React, { Component } from 'react';\nimport Prism from \"prismjs\"\nimport { Grid, Paper, withStyles, List } from \"@material-ui/core\";\n\nimport '../../ReactJs/styles.css'\nimport Sidebar from '../sidebar';\nimport PrismCode from '../../ReactJs/prismCode';\n\n\nconst titles = { backgroundColor: '#F0F8FF', padding: '1px', fontSize: '16px' }\n\nconst styles = theme => ({\n  paper: {\n    margin: theme.spacing(1),\n    padding: theme.spacing(1)\n  },\n  smMargin: {\n    margin: theme.spacing(1)\n  },\n  actionDiv: {\n    textAlign: \"center\"\n  }\n})\n\nconst kmeansClucs = `\nfrom sklearn.cluster import KMeans\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nfrom matplotlib import pyplot as plt\n%matplotlib inline\n\ndf = pd.read_csv(\"income.csv\")\n\nplt.scatter(df.Age,df['Income($)'])\nplt.xlabel('Age')\nplt.ylabel('Income($)')\n\nkm = KMeans(n_clusters=3)\ny_predicted = km.fit_predict(df[['Age','Income($)']])\n\ndf['cluster']=y_predicted\n\nkm.cluster_centers_\n\ndf1 = df[df.cluster==0]\ndf2 = df[df.cluster==1]\ndf3 = df[df.cluster==2]\n\nplt.scatter(df1.Age,df1['Income($)'],color='green')\nplt.scatter(df2.Age,df2['Income($)'],color='red')\nplt.scatter(df3.Age,df3['Income($)'],color='black')\nplt.scatter(km.cluster_centers_[:,0],km.cluster_centers_[:,1],color='purple',marker='*',label='centroid')\n\nplt.xlabel('Age')\nplt.ylabel('Income ($)')\nplt.legend()\n`.trim();\n\nconst preprocessing = `\nscaler = MinMaxScaler()\n\nscaler.fit(df[['Income($)']])\ndf['Income($)'] = scaler.transform(df[['Income($)']])\n\nscaler.fit(df[['Age']])\ndf['Age'] = scaler.transform(df[['Age']])\n\nplt.scatter(df.Age,df['Income($)'])\n\nkm = KMeans(n_clusters=3)\ny_predicted = km.fit_predict(df[['Age','Income($)']])\n\ndf['cluster'] = y_predicted\nkm.cluster_centers_\n\ndf1 = df[df.cluster==0]\ndf2 = df[df.cluster==1]\n\nplt.scatter(df1.Age,df1['Income($)'],color='green')\nplt.scatter(df2.Age,df2['Income($)'],color='red')\nplt.scatter(df3.Age,df3['Income($)'],color='black')\nplt.scatter(km.cluster_centers_[:,0],km.cluster_centers_[:,1],color='purple',marker='*',label='centroid')\nplt.legend()\n`.trim();\n\nconst elbo = `\nsse = []\nk_rng = range(1,10)\nfor k in k_rng:\n    km = KMeans(n_clusters=k)\n    km.fit(df[['Age','Income($)']])\n    sse.append(km.inertia_)\n    \nplt.xlabel('K')\nplt.ylabel('Sum of squared error')\nplt.plot(k_rng,sse)\n`.trim();\n\nconst cluster = `      %matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns; sns.set()\nimport numpy as np\nfrom sklearn.cluster import KMeans\nfrom sklearn.datasets.samples_generator import make_blobs\nX, y_true = make_blobs(n_samples = 400, centers = 4, cluster_std = 0.60, random_state = 0)\nplt.scatter(X[:, 0], X[:, 1], s = 20);\nplt.show()\n`.trim()\n\nconst cluster_2 = `kmeans = KMeans(n_clusters = 4)\nkmeans.fit(X)\ny_kmeans = kmeans.predict(X)\n`.trim()\n\nconst cluster_3 = `plt.scatter(X[:, 0], X[:, 1], c = y_kmeans, s = 20, cmap = 'summer')\ncenters = kmeans.cluster_centers_\nplt.scatter(centers[:, 0], centers[:, 1], c = 'blue', s = 100, alpha = 0.9);\nplt.show()\n`.trim()\n\nconst cluster_4 = `%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns; sns.set()\nimport numpy as np\nfrom sklearn.cluster import KMeans\nfrom sklearn.datasets import load_digits\ndigits = load_digits()\ndigits.data.shape\n`.trim()\n\n\nclass K_Mean extends Component {\n  componentDidMount() {\n    setTimeout(() => Prism.highlightAll(), 0)\n  }\n  render() {\n    const { classes } = this.props;\n    return (\n      <Grid container>\n        <Grid item xs={2}>\n          <Paper className={classes.paper}>\n            <h4><Sidebar /></h4>\n          </Paper>\n        </Grid>\n        <Grid item xs={10}>\n          <Paper className={classes.paper}>\n            <List>\n              <h3>Clustering With K Means</h3>\n              K Means is an Unsuperwised Learning.\n              <br />\n              <br />\n              <div style={titles}>\n                <PrismCode\n                  code={kmeansClucs}\n                  language=\"js\"\n                  plugins={[\"line-numbers\"]}\n                />\n              </div>\n              <br />\n\n              <h3>Preprocessing using min max scaler</h3>\n              <div style={titles}>\n                <PrismCode\n                  code={preprocessing}\n                  language=\"js\"\n                  plugins={[\"line-numbers\"]}\n                />\n              </div>\n              <br />\n\n              <h3>Elbow Plot</h3>\n              <div style={titles}>\n                <PrismCode\n                  code={elbo}\n                  language=\"js\"\n                  plugins={[\"line-numbers\"]}\n                />\n              </div>\n              <br />\n\n              <h3>K-Means clustering:</h3>\n              K-means clustering algorithm computes the centroids and iterates until we it finds optimal centroid. It assumes that\n              the number of clusters are already known. It is also called flat clustering algorithm. The number of clusters\n              identified from data by algorithm is represented by ‘K’ in K-means.\n              <br />\n              <br />\n              In this, the data points are assigned to a cluster in such a manner that the sum of the squared distance\n              between the data points and centroid would be minimum. It is to be understood that less variation within the clusters\n              will lead to more similar data points within same cluster.\n              <br />\n              <br />\n              <b>Working of K-Means Algorithm: </b>\n              <ul>\n                <li>1. We need to specify the number of clusters, K, need to be generated by this algorithm.</li>\n                <li>2. Randomly select K data points and assign each data point to a cluster. In simple words, classify the\n                  data based on the number of data points.</li>\n                <li>3. Now it will compute the cluster centroids.</li>\n                <li>4. Keep iterating the following until we find optimal centroid which is the assignment of data points to\n                  the clusters that are not changing any more</li>\n                <ul>\n                  <li>1. The sum of squared distance between data points and centroids would be computed.</li>\n                  <li>2. Now, we have to assign each data point to the cluster that is closer than other cluster (centroid).</li>\n                  <li>3. At last compute the centroids for the clusters by taking the average of all data points of that cluster.</li>\n                </ul>\n              </ul>\n              <br />\n\n              K-means follows Expectation-Maximization approach to solve the problem. The Expectation-step is used for assigning\n              the data points to the closest cluster and the Maximization-step is used for computing the centroid of each cluster.\n              <br />\n              <br />\n              While working with K-means algorithm we need to take care of the following things −\n              <ul>\n                <li>It is recommended to standardize the data because such\n                  algorithms use distance-based measurement to determine the similarity between data points.</li>\n                <li>Due to the iterative nature of K-Means and random initialization of centroids, K-Means may stick in a local optimum\n                  and may not converge to global optimum. That is why it is recommended to use different initializations of centroids.</li>\n              </ul>\n              <br />\n              <b>Ex.</b> We are going to first generate 2D dataset containing 4 different blobs and after that will apply k-means\n              algorithm to see the result.\n\n              <div style={titles}>\n                <PrismCode\n                  code={cluster}\n                  language=\"js\"\n                  plugins={[\"line-numbers\"]}\n                />\n              </div>\n              <br />\n              <br />\n              <i>Next, make an object of KMeans along with providing number of clusters, train the model and do the prediction as\n                follows −\n              </i>\n              <div style={titles}>\n                <PrismCode\n                  code={cluster_2}\n                  language=\"js\"\n                  plugins={[\"line-numbers\"]}\n                />\n              </div>\n              <br />\n              Now, with the help of following code we can plot and visualize the cluster’s centers picked by k-means Python\n              estimator −\n              <br />\n              from sklearn.datasets.samples_generator import make_blobs<br />\n              X, y_true = make_blobs(n_samples = 400, centers = 4, cluster_std = 0.60, random_state = 0)\n              <br />\n              Next, the following code will help us to visualize the dataset −\n              <div style={titles}>\n                <PrismCode\n                  code={cluster_3}\n                  language=\"js\"\n                  plugins={[\"line-numbers\"]}\n                />\n              </div>\n              <h3>Let us move to another example in which we are going to apply K-means clustering on simple digits dataset.\n                K-means will try to identify similar digits without using the original label information.\n              </h3>\n              <div style={titles}>\n                <PrismCode\n                  code={cluster_4}\n                  language=\"js\"\n                  plugins={[\"line-numbers\"]}\n                />\n              </div>\n              <h3>Advantages and Disadvantages:</h3>\n              <b>Advantages:</b>\n              <ul>\n                <li>It is very easy to understand and implement.</li>\n                <li>If we have large number of variables then, K-means would be faster than Hierarchical clustering.</li>\n                <li>On re-computation of centroids, an instance can change the cluster.</li>\n                <li>Tighter clusters are formed with K-means as compared to Hierarchical clustering</li>\n              </ul>\n              <br />\n\n              <b>Disadvantages:</b>\n              <ul>\n                <li>It is a bit difficult to predict the number of clusters i.e. the value of k.</li>\n                <li>Output is strongly impacted by initial inputs like number of clusters (value of k)\n                  Order of data will have strong impact on the final output.</li>\n                <li>It is very sensitive to rescaling. If we will rescale our data by means of normalization or standardization, then the\n                  output will completely change.</li>\n                <li>It is not good in doing clustering job if the clusters have a complicated geometric shape.</li>\n              </ul>\n              <br />\n\n              <b>Applications:</b>\n              To get a meaningful intuition from the data we are working with.\n              Cluster-then-predict where different models will be built for different subgroups.\n            </List>\n          </Paper>\n        </Grid>\n      </Grid>\n    )\n  }\n}\n\nexport default (withStyles(styles)(K_Mean));\n"]},"metadata":{},"sourceType":"module"}