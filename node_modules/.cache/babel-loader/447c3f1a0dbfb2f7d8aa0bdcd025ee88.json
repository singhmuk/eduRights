{"ast":null,"code":"import _classCallCheck from\"/home/mukeshs/Projects/edurights/node_modules/@babel/runtime/helpers/esm/classCallCheck\";import _createClass from\"/home/mukeshs/Projects/edurights/node_modules/@babel/runtime/helpers/esm/createClass\";import _possibleConstructorReturn from\"/home/mukeshs/Projects/edurights/node_modules/@babel/runtime/helpers/esm/possibleConstructorReturn\";import _getPrototypeOf from\"/home/mukeshs/Projects/edurights/node_modules/@babel/runtime/helpers/esm/getPrototypeOf\";import _inherits from\"/home/mukeshs/Projects/edurights/node_modules/@babel/runtime/helpers/esm/inherits\";import React,{Component}from'react';import Prism from\"prismjs\";import{Grid,Paper,withStyles,List}from\"@material-ui/core\";import'../../ReactJs/styles.css';import Sidebar from'../sidebar';import PrismCode from'../../ReactJs/prismCode';var titles={backgroundColor:'#F0F8FF',padding:'1px',fontSize:'16px'};var styles=function styles(theme){return{paper:{margin:theme.spacing(1),padding:theme.spacing(1)},smMargin:{margin:theme.spacing(1)},actionDiv:{textAlign:\"center\"}};};var cluster=\"\\nimport numpy as np\\n\\n\\nclass LDA:\\n    def __init__(self, n_components):\\n        self.n_components = n_components\\n        self.linear_discriminants = None\\n\\n    def fit(self, X, y):\\n        n_features = X.shape[1]\\n        class_labels = np.unique(y)\\n\\n        # SW = sum((X_c - mean_X_c)^2 )                                                     # Within class scatter matrix:\\n        # SB = sum( n_c * (mean_X_c - mean_overall)^2 )                                     # Between class scatter:\\n\\n        mean_overall = np.mean(X, axis=0)\\n        SW = np.zeros((n_features, n_features))\\n        SB = np.zeros((n_features, n_features))\\n        for c in class_labels:\\n            X_c = X[y == c]\\n            mean_c = np.mean(X_c, axis=0)\\n            \\n            SW += (X_c - mean_c).T.dot((X_c - mean_c))                            # (4, n_c) * (n_c, 4) = (4,4) -> transpose\\n            n_c = X_c.shape[0]                                                    # (4, 1) * (1, 4) = (4,4) -> reshape\\n            mean_diff = (mean_c - mean_overall).reshape(n_features, 1)\\n            SB += n_c * (mean_diff).dot(mean_diff.T)\\n\\n        \\n        A = np.linalg.inv(SW).dot(SB)                                      # Determine SW^-1 * SB\\n                                                                           # Get eigenvalues and eigenvectors of SW^-1 * SB\\n        eigenvalues, eigenvectors = np.linalg.eig(A)\\n                                                            # eigenvector v = [:,i] column vector, transpose for easier cal.\\n        \\n        eigenvectors = eigenvectors.T                                      # sort eigenvalues high to low\\n        idxs = np.argsort(abs(eigenvalues))[::-1]\\n        eigenvalues = eigenvalues[idxs]\\n        eigenvectors = eigenvectors[idxs]\\n        \\n        self.linear_discriminants = eigenvectors[0 : self.n_components]   # store first n eigenvectors\\n\\n    def transform(self, X):\\n        return np.dot(X, self.linear_discriminants.T)                     # project data\\n\".trim();var testings=\"\\nif __name__ == \\\"__main__\\\":\\n    # Imports\\n    import matplotlib.pyplot as plt\\n    from sklearn import datasets\\n\\n    data = datasets.load_iris()\\n    X, y = data.data, data.target\\n\\n    # Project the data onto the 2 primary linear discriminants\\n    lda = LDA(2)\\n    lda.fit(X, y)\\n    X_projected = lda.transform(X)\\n\\n    print(\\\"Shape of X:\\\", X.shape)\\n    print(\\\"Shape of transformed X:\\\", X_projected.shape)\\n\\n    x1, x2 = X_projected[:, 0], X_projected[:, 1]\\n\\n    plt.scatter(\\n        x1, x2, c=y, edgecolor=\\\"none\\\", alpha=0.8, cmap=plt.cm.get_cmap(\\\"viridis\\\", 3)\\n    )\\n\\n    plt.xlabel(\\\"Linear Discriminant 1\\\")\\n    plt.ylabel(\\\"Linear Discriminant 2\\\")\\n    plt.colorbar()\\n    plt.show()\\n\".trim();// const stack = ``.trim();\n// const stack = ``.trim();\nvar LdaPy=/*#__PURE__*/function(_Component){_inherits(LdaPy,_Component);function LdaPy(){_classCallCheck(this,LdaPy);return _possibleConstructorReturn(this,_getPrototypeOf(LdaPy).apply(this,arguments));}_createClass(LdaPy,[{key:\"componentDidMount\",value:function componentDidMount(){setTimeout(function(){return Prism.highlightAll();},0);}},{key:\"render\",value:function render(){var classes=this.props.classes;return React.createElement(Grid,{container:true},React.createElement(Grid,{item:true,xs:2},React.createElement(Paper,{className:classes.paper},React.createElement(\"h4\",null,React.createElement(Sidebar,null)))),React.createElement(Grid,{item:true,xs:10},React.createElement(Paper,{className:classes.paper},React.createElement(List,null,React.createElement(\"h3\",null,\"Linear Discriminant Analysis\"),\"It is a linear model for classification and dimensionality reduction. Most commonly used for feature extraction in pattern classification problems.\",React.createElement(\"br\",null),React.createElement(\"br\",null),React.createElement(\"b\",null,\"Why LDA:\"),React.createElement(\"ul\",null,React.createElement(\"li\",null,\"Logistic Regression perform well for binary classification but falls short in the case of multiple classification problems with well-separated classes. While LDA handles these.\"),React.createElement(\"li\",null,\"LDA also used in data preprocessing to reduce the number of features just as PCA which reduces the computing cost significantly.\"),React.createElement(\"li\",null,\"LDA is also used in face detection algorithms. In Fisherfaces LDA is used to extract useful data from different faces. Coupled with eigenfaces it produces effective results.\")),React.createElement(\"br\",null),React.createElement(\"b\",null,\"Shortcomings:\"),React.createElement(\"ul\",null,React.createElement(\"li\",null,\"Linear decision boundaries may not effectively separate non-linearly separable classes. More flexible boundaries are desired.\"),React.createElement(\"li\",null,\"In cases where the number of observations exceeds the number of features, LDA might not perform as desired. This is called Small Sample Size (SSS) problem. Regularization is required.\")),React.createElement(\"br\",null),React.createElement(\"b\",null,\"Assumptions:\"),\"LDA makes some assumptions about the data:\",React.createElement(\"br\",null),React.createElement(\"ul\",null,React.createElement(\"li\",null,\"Assumes the data to be distributed normally/ Gaussian distribution of data points i.e. each feature must make a bell-shaped curve when plotted. \"),React.createElement(\"li\",null,\"Each of the classes has identical covariance matrices.\")),React.createElement(\"br\",null),React.createElement(\"div\",{style:titles},React.createElement(PrismCode,{code:cluster,language:\"js\",plugins:[\"line-numbers\"]})),React.createElement(\"br\",null),React.createElement(\"b\",null,\"Testing\"),React.createElement(\"div\",{style:titles},React.createElement(PrismCode,{code:testings,language:\"js\",plugins:[\"line-numbers\"]}))))));}}]);return LdaPy;}(Component);export default withStyles(styles)(LdaPy);","map":{"version":3,"sources":["/home/mukeshs/Projects/edurights/src/components/ml/deepMl/lda.js"],"names":["React","Component","Prism","Grid","Paper","withStyles","List","Sidebar","PrismCode","titles","backgroundColor","padding","fontSize","styles","theme","paper","margin","spacing","smMargin","actionDiv","textAlign","cluster","trim","testings","LdaPy","setTimeout","highlightAll","classes","props"],"mappings":"6kBAAA,MAAOA,CAAAA,KAAP,EAAgBC,SAAhB,KAAiC,OAAjC,CACA,MAAOC,CAAAA,KAAP,KAAkB,SAAlB,CACA,OAASC,IAAT,CAAeC,KAAf,CAAsBC,UAAtB,CAAkCC,IAAlC,KAA8C,mBAA9C,CAEA,MAAO,0BAAP,CACA,MAAOC,CAAAA,OAAP,KAAoB,YAApB,CACA,MAAOC,CAAAA,SAAP,KAAsB,yBAAtB,CAGA,GAAMC,CAAAA,MAAM,CAAG,CAAEC,eAAe,CAAE,SAAnB,CAA8BC,OAAO,CAAE,KAAvC,CAA8CC,QAAQ,CAAE,MAAxD,CAAf,CAEA,GAAMC,CAAAA,MAAM,CAAG,QAATA,CAAAA,MAAS,CAAAC,KAAK,QAAK,CACvBC,KAAK,CAAE,CACLC,MAAM,CAAEF,KAAK,CAACG,OAAN,CAAc,CAAd,CADH,CAELN,OAAO,CAAEG,KAAK,CAACG,OAAN,CAAc,CAAd,CAFJ,CADgB,CAKvBC,QAAQ,CAAE,CACRF,MAAM,CAAEF,KAAK,CAACG,OAAN,CAAc,CAAd,CADA,CALa,CAQvBE,SAAS,CAAE,CACTC,SAAS,CAAE,QADF,CARY,CAAL,EAApB,CAcA,GAAMC,CAAAA,OAAO,CAAG,0/DA2CdC,IA3Cc,EAAhB,CA6CA,GAAMC,CAAAA,QAAQ,CAAG,ktBA2BfD,IA3Be,EAAjB,CA6BA;AAEA;GAGME,CAAAA,K,iRACgB,CAClBC,UAAU,CAAC,iBAAMvB,CAAAA,KAAK,CAACwB,YAAN,EAAN,EAAD,CAA6B,CAA7B,CAAV,CACD,C,uCACQ,IACCC,CAAAA,OADD,CACa,KAAKC,KADlB,CACCD,OADD,CAEP,MACE,qBAAC,IAAD,EAAM,SAAS,KAAf,EACE,oBAAC,IAAD,EAAM,IAAI,KAAV,CAAW,EAAE,CAAE,CAAf,EACE,oBAAC,KAAD,EAAO,SAAS,CAAEA,OAAO,CAACZ,KAA1B,EACE,8BAAI,oBAAC,OAAD,MAAJ,CADF,CADF,CADF,CAME,oBAAC,IAAD,EAAM,IAAI,KAAV,CAAW,EAAE,CAAE,EAAf,EACE,oBAAC,KAAD,EAAO,SAAS,CAAEY,OAAO,CAACZ,KAA1B,EACE,oBAAC,IAAD,MACE,6DADF,uJAGE,8BAHF,CAIE,8BAJF,CAKE,wCALF,CAME,8BACE,iNADF,CAGE,iKAHF,CAKE,8MALF,CANF,CAcE,8BAdF,CAgBE,6CAhBF,CAiBE,8BACE,8JADF,CAGE,wNAHF,CAjBF,CAuBE,8BAvBF,CAyBE,4CAzBF,8CA2BE,8BA3BF,CA4BE,8BACE,iLADF,CAGE,uFAHF,CA5BF,CAiCE,8BAjCF,CAmCE,2BAAK,KAAK,CAAEN,MAAZ,EACE,oBAAC,SAAD,EACE,IAAI,CAAEY,OADR,CAEE,QAAQ,CAAC,IAFX,CAGE,OAAO,CAAE,CAAC,cAAD,CAHX,EADF,CAnCF,CA0CE,8BA1CF,CA4CE,uCA5CF,CA6CE,2BAAK,KAAK,CAAEZ,MAAZ,EACE,oBAAC,SAAD,EACE,IAAI,CAAEc,QADR,CAEE,QAAQ,CAAC,IAFX,CAGE,OAAO,CAAE,CAAC,cAAD,CAHX,EADF,CA7CF,CADF,CADF,CANF,CADF,CAsFD,C,mBA5FiBtB,S,EA+FpB,cAAgBI,CAAAA,UAAU,CAACQ,MAAD,CAAV,CAAmBW,KAAnB,CAAhB","sourcesContent":["import React, { Component } from 'react';\nimport Prism from \"prismjs\"\nimport { Grid, Paper, withStyles, List } from \"@material-ui/core\";\n\nimport '../../ReactJs/styles.css'\nimport Sidebar from '../sidebar';\nimport PrismCode from '../../ReactJs/prismCode';\n\n\nconst titles = { backgroundColor: '#F0F8FF', padding: '1px', fontSize: '16px' }\n\nconst styles = theme => ({\n  paper: {\n    margin: theme.spacing(1),\n    padding: theme.spacing(1)\n  },\n  smMargin: {\n    margin: theme.spacing(1)\n  },\n  actionDiv: {\n    textAlign: \"center\"\n  }\n})\n\n\nconst cluster = `\nimport numpy as np\n\n\nclass LDA:\n    def __init__(self, n_components):\n        self.n_components = n_components\n        self.linear_discriminants = None\n\n    def fit(self, X, y):\n        n_features = X.shape[1]\n        class_labels = np.unique(y)\n\n        # SW = sum((X_c - mean_X_c)^2 )                                                     # Within class scatter matrix:\n        # SB = sum( n_c * (mean_X_c - mean_overall)^2 )                                     # Between class scatter:\n\n        mean_overall = np.mean(X, axis=0)\n        SW = np.zeros((n_features, n_features))\n        SB = np.zeros((n_features, n_features))\n        for c in class_labels:\n            X_c = X[y == c]\n            mean_c = np.mean(X_c, axis=0)\n            \n            SW += (X_c - mean_c).T.dot((X_c - mean_c))                            # (4, n_c) * (n_c, 4) = (4,4) -> transpose\n            n_c = X_c.shape[0]                                                    # (4, 1) * (1, 4) = (4,4) -> reshape\n            mean_diff = (mean_c - mean_overall).reshape(n_features, 1)\n            SB += n_c * (mean_diff).dot(mean_diff.T)\n\n        \n        A = np.linalg.inv(SW).dot(SB)                                      # Determine SW^-1 * SB\n                                                                           # Get eigenvalues and eigenvectors of SW^-1 * SB\n        eigenvalues, eigenvectors = np.linalg.eig(A)\n                                                            # eigenvector v = [:,i] column vector, transpose for easier cal.\n        \n        eigenvectors = eigenvectors.T                                      # sort eigenvalues high to low\n        idxs = np.argsort(abs(eigenvalues))[::-1]\n        eigenvalues = eigenvalues[idxs]\n        eigenvectors = eigenvectors[idxs]\n        \n        self.linear_discriminants = eigenvectors[0 : self.n_components]   # store first n eigenvectors\n\n    def transform(self, X):\n        return np.dot(X, self.linear_discriminants.T)                     # project data\n`.trim();\n\nconst testings = `\nif __name__ == \"__main__\":\n    # Imports\n    import matplotlib.pyplot as plt\n    from sklearn import datasets\n\n    data = datasets.load_iris()\n    X, y = data.data, data.target\n\n    # Project the data onto the 2 primary linear discriminants\n    lda = LDA(2)\n    lda.fit(X, y)\n    X_projected = lda.transform(X)\n\n    print(\"Shape of X:\", X.shape)\n    print(\"Shape of transformed X:\", X_projected.shape)\n\n    x1, x2 = X_projected[:, 0], X_projected[:, 1]\n\n    plt.scatter(\n        x1, x2, c=y, edgecolor=\"none\", alpha=0.8, cmap=plt.cm.get_cmap(\"viridis\", 3)\n    )\n\n    plt.xlabel(\"Linear Discriminant 1\")\n    plt.ylabel(\"Linear Discriminant 2\")\n    plt.colorbar()\n    plt.show()\n`.trim();\n\n// const stack = ``.trim();\n\n// const stack = ``.trim();\n\n\nclass LdaPy extends Component {\n  componentDidMount() {\n    setTimeout(() => Prism.highlightAll(), 0)\n  }\n  render() {\n    const { classes } = this.props;\n    return (\n      <Grid container>\n        <Grid item xs={2}>\n          <Paper className={classes.paper}>\n            <h4><Sidebar /></h4>\n          </Paper>\n        </Grid>\n        <Grid item xs={10}>\n          <Paper className={classes.paper}>\n            <List>\n              <h3>Linear Discriminant Analysis</h3>\n              It is a linear model for classification and dimensionality reduction. Most commonly used for feature extraction in pattern classification problems.\n              <br />\n              <br />\n              <b>Why LDA:</b>\n              <ul>\n                <li>Logistic Regression perform well for binary classification but falls short in the case of multiple classification problems\n                  with well-separated classes. While LDA handles these.</li>\n                <li>LDA also used in data preprocessing to reduce the number of features just as PCA which\n                  reduces the computing cost significantly.</li>\n                <li>LDA is also used in face detection algorithms. In Fisherfaces LDA is used to extract useful data\n                  from different faces. Coupled with eigenfaces it produces effective results.</li>\n              </ul>\n              <br />\n\n              <b>Shortcomings:</b>\n              <ul>\n                <li>Linear decision boundaries may not effectively separate non-linearly separable classes. More\n                  flexible boundaries are desired.</li>\n                <li>In cases where the number of observations exceeds the number of features, LDA might not perform\n                  as desired. This is called Small Sample Size (SSS) problem. Regularization is required.</li>\n              </ul>\n              <br />\n\n              <b>Assumptions:</b>\n              LDA makes some assumptions about the data:\n              <br />\n              <ul>\n                <li>Assumes the data to be distributed normally/ Gaussian distribution of data points i.e. each\n                  feature must make a bell-shaped curve when plotted. </li>\n                <li>Each of the classes has identical covariance matrices.</li>\n              </ul>\n              <br />\n\n              <div style={titles}>\n                <PrismCode\n                  code={cluster}\n                  language=\"js\"\n                  plugins={[\"line-numbers\"]}\n                />\n              </div>\n              <br />\n\n              <b>Testing</b>\n              <div style={titles}>\n                <PrismCode\n                  code={testings}\n                  language=\"js\"\n                  plugins={[\"line-numbers\"]}\n                />\n              </div>\n              {/* <br />\n\n              <h3></h3>\n              <div style={titles}>\n                <PrismCode\n                  code={stack}\n                  language=\"js\"\n                  plugins={[\"line-numbers\"]}\n                />\n              </div>\n              <br />\n\n              <h3></h3>\n              <div style={titles}>\n                <PrismCode\n                  code={stack}\n                  language=\"js\"\n                  plugins={[\"line-numbers\"]}\n                />\n              </div> */}\n            </List>\n          </Paper>\n        </Grid>\n      </Grid>\n    )\n  }\n}\n\nexport default (withStyles(styles)(LdaPy));\n"]},"metadata":{},"sourceType":"module"}