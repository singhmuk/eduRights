{"ast":null,"code":"import _classCallCheck from\"/home/mukeshs/Projects/edurights/node_modules/@babel/runtime/helpers/esm/classCallCheck\";import _createClass from\"/home/mukeshs/Projects/edurights/node_modules/@babel/runtime/helpers/esm/createClass\";import _possibleConstructorReturn from\"/home/mukeshs/Projects/edurights/node_modules/@babel/runtime/helpers/esm/possibleConstructorReturn\";import _getPrototypeOf from\"/home/mukeshs/Projects/edurights/node_modules/@babel/runtime/helpers/esm/getPrototypeOf\";import _inherits from\"/home/mukeshs/Projects/edurights/node_modules/@babel/runtime/helpers/esm/inherits\";import React,{Component}from'react';import Prism from\"prismjs\";import{Grid,Paper,withStyles,List}from\"@material-ui/core\";import'../../ReactJs/styles.css';import Sidebar from'../sidebar';import PrismCode from'../../ReactJs/prismCode';import Equations from'../../../assets/ML/perceptrons.png';import Perceptrons from'../../../assets/ML/perceptrons2.png';import PerceptronsGrapg from'../../../assets/ML/perceptrons3.png';import PerceptronsGraps from'../../../assets/ML/perceptrons4.png';var titles={backgroundColor:'#F0F8FF',padding:'1px',fontSize:'16px'};var redesign={height:350,width:600};var styles=function styles(theme){return{paper:{margin:theme.spacing(1),padding:theme.spacing(1)},smMargin:{margin:theme.spacing(1)},actionDiv:{textAlign:\"center\"}};};var stack=\"\\nimport numpy as np\\n\\nclass Perceptron:\\n    def __init__(self, learning_rate=0.01, n_iters=1000):\\n        self.lr = learning_rate\\n        self.n_iters = n_iters\\n        self.activation_func = self._unit_step_func\\n        self.weights = None\\n        self.bias = None\\n\\n    def fit(self, X, y):\\n        n_samples, n_features = X.shape\\n\\n        # init parameters\\n        self.weights = np.zeros(n_features)\\n        self.bias = 0\\n\\n        y_ = np.array([1 if i > 0 else 0 for i in y])\\n\\n        for _ in range(self.n_iters):\\n\\n            for idx, x_i in enumerate(X):\\n\\n                linear_output = np.dot(x_i, self.weights) + self.bias\\n                y_predicted = self.activation_func(linear_output)\\n\\n                # Perceptron update rule\\n                update = self.lr * (y_[idx] - y_predicted)\\n\\n                self.weights += update * x_i\\n                self.bias += update\\n\\n    def predict(self, X):\\n        linear_output = np.dot(X, self.weights) + self.bias\\n        y_predicted = self.activation_func(linear_output)\\n        return y_predicted\\n\\n    def _unit_step_func(self, x):\\n        return np.where(x >= 0, 1, 0)\\n\".trim();var label=\"\\nif 0.5x + 0.5y => 0, then 1\\nif 0.5x + 0.5y < 0, then 0.\\n\".trim();var testings=\"\\nif __name__ == \\\"__main__\\\":\\n    # Imports\\n    import matplotlib.pyplot as plt\\n    from sklearn.model_selection import train_test_split\\n    from sklearn import datasets\\n\\n    def accuracy(y_true, y_pred):\\n        accuracy = np.sum(y_true == y_pred) / len(y_true)\\n        return accuracy\\n\\n    X, y = datasets.make_blobs(\\n        n_samples=150, n_features=2, centers=2, cluster_std=1.05, random_state=2\\n    )\\n    X_train, X_test, y_train, y_test = train_test_split(\\n        X, y, test_size=0.2, random_state=123\\n    )\\n\\n    p = Perceptron(learning_rate=0.01, n_iters=1000)\\n    p.fit(X_train, y_train)\\n    predictions = p.predict(X_test)\\n\\n    print(\\\"Perceptron classification accuracy\\\", accuracy(y_test, predictions))\\n\\n    fig = plt.figure()\\n    ax = fig.add_subplot(1, 1, 1)\\n    plt.scatter(X_train[:, 0], X_train[:, 1], marker=\\\"o\\\", c=y_train)\\n\\n    x0_1 = np.amin(X_train[:, 0])\\n    x0_2 = np.amax(X_train[:, 0])\\n\\n    x1_1 = (-p.weights[0] * x0_1 - p.bias) / p.weights[1]\\n    x1_2 = (-p.weights[0] * x0_2 - p.bias) / p.weights[1]\\n\\n    ax.plot([x0_1, x0_2], [x1_1, x1_2], \\\"k\\\")\\n\\n    ymin = np.amin(X_train[:, 1])\\n    ymax = np.amax(X_train[:, 1])\\n    ax.set_ylim([ymin - 3, ymax + 3])\\n\\n    plt.show()\\n    \".trim();// const stack = ``.trim();\nvar Perceptron=/*#__PURE__*/function(_Component){_inherits(Perceptron,_Component);function Perceptron(){_classCallCheck(this,Perceptron);return _possibleConstructorReturn(this,_getPrototypeOf(Perceptron).apply(this,arguments));}_createClass(Perceptron,[{key:\"componentDidMount\",value:function componentDidMount(){setTimeout(function(){return Prism.highlightAll();},0);}},{key:\"render\",value:function render(){var classes=this.props.classes;return React.createElement(Grid,{container:true},React.createElement(Grid,{item:true,xs:2},React.createElement(Paper,{className:classes.paper},React.createElement(\"h4\",null,React.createElement(Sidebar,null)))),React.createElement(Grid,{item:true,xs:10},React.createElement(Paper,{className:classes.paper},React.createElement(List,null,React.createElement(\"h3\",null,\"Perceptron \\u2013 Basics of Neural Networks\"),\"A single-layer perceptron is the basic unit of a neural network. A perceptron consists of input values, weights and a bias, a weighted sum and activation function.\",React.createElement(\"br\",null),React.createElement(\"br\",null),React.createElement(\"i\",null,\"Perceptron consists of one/ more inputs, a processor, and only one o/p.\"),React.createElement(\"br\",null),React.createElement(\"br\",null),\"A perceptron works by taking in some numerical i/p along with what is known as weights and a bias. It then multiplies these i/p with the respective weights(weighted sum). These products are then added together along with the bias. The activation function takes the weighted sum and the bias as i/p and returns a final o/p.\",React.createElement(\"br\",null),React.createElement(\"br\",null),React.createElement(\"ul\",null,React.createElement(\"li\",null,\"A perceptron consists of four parts: input values, weights and a bias, a weighted sum, and activation function.\"),React.createElement(\"li\",null,React.createElement(\"b\",null,\"Function may look like: \"),\"y = xw + x2w2 +...+ xnwn\"),React.createElement(\"ul\",null,React.createElement(\"li\",null,\"bias  is alwase 1.\"),React.createElement(\"li\",null,\"This function is called the weighted sum because it is the sum of the weights and inputs. This looks like a good function, but what if we wanted the outputs to fall into a certain range 0 to 1.\"),React.createElement(\"li\",null,\"We can do this by using an activation function. An \",React.createElement(\"b\",null,\"activation function\"),\" is a function that converts the i/p into a certain o/p based on a set of rules.\"),React.createElement(\"br\",null),React.createElement(\"img\",{src:Equations,alt:\"Equations\",className:\"responsive\",style:redesign}),React.createElement(\"br\",null),React.createElement(\"br\",null),\"There are different kinds of activation functions that exist.\",React.createElement(\"br\",null),React.createElement(\"b\",null,\"1. Hyperbolic Tangent: \"),\"Used to o/p a number from -1 to 1.\",React.createElement(\"br\",null),React.createElement(\"b\",null,\"2. Logistic Function: \"),\"Used to o/p a number from 0 to 1.\")),React.createElement(\"br\",null),React.createElement(\"b\",null,\"Why are perceptron's used?\"),React.createElement(\"br\",null),\"Perceptrons are the building blocks of neural networks. It is typically used for supervised learning of binary classifiers.\",React.createElement(\"br\",null),React.createElement(\"img\",{src:Perceptrons,alt:\"Equations\",className:\"responsive\",style:redesign}),React.createElement(\"br\",null),\"Suppose our goal was to separates this data so that there is a distinction between the blue dots and the red dots.\",React.createElement(\"br\",null),React.createElement(\"br\",null),\"A perceptron can create a decision boundary for a binary classification, where a decision boundary is regions of space on a graph that separates different data points.\",React.createElement(\"br\",null),React.createElement(\"br\",null),\"Let wx = -0.5, wy = 0.5 and b = 0\",React.createElement(\"br\",null),\"Then the function for the perceptron.\",React.createElement(\"br\",null),\"0.5x + 0.5y = 0\",React.createElement(\"br\",null),React.createElement(\"br\",null),\"and the graph is.\",React.createElement(\"br\",null),React.createElement(\"img\",{src:PerceptronsGrapg,alt:\"Equations\",className:\"responsive\",style:redesign}),React.createElement(\"br\",null),\"Let\\u2019s suppose that the activation function, in this case, is a simple step function that outputs either 0 or 1. The perceptron function will then label the blue dots as 1 and the red dots as 0.\",React.createElement(\"div\",{style:titles},React.createElement(PrismCode,{code:label,language:\"js\",plugins:[\"line-numbers\"]})),React.createElement(\"br\",null),\"Therefore, the function 0.5x + 0.5y = 0 creates a decision boundary that separates the red and blue points.\",React.createElement(\"br\",null),React.createElement(\"img\",{src:PerceptronsGraps,alt:\"Equations\",className:\"responsive\",style:redesign}),React.createElement(\"br\",null),React.createElement(\"b\",null,\"Overall, we see that a perceptron can do basic classification using a decision boundary.\"),React.createElement(\"br\",null),React.createElement(\"h3\",null,\"Example\"),React.createElement(\"div\",{style:titles},React.createElement(PrismCode,{code:stack,language:\"js\",plugins:[\"line-numbers\"]})),React.createElement(\"br\",null),React.createElement(\"h3\",null,\"Testing\"),React.createElement(\"div\",{style:titles},React.createElement(PrismCode,{code:testings,language:\"js\",plugins:[\"line-numbers\"]}))))));}}]);return Perceptron;}(Component);export default withStyles(styles)(Perceptron);","map":{"version":3,"sources":["/home/mukeshs/Projects/edurights/src/components/ml/deepMl/perceptron.js"],"names":["React","Component","Prism","Grid","Paper","withStyles","List","Sidebar","PrismCode","Equations","Perceptrons","PerceptronsGrapg","PerceptronsGraps","titles","backgroundColor","padding","fontSize","redesign","height","width","styles","theme","paper","margin","spacing","smMargin","actionDiv","textAlign","stack","trim","label","testings","Perceptron","setTimeout","highlightAll","classes","props"],"mappings":"6kBAAA,MAAOA,CAAAA,KAAP,EAAgBC,SAAhB,KAAiC,OAAjC,CACA,MAAOC,CAAAA,KAAP,KAAkB,SAAlB,CACA,OAASC,IAAT,CAAeC,KAAf,CAAsBC,UAAtB,CAAkCC,IAAlC,KAA8C,mBAA9C,CAEA,MAAO,0BAAP,CACA,MAAOC,CAAAA,OAAP,KAAoB,YAApB,CACA,MAAOC,CAAAA,SAAP,KAAsB,yBAAtB,CAEA,MAAOC,CAAAA,SAAP,KAAsB,oCAAtB,CACA,MAAOC,CAAAA,WAAP,KAAwB,qCAAxB,CACA,MAAOC,CAAAA,gBAAP,KAA6B,qCAA7B,CACA,MAAOC,CAAAA,gBAAP,KAA6B,qCAA7B,CAGA,GAAMC,CAAAA,MAAM,CAAG,CAAEC,eAAe,CAAE,SAAnB,CAA8BC,OAAO,CAAE,KAAvC,CAA8CC,QAAQ,CAAE,MAAxD,CAAf,CAEA,GAAMC,CAAAA,QAAQ,CAAG,CACfC,MAAM,CAAE,GADO,CAEfC,KAAK,CAAE,GAFQ,CAAjB,CAKA,GAAMC,CAAAA,MAAM,CAAG,QAATA,CAAAA,MAAS,CAAAC,KAAK,QAAK,CACvBC,KAAK,CAAE,CACLC,MAAM,CAAEF,KAAK,CAACG,OAAN,CAAc,CAAd,CADH,CAELT,OAAO,CAAEM,KAAK,CAACG,OAAN,CAAc,CAAd,CAFJ,CADgB,CAKvBC,QAAQ,CAAE,CACRF,MAAM,CAAEF,KAAK,CAACG,OAAN,CAAc,CAAd,CADA,CALa,CAQvBE,SAAS,CAAE,CACTC,SAAS,CAAE,QADF,CARY,CAAL,EAApB,CAcA,GAAMC,CAAAA,KAAK,CAAG,ipCAwCZC,IAxCY,EAAd,CA0CA,GAAMC,CAAAA,KAAK,CAAG,+DAGZD,IAHY,EAAd,CAKA,GAAME,CAAAA,QAAQ,CAAG,kuCAyCXF,IAzCW,EAAjB,CA2CA;GAGMG,CAAAA,U,0SACgB,CAClBC,UAAU,CAAC,iBAAM/B,CAAAA,KAAK,CAACgC,YAAN,EAAN,EAAD,CAA6B,CAA7B,CAAV,CACD,C,uCACQ,IACCC,CAAAA,OADD,CACa,KAAKC,KADlB,CACCD,OADD,CAEP,MACE,qBAAC,IAAD,EAAM,SAAS,KAAf,EACE,oBAAC,IAAD,EAAM,IAAI,KAAV,CAAW,EAAE,CAAE,CAAf,EACE,oBAAC,KAAD,EAAO,SAAS,CAAEA,OAAO,CAACb,KAA1B,EACE,8BAAI,oBAAC,OAAD,MAAJ,CADF,CADF,CADF,CAME,oBAAC,IAAD,EAAM,IAAI,KAAV,CAAW,EAAE,CAAE,EAAf,EACE,oBAAC,KAAD,EAAO,SAAS,CAAEa,OAAO,CAACb,KAA1B,EACE,oBAAC,IAAD,MACE,4EADF,uKAIE,8BAJF,CAKE,8BALF,CAOE,uGAPF,CAQE,8BARF,CASE,8BATF,sUAaE,8BAbF,CAcE,8BAdF,CAgBE,8BACE,gJADF,CAGE,8BAAI,wDAAJ,4BAHF,CAIE,8BACE,mDADF,CAEE,kOAFF,CAIE,oFAAuD,mDAAvD,oFAJF,CAME,8BANF,CAOE,2BAAK,GAAG,CAAEb,SAAV,CAAqB,GAAG,CAAC,WAAzB,CAAqC,SAAS,CAAC,YAA/C,CAA4D,KAAK,CAAEQ,QAAnE,EAPF,CAQE,8BARF,CASE,8BATF,iEAWE,8BAXF,CAYE,uDAZF,sCAYkE,8BAZlE,CAaE,sDAbF,qCAJF,CAhBF,CAoCE,8BApCF,CAsCE,0DAtCF,CAuCE,8BAvCF,+HA0CE,8BA1CF,CA2CE,2BAAK,GAAG,CAAEP,WAAV,CAAuB,GAAG,CAAC,WAA3B,CAAuC,SAAS,CAAC,YAAjD,CAA8D,KAAK,CAAEO,QAArE,EA3CF,CA4CE,8BA5CF,sHA+CE,8BA/CF,CAgDE,8BAhDF,2KAmDE,8BAnDF,CAoDE,8BApDF,qCAsDE,8BAtDF,yCAuDuC,8BAvDvC,mBAyDE,8BAzDF,CA0DE,8BA1DF,qBA4DE,8BA5DF,CA6DE,2BAAK,GAAG,CAAEN,gBAAV,CAA4B,GAAG,CAAC,WAAhC,CAA4C,SAAS,CAAC,YAAtD,CAAmE,KAAK,CAAEM,QAA1E,EA7DF,CA8DE,8BA9DF,0MAiEE,2BAAK,KAAK,CAAEJ,MAAZ,EACE,oBAAC,SAAD,EACE,IAAI,CAAEiB,KADR,CAEE,QAAQ,CAAC,IAFX,CAGE,OAAO,CAAE,CAAC,cAAD,CAHX,EADF,CAjEF,CAwEE,8BAxEF,+GA0EE,8BA1EF,CA2EE,2BAAK,GAAG,CAAElB,gBAAV,CAA4B,GAAG,CAAC,WAAhC,CAA4C,SAAS,CAAC,YAAtD,CAAmE,KAAK,CAAEK,QAA1E,EA3EF,CA4EE,8BA5EF,CA8EE,wHA9EF,CA+EE,8BA/EF,CAiFE,wCAjFF,CAkFE,2BAAK,KAAK,CAAEJ,MAAZ,EACE,oBAAC,SAAD,EACE,IAAI,CAAEe,KADR,CAEE,QAAQ,CAAC,IAFX,CAGE,OAAO,CAAE,CAAC,cAAD,CAHX,EADF,CAlFF,CAyFE,8BAzFF,CA2FE,wCA3FF,CA4FE,2BAAK,KAAK,CAAEf,MAAZ,EACE,oBAAC,SAAD,EACE,IAAI,CAAEkB,QADR,CAEE,QAAQ,CAAC,IAFX,CAGE,OAAO,CAAE,CAAC,cAAD,CAHX,EADF,CA5FF,CADF,CADF,CANF,CADF,CAqID,C,wBA3IsB9B,S,EA8IzB,cAAgBI,CAAAA,UAAU,CAACe,MAAD,CAAV,CAAmBY,UAAnB,CAAhB","sourcesContent":["import React, { Component } from 'react';\nimport Prism from \"prismjs\"\nimport { Grid, Paper, withStyles, List } from \"@material-ui/core\";\n\nimport '../../ReactJs/styles.css'\nimport Sidebar from '../sidebar';\nimport PrismCode from '../../ReactJs/prismCode';\n\nimport Equations from '../../../assets/ML/perceptrons.png'\nimport Perceptrons from '../../../assets/ML/perceptrons2.png'\nimport PerceptronsGrapg from '../../../assets/ML/perceptrons3.png'\nimport PerceptronsGraps from '../../../assets/ML/perceptrons4.png'\n\n\nconst titles = { backgroundColor: '#F0F8FF', padding: '1px', fontSize: '16px' }\n\nconst redesign = {\n  height: 350,\n  width: 600\n}\n\nconst styles = theme => ({\n  paper: {\n    margin: theme.spacing(1),\n    padding: theme.spacing(1)\n  },\n  smMargin: {\n    margin: theme.spacing(1)\n  },\n  actionDiv: {\n    textAlign: \"center\"\n  }\n})\n\n\nconst stack = `\nimport numpy as np\n\nclass Perceptron:\n    def __init__(self, learning_rate=0.01, n_iters=1000):\n        self.lr = learning_rate\n        self.n_iters = n_iters\n        self.activation_func = self._unit_step_func\n        self.weights = None\n        self.bias = None\n\n    def fit(self, X, y):\n        n_samples, n_features = X.shape\n\n        # init parameters\n        self.weights = np.zeros(n_features)\n        self.bias = 0\n\n        y_ = np.array([1 if i > 0 else 0 for i in y])\n\n        for _ in range(self.n_iters):\n\n            for idx, x_i in enumerate(X):\n\n                linear_output = np.dot(x_i, self.weights) + self.bias\n                y_predicted = self.activation_func(linear_output)\n\n                # Perceptron update rule\n                update = self.lr * (y_[idx] - y_predicted)\n\n                self.weights += update * x_i\n                self.bias += update\n\n    def predict(self, X):\n        linear_output = np.dot(X, self.weights) + self.bias\n        y_predicted = self.activation_func(linear_output)\n        return y_predicted\n\n    def _unit_step_func(self, x):\n        return np.where(x >= 0, 1, 0)\n`.trim();\n\nconst label = `\nif 0.5x + 0.5y => 0, then 1\nif 0.5x + 0.5y < 0, then 0.\n`.trim();\n\nconst testings = `\nif __name__ == \"__main__\":\n    # Imports\n    import matplotlib.pyplot as plt\n    from sklearn.model_selection import train_test_split\n    from sklearn import datasets\n\n    def accuracy(y_true, y_pred):\n        accuracy = np.sum(y_true == y_pred) / len(y_true)\n        return accuracy\n\n    X, y = datasets.make_blobs(\n        n_samples=150, n_features=2, centers=2, cluster_std=1.05, random_state=2\n    )\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, random_state=123\n    )\n\n    p = Perceptron(learning_rate=0.01, n_iters=1000)\n    p.fit(X_train, y_train)\n    predictions = p.predict(X_test)\n\n    print(\"Perceptron classification accuracy\", accuracy(y_test, predictions))\n\n    fig = plt.figure()\n    ax = fig.add_subplot(1, 1, 1)\n    plt.scatter(X_train[:, 0], X_train[:, 1], marker=\"o\", c=y_train)\n\n    x0_1 = np.amin(X_train[:, 0])\n    x0_2 = np.amax(X_train[:, 0])\n\n    x1_1 = (-p.weights[0] * x0_1 - p.bias) / p.weights[1]\n    x1_2 = (-p.weights[0] * x0_2 - p.bias) / p.weights[1]\n\n    ax.plot([x0_1, x0_2], [x1_1, x1_2], \"k\")\n\n    ymin = np.amin(X_train[:, 1])\n    ymax = np.amax(X_train[:, 1])\n    ax.set_ylim([ymin - 3, ymax + 3])\n\n    plt.show()\n    `.trim();\n\n// const stack = ``.trim();\n\n\nclass Perceptron extends Component {\n  componentDidMount() {\n    setTimeout(() => Prism.highlightAll(), 0)\n  }\n  render() {\n    const { classes } = this.props;\n    return (\n      <Grid container>\n        <Grid item xs={2}>\n          <Paper className={classes.paper}>\n            <h4><Sidebar /></h4>\n          </Paper>\n        </Grid>\n        <Grid item xs={10}>\n          <Paper className={classes.paper}>\n            <List>\n              <h3>Perceptron – Basics of Neural Networks</h3>\n              A single-layer perceptron is the basic unit of a neural network. A perceptron consists of input values, weights\n              and a bias, a weighted sum and activation function.\n              <br />\n              <br />\n\n              <i>Perceptron consists of one/ more inputs, a processor, and only one o/p.</i>\n              <br />\n              <br />\n              A perceptron works by taking in some numerical i/p along with what is known as weights and a bias. It then\n              multiplies these i/p with the respective weights(weighted sum). These products are then added together along\n              with the bias. The activation function takes the weighted sum and the bias as i/p and returns a final o/p.\n              <br />\n              <br />\n\n              <ul>\n                <li>A perceptron consists of four parts: input values, weights and a bias, a weighted sum, and\n                  activation function.</li>\n                <li><b>Function may look like: </b>y = xw + x2w2 +...+ xnwn</li>\n                <ul>\n                  <li>bias  is alwase 1.</li>\n                  <li>This function is called the weighted sum because it is the sum of the weights and inputs. This looks like a\n                    good function, but what if we wanted the outputs to fall into a certain range 0 to 1.</li>\n                  <li>We can do this by using an activation function. An <b>activation function</b> is\n                    a function that converts the i/p into a certain o/p based on a set of rules.</li>\n                  <br />\n                  <img src={Equations} alt=\"Equations\" className=\"responsive\" style={redesign} />\n                  <br />\n                  <br />\n                  There are different kinds of activation functions that exist.\n                  <br />\n                  <b>1. Hyperbolic Tangent: </b>Used to o/p a number from -1 to 1.<br />\n                  <b>2. Logistic Function: </b>Used to o/p a number from 0 to 1.\n                </ul>\n              </ul>\n              <br />\n\n              <b>Why are perceptron's used?</b>\n              <br />\n              Perceptrons are the building blocks of neural networks. It is typically used for supervised learning of\n              binary classifiers.\n              <br />\n              <img src={Perceptrons} alt=\"Equations\" className=\"responsive\" style={redesign} />\n              <br />\n\n              Suppose our goal was to separates this data so that there is a distinction between the blue dots and the red dots.\n              <br />\n              <br />\n              A perceptron can create a decision boundary for a binary classification, where a decision boundary is\n              regions of space on a graph that separates different data points.\n              <br />\n              <br />\n              Let wx = -0.5, wy = 0.5 and b = 0\n              <br />\n              Then the function for the perceptron.<br />\n              0.5x + 0.5y = 0\n              <br />\n              <br />\n              and the graph is.\n              <br />\n              <img src={PerceptronsGrapg} alt=\"Equations\" className=\"responsive\" style={redesign} />\n              <br />\n              Let’s suppose that the activation function, in this case, is a simple step function that outputs either 0 or 1.\n              The perceptron function will then label the blue dots as 1 and the red dots as 0.\n              <div style={titles}>\n                <PrismCode\n                  code={label}\n                  language=\"js\"\n                  plugins={[\"line-numbers\"]}\n                />\n              </div>\n              <br />\n              Therefore, the function 0.5x + 0.5y = 0 creates a decision boundary that separates the red and blue points.\n              <br />\n              <img src={PerceptronsGraps} alt=\"Equations\" className=\"responsive\" style={redesign} />\n              <br />\n\n              <b>Overall, we see that a perceptron can do basic classification using a decision boundary.</b>\n              <br />\n\n              <h3>Example</h3>\n              <div style={titles}>\n                <PrismCode\n                  code={stack}\n                  language=\"js\"\n                  plugins={[\"line-numbers\"]}\n                />\n              </div>\n              <br />\n\n              <h3>Testing</h3>\n              <div style={titles}>\n                <PrismCode\n                  code={testings}\n                  language=\"js\"\n                  plugins={[\"line-numbers\"]}\n                />\n              </div>\n              {/* <br />\n\n              <h3></h3>\n              <div style={titles}>\n                <PrismCode\n                  code={stack}\n                  language=\"js\"\n                  plugins={[\"line-numbers\"]}\n                />\n              </div>\n              <br />\n\n              <h3></h3>\n              <div style={titles}>\n                <PrismCode\n                  code={stack}\n                  language=\"js\"\n                  plugins={[\"line-numbers\"]}\n                />\n              </div> */}\n            </List>\n          </Paper>\n        </Grid>\n      </Grid>\n    )\n  }\n}\n\nexport default (withStyles(styles)(Perceptron));\n"]},"metadata":{},"sourceType":"module"}