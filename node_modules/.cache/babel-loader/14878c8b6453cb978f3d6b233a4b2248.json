{"ast":null,"code":"import _classCallCheck from\"/home/mukeshs/Projects/edurights/node_modules/@babel/runtime/helpers/esm/classCallCheck\";import _createClass from\"/home/mukeshs/Projects/edurights/node_modules/@babel/runtime/helpers/esm/createClass\";import _possibleConstructorReturn from\"/home/mukeshs/Projects/edurights/node_modules/@babel/runtime/helpers/esm/possibleConstructorReturn\";import _getPrototypeOf from\"/home/mukeshs/Projects/edurights/node_modules/@babel/runtime/helpers/esm/getPrototypeOf\";import _inherits from\"/home/mukeshs/Projects/edurights/node_modules/@babel/runtime/helpers/esm/inherits\";import React,{Component}from'react';import Prism from\"prismjs\";import{Grid,Paper,withStyles,List}from\"@material-ui/core\";import'../../ReactJs/styles.css';import Sidebar from'../sidebar';import PrismCode from'../../ReactJs/prismCode';import Overfitting from'../../../assets/ML/overfitting.png';var titles={backgroundColor:'#F0F8FF',padding:'1px',fontSize:'16px'};var styles=function styles(theme){return{paper:{margin:theme.spacing(1),padding:theme.spacing(1)},smMargin:{margin:theme.spacing(1)},actionDiv:{textAlign:\"center\"}};};var redesign={height:350,width:600};var cluster=\"\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nimport pandas as pd\\nimport seaborn as sns\\n\\nimport warnings                                                                     # Suppress Warnings for clean notebook\\nwarnings.filterwarnings('ignore')\\n\\ndf = pd.read_csv('./Melbourne_housing_FULL.csv')\\ndf.head()\\ndf.nunique()\\n\\n# use limited columns which makes more sense for serving our purpose\\ncols_to_use = ['Suburb', 'Rooms', 'Type', 'Method', 'SellerG', 'Regionname', 'Propertycount', \\n               'Distance', 'CouncilArea', 'Bedroom2', 'Bathroom', 'Car', 'Landsize', 'BuildingArea', 'Price']\\n\\ndf = df[cols_to_use]\\ndf.isna().sum()                                                                 \\n\\ncols_to_fill_zero = ['Propertycount', 'Distance', 'Bedroom2', 'Bathroom', 'Car']     \\ndf[cols_to_fill_zero] = df[cols_to_fill_zero].fillna(0)\\n\\n# other continuous features can be imputed with mean for faster results since our focus is on Reducing overfitting\\n# using Lasso and Ridge Regression\\n\\ndf['Landsize'] = df['Landsize'].fillna(df.Landsize.mean())\\ndf['BuildingArea'] = df['BuildingArea'].fillna(df.BuildingArea.mean())\\n\\ndf.dropna(inplace=True)                                                        \\ndf.shape\\n\".trim();var cagorical=\"\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.linear_model import LinearRegression  \\n\\ndf = pd.get_dummies(df, drop_first=True)\\n\\nX = df.drop('Price', axis=1)\\ny = df['Price']\\n\\ntrain_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.3, random_state=2)                             \\nmodel = LinearRegression()\\nmodel.fit(train_X, train_y)\\n\\nmodel.score(test_X, test_y)\\nmodel.score(train_X, train_y)\\n\".trim();var regularized=\"\\nfrom sklearn import linear_model\\nfrom sklearn.linear_model import Ridge   \\n\\nlasso_reg = linear_model.Lasso(alpha=50, max_iter=100, tol=0.1)\\nlasso_reg.fit(train_X, train_y)\\n\\nlasso_reg.score(test_X, test_y)\\nlasso_reg.score(train_X, train_y)\\n                                               \\nridge_reg= Ridge(alpha=50, max_iter=100, tol=0.1)                                      #Using Ridge Regression Model.\\nridge_reg.fit(train_X, train_y)\\n\\nridge_reg.score(test_X, test_y)\\nridge_reg.score(train_X, train_y)\\n\".trim();var clustering=\"\\nimport pandas as pd\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.preprocessing import StandardScaler\\n\\ndf = pd.read_csv(\\\"diabetes.csv\\\")\\ndf.isnull().sum()\\ndf.Outcome.value_counts()\\n\\n#There is slight imbalance in our dataset but since it is not major we will not worry about it!\\nX = df.drop(\\\"Outcome\\\",axis=\\\"columns\\\")\\ny = df.Outcome\\n\\nscaler = StandardScaler()\\nX_scaled = scaler.fit_transform(X)\\nX_scaled[:3]\\n\\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y, stratify=y, random_state=10)\\n\\ny_train.value_counts()\\ny_test.value_counts()\\n\".trim();var standAlone=\"\\nfrom sklearn.model_selection import cross_val_score\\nfrom sklearn.tree import DecisionTreeClassifier\\n\\nscores = cross_val_score(DecisionTreeClassifier(), X, y, cv=5)\\nscores.mean()\\n\".trim();var bagging=\"\\nfrom sklearn.ensemble import BaggingClassifier\\n\\nbag_model = BaggingClassifier(\\n    base_estimator=DecisionTreeClassifier(), \\n    n_estimators=100, \\n    max_samples=0.8, \\n    oob_score=True,\\n    random_state=0\\n)\\nbag_model.fit(X_train, y_train)\\nbag_model.oob_score_\\n\\nbag_model = BaggingClassifier(\\n  base_estimator=DecisionTreeClassifier(), \\n  n_estimators=100, \\n  max_samples=0.8, \\n  oob_score=True,\\n  random_state=0\\n)\\n\\nscores = cross_val_score(bag_model, X, y, cv=5)\\nscores.mean()\\n\".trim();var improvement=\"\\nfrom sklearn.ensemble import RandomForestClassifier\\n\\nscores = cross_val_score(RandomForestClassifier(n_estimators=50), X, y, cv=5)\\nscores.mean()\\n\".trim();var Regularizations=/*#__PURE__*/function(_Component){_inherits(Regularizations,_Component);function Regularizations(){_classCallCheck(this,Regularizations);return _possibleConstructorReturn(this,_getPrototypeOf(Regularizations).apply(this,arguments));}_createClass(Regularizations,[{key:\"componentDidMount\",value:function componentDidMount(){setTimeout(function(){return Prism.highlightAll();},0);}},{key:\"render\",value:function render(){var classes=this.props.classes;return React.createElement(Grid,{container:true},React.createElement(Grid,{item:true,xs:2},React.createElement(Paper,{className:classes.paper},React.createElement(\"h4\",null,React.createElement(Sidebar,null)))),React.createElement(Grid,{item:true,xs:10},React.createElement(Paper,{className:classes.paper},React.createElement(List,null,React.createElement(\"h1\",null,\"1. What is overfitting and underfitting ?\"),React.createElement(\"ul\",null,React.createElement(\"b\",null,\"Overfitting:\"),React.createElement(\"li\",null,\"Overfitting refers to the scenario where a ML model can\\u2019t generalize or fit well on unseen dataset. A clear sign of ML overfitting is if its error on the testing/ validation dataset is much greater than the error on training dataset.\"),React.createElement(\"li\",null,\"Overfitting happens when a model learns the detail and noise in the training dataset to the extent that it negatively impacts the performance of the model on a new dataset. This means that the noise/ random fluctuations in the training dataset is picked up and learned as concepts by the model. The problem is that these concepts do not apply to new datasets and negatively impact the model\\u2019s ability to generalize.\"),React.createElement(\"br\",null),React.createElement(\"b\",null,\"Underfitting:\"),React.createElement(\"li\",null,\"Underfitting refers to a model that can neither model the training dataset nor generalize to new dataset. An underfit ML model is not a suitable model and will be obvious as it will have poor performance on the training dataset.\")),React.createElement(\"br\",null),React.createElement(\"b\",null,\"Example: \"),\"Let\\u2019s three students have prepared for a mathematics examination. The first student has only studied Addition mathematic operations and skipped other mathematics operations such as Subtraction, Division, Multiplication etc. The second student has a particularly good memory. Thus, second student has memorized all the problems presented in the textbook. And the third student has studied all mathematical operations and is well prepared for the exam. In the exam student one will only be able to solve the questions related to Addition and will fail in problems or questions asked related to other mathematics operations. Student two will only be able to answer questions if they happened to appear in the textbook and will not be able to answer any other questions. Student three will be able to solve all the exam problems reasonably well.\",React.createElement(\"br\",null),React.createElement(\"br\",null),\"ML algorithms have similar behavior to our three students, sometimes the model generated by the algorithm are similar to the first student. They learn from only from a small part of the training dataset, in such cases the model is \",React.createElement(\"b\",null,\"Underfitting\"),\".\",React.createElement(\"br\",null),React.createElement(\"br\",null),\"Sometimes the model will memorize the entire training dataset, like the second student. They perform very well on known instances, but faulter badly on unseen data. In such cases the model is said to be \",React.createElement(\"b\",null,\"Overfitting\"),\".\",React.createElement(\"br\",null),React.createElement(\"br\",null),\"And when model does well in both the training dataset and unseen data, it is a good fit.\",React.createElement(\"br\",null),React.createElement(\"br\",null),React.createElement(\"br\",null),React.createElement(\"b\",null,\"Example: \"),\"Consider you have visited a city \\u201CX\\u201D and took a ride in a taxi. On speaking to friends, you later realize that the taxi driver charged you twice/ three times more than the standard fare. This occurred as you were new in the city and driver quite literally took you for a ride. Also, you purchased some items from a street vendor, and you again ended up paying more than they were worth. You finally decide that the people in the city \\u201CX\\u201D are dishonest. Which is a human trait, people often generalize.\",React.createElement(\"br\",null),React.createElement(\"br\",null),\"ML models also have this weakness if we are not careful to avoid bias during the development stages: modeling, selecting algorithms, features, training dataset etc.\",React.createElement(\"br\",null),\"Suppose in the same city \\u201CX\\u201D another taxi driver charged you reasonably and as per the meter, but based on experience, you consider that this driver has also charged more. This is called Overfitting.\",React.createElement(\"br\",null),React.createElement(\"br\",null),\"So can say that, if model performs well on test or unseen dataset then it is a best fit/ good model. And if did not perform well on test or unseen dataset but did well on training dataset then it is an Overfit model. And any model that did not do well in the training dataset nor in test dataset then it is a Underfit model.\",React.createElement(\"br\",null),React.createElement(\"img\",{src:Overfitting,alt:\"Equations\",className:\"responsive\",style:redesign}),React.createElement(\"br\",null),React.createElement(\"br\",null),React.createElement(\"b\",null,\"Detecting Overfitting or Underfitting :\"),React.createElement(\"br\",null),\"A key challenge of detecting any kind of fit (underfitting or best fit or overfitting), is almost impossible before you test the data.\",React.createElement(\"br\",null),React.createElement(\"br\",null),React.createElement(\"ul\",null,React.createElement(\"li\",null,\"If our model does much better on the training dataset than on the test dataset, then we\\u2019re likely overfitting.\"),React.createElement(\"li\",null,\"If our model does much better on the test dataset than on the training dataset, then we are likely underfitting.\"),React.createElement(\"li\",null,\"If our model does well on both the training and test datasets then we have the best fit.\")),React.createElement(\"br\",null),\"Another simple way to detect this is by using cross-validation. This attempts to examine the trained model with a new data set to check its predictive accuracy. Given a dataset, some portion of this is held back (30%) while the rest is used in training the model. Once the model has been trained the reserved data is then used to check the accuracy of the model compared to the accuracy of derived from the data used in training. A significant variance in these two flags overfitting.\",React.createElement(\"br\",null),React.createElement(\"br\",null),React.createElement(\"b\",null,\"How to Prevent Overfitting or Underfitting :\"),React.createElement(\"br\",null),\"Detecting overfitting or underfitting is useful, but it does not solve the problem. Here are a few of the most popular solutions.\",React.createElement(\"br\",null),React.createElement(\"b\",null,\"1. Cross-validation:\"),React.createElement(\"ul\",null,React.createElement(\"li\",null,\"Cross-validation is a powerful preventative measure against overfitting.\"),React.createElement(\"li\",null,\"Use initial training data to generate multiple mini train-test splits. Use these splits to tune your model.\"),React.createElement(\"li\",null,\"In standard k-fold cross-validation, we partition the data into k subsets, called folds. Then, we iteratively train the algorithm on k-1 folds while using the remaining fold as the test set or holdout fold.\"),React.createElement(\"li\",null,\"Cross-validation allows to tune hyperparameters with only original training dataset. This allows to keep test dataset as a truly unseen dataset for selecting final model.\")),React.createElement(\"br\",null),React.createElement(\"b\",null,\"2. Train with more data:\"),React.createElement(\"ul\",null,React.createElement(\"li\",null,\"It won\\u2019t work every time, but training with more data can help algorithms detect the signal better.\"),React.createElement(\"li\",null,\"As the user feeds more training data into the model, it will be unable to overfit all the samples and will be forced to generalize to obtain results.\"),React.createElement(\"li\",null,\"However, this method is considered expensive, and, therefore, users should ensure that the data being used is relevant and clean.\"),React.createElement(\"li\",null,\"If we just add more noisy data, this technique will not help. That is why we should always ensure our data is clean and relevant.\")),React.createElement(\"br\",null),React.createElement(\"b\",null,\"3. Data augmentation:\"),React.createElement(\"ul\",null,React.createElement(\"li\",null,\"If unable to continually collect more data, we can make the available data sets appear diverse.\"),React.createElement(\"li\",null,\"Data augmentation makes a data sample look slightly different every time it is processed by the model. The process makes each data set appear unique to the model and prevents the model from learning the characteristics of the data sets.\")),React.createElement(\"br\",null),React.createElement(\"b\",null,\"4. Reduce Complexity or Data Simplification:\"),React.createElement(\"ul\",null,React.createElement(\"li\",null,\"Overfitting can occur due to the complexity of a model, such that, even with large volumes of data, the model still manages to overfit the training dataset.\"),React.createElement(\"li\",null,\"Reduce overfitting by decreasing the complexity of the model to make it simple enough that it does not overfit.\"),React.createElement(\"li\",null,\"Some actions that can be implemented include pruning a decision tree, reducing the number of parameters in a Neural Networks, and using dropout on a Neural Networks.\")),React.createElement(\"br\",null),React.createElement(\"b\",null,\"5. Regularization:\"),React.createElement(\"ul\",null,React.createElement(\"li\",null,\"Regularization refers to a broad range of techniques for artificially forcing our model to be simpler.\"),React.createElement(\"li\",null,\"The method will depend on the type of learner we are using.\"),React.createElement(\"li\",null,\"Oftentimes, the regularization method is a hyperparameter as well, which means it can be tuned through cross-validation.\")),React.createElement(\"br\",null),React.createElement(\"b\",null,\"6. Ensembling:\"),React.createElement(\"ul\",null,React.createElement(\"li\",null,\"Ensembles are ML methods for combining predictions from multiple separate models. There are different methods for ensembling, but the two most common are. \",React.createElement(\"b\",null,\"Boosting and Bagging\"),\".\"),React.createElement(\"li\",null,\"Boosting works by using simple base models to increase their aggregate complexity. It trains a large number of weak learners arranged in a sequence, such that each learner in the sequence learns from the mistakes of the learner before it.\"),React.createElement(\"br\",null),React.createElement(\"b\",null,\"Boosting: \"),React.createElement(\"ul\",null,React.createElement(\"li\",null,\"Attempts to improve the predictive flexibility of simple models.\"),React.createElement(\"li\",null,\"Combines all the weak learners in the sequence to bring out one strong learner.\")),React.createElement(\"br\",null),React.createElement(\"b\",null,\"Bagging: \"),React.createElement(\"ul\",null,React.createElement(\"li\",null,\"It works by training many strong learners arranged in a parallel pattern and then combining them to optimize their predictions.\"),React.createElement(\"li\",null,\"Attempts to reduce the chance of overfitting complex models.\"),React.createElement(\"li\",null,\"It combines all the strong learners together to \\\"smooth out\\\" their predictions.\"))),React.createElement(\"br\",null),React.createElement(\"b\",null,\"7. Early Stopping:\"),React.createElement(\"ul\",null,React.createElement(\"li\",null,\"When training a learning algorithm iteratively, we can measure how well each iteration of the model performs.\"),React.createElement(\"li\",null,\"Up until a certain number of iterations, new iterations improve the model. After that point, however, the model\\u2019s ability to generalize can weaken as it begins to overfit the training data.\"),React.createElement(\"li\",null,\"Early stopping refers stopping the training process before the learner passes that point.\"),React.createElement(\"li\",null,\"Today, this technique is mostly used in deep learning while other techniques (regularization) are preferred for classical machine learning.\")),React.createElement(\"br\",null),React.createElement(\"h3\",null,\"2. L1 and L2 Regularization\"),\"We are going to use Melbourne House Price Dataset where we'll predict House Predictions based on various features.\",React.createElement(\"br\",null),React.createElement(\"br\",null),React.createElement(\"i\",null,\"housing price prediction where we will see a model overfit when we use simple linear regression. Then we will use Lasso regression (L1 regularization) and ridge regression (L2 regression) to address this overfitting issue.\"),React.createElement(\"br\",null),React.createElement(\"br\",null),React.createElement(\"b\",null,\"Steps to handling Missing values:\"),React.createElement(\"ul\",null,React.createElement(\"li\",null,\"Some feature's missing values can be treated as zero (or NA values).\"),React.createElement(\"li\",null,\"Like 0 for Propertycount, Bedroom2 will refer to other class of NA values.\"),React.createElement(\"li\",null,\"like 0 for Car feature will mean that there's no car parking feature with house.\")),React.createElement(\"br\",null),React.createElement(\"br\",null),React.createElement(\"div\",{style:titles},React.createElement(PrismCode,{code:cluster,language:\"js\",plugins:[\"line-numbers\"]})),React.createElement(\"br\",null),React.createElement(\"h3\",null,\"3. Let's one hot encode the categorical features\"),React.createElement(\"div\",{style:titles},React.createElement(PrismCode,{code:cagorical,language:\"js\",plugins:[\"line-numbers\"]})),React.createElement(\"i\",null,\"Here training score is 68% but test score is 13.85% which is very low.\"),React.createElement(\"br\",null),React.createElement(\"h3\",null,\"4. Normal Regression is clearly overfitting the data, let's try other models\"),\"Using Lasso (L1 Regularized) Regression Model.\",React.createElement(\"div\",{style:titles},React.createElement(PrismCode,{code:regularized,language:\"js\",plugins:[\"line-numbers\"]})),React.createElement(\"i\",null,\"We see that Lasso and Ridge Regularizations prove to be beneficial when our Simple Linear Regression Model overfits. These results may not be that contrast but significant in most cases.Also that L1 & L2 Regularizations are used in Neural Networks too.\"),React.createElement(\"br\",null),React.createElement(\"br\",null),React.createElement(\"h3\",null,\"5. Ensemble Learning: Bagging\"),\"We will use pima indian diabetes dataset to predict if a person has a diabetes or not based on certain features such as blood pressure, skin thickness, age etc. We will train a standalone model first and then use bagging ensemble technique to check how it can improve the performance of the model.\",React.createElement(\"div\",{style:titles},React.createElement(PrismCode,{code:clustering,language:\"js\",plugins:[\"line-numbers\"]})),React.createElement(\"br\",null),React.createElement(\"h3\",null,\"6. Train using stand alone model\"),React.createElement(\"div\",{style:titles},React.createElement(PrismCode,{code:standAlone,language:\"js\",plugins:[\"line-numbers\"]})),React.createElement(\"br\",null),React.createElement(\"h3\",null,\"7. Train using Bagging\"),React.createElement(\"div\",{style:titles},React.createElement(PrismCode,{code:bagging,language:\"js\",plugins:[\"line-numbers\"]})),React.createElement(\"br\",null),React.createElement(\"h3\",null,\"8. We can see some improvement in test score with bagging classifier as compared to a standalone classifier\"),React.createElement(\"b\",null,\"Train using Random Forest\"),React.createElement(\"br\",null),React.createElement(\"div\",{style:titles},React.createElement(PrismCode,{code:improvement,language:\"js\",plugins:[\"line-numbers\"]}))))));}}]);return Regularizations;}(Component);export default withStyles(styles)(Regularizations);","map":{"version":3,"sources":["/home/mukeshs/Projects/edurights/src/components/ml/deepMl/regularizations.js"],"names":["React","Component","Prism","Grid","Paper","withStyles","List","Sidebar","PrismCode","Overfitting","titles","backgroundColor","padding","fontSize","styles","theme","paper","margin","spacing","smMargin","actionDiv","textAlign","redesign","height","width","cluster","trim","cagorical","regularized","clustering","standAlone","bagging","improvement","Regularizations","setTimeout","highlightAll","classes","props"],"mappings":"6kBAAA,MAAOA,CAAAA,KAAP,EAAgBC,SAAhB,KAAiC,OAAjC,CACA,MAAOC,CAAAA,KAAP,KAAkB,SAAlB,CACA,OAASC,IAAT,CAAeC,KAAf,CAAsBC,UAAtB,CAAkCC,IAAlC,KAA8C,mBAA9C,CAEA,MAAO,0BAAP,CACA,MAAOC,CAAAA,OAAP,KAAoB,YAApB,CACA,MAAOC,CAAAA,SAAP,KAAsB,yBAAtB,CAEA,MAAOC,CAAAA,WAAP,KAAwB,oCAAxB,CAGA,GAAMC,CAAAA,MAAM,CAAG,CAAEC,eAAe,CAAE,SAAnB,CAA8BC,OAAO,CAAE,KAAvC,CAA8CC,QAAQ,CAAE,MAAxD,CAAf,CAEA,GAAMC,CAAAA,MAAM,CAAG,QAATA,CAAAA,MAAS,CAAAC,KAAK,QAAK,CACvBC,KAAK,CAAE,CACLC,MAAM,CAAEF,KAAK,CAACG,OAAN,CAAc,CAAd,CADH,CAELN,OAAO,CAAEG,KAAK,CAACG,OAAN,CAAc,CAAd,CAFJ,CADgB,CAKvBC,QAAQ,CAAE,CACRF,MAAM,CAAEF,KAAK,CAACG,OAAN,CAAc,CAAd,CADA,CALa,CAQvBE,SAAS,CAAE,CACTC,SAAS,CAAE,QADF,CARY,CAAL,EAApB,CAaA,GAAMC,CAAAA,QAAQ,CAAG,CACfC,MAAM,CAAE,GADO,CAEfC,KAAK,CAAE,GAFQ,CAAjB,CAMA,GAAMC,CAAAA,OAAO,CAAG,ouCA+BdC,IA/Bc,EAAhB,CAiCA,GAAMC,CAAAA,SAAS,CAAG,6bAehBD,IAfgB,EAAlB,CAiBA,GAAME,CAAAA,WAAW,CAAG,2gBAelBF,IAfkB,EAApB,CAiBA,GAAMG,CAAAA,UAAU,CAAG,qlBAqBjBH,IArBiB,EAAnB,CAuBA,GAAMI,CAAAA,UAAU,CAAG,4LAMjBJ,IANiB,EAAnB,CAQA,GAAMK,CAAAA,OAAO,CAAG,4fAuBdL,IAvBc,EAAhB,CAyBA,GAAMM,CAAAA,WAAW,CAAG,0JAKlBN,IALkB,EAApB,C,GAQMO,CAAAA,e,mUACgB,CAClBC,UAAU,CAAC,iBAAMhC,CAAAA,KAAK,CAACiC,YAAN,EAAN,EAAD,CAA6B,CAA7B,CAAV,CACD,C,uCACQ,IACCC,CAAAA,OADD,CACa,KAAKC,KADlB,CACCD,OADD,CAEP,MACE,qBAAC,IAAD,EAAM,SAAS,KAAf,EACE,oBAAC,IAAD,EAAM,IAAI,KAAV,CAAW,EAAE,CAAE,CAAf,EACE,oBAAC,KAAD,EAAO,SAAS,CAAEA,OAAO,CAACpB,KAA1B,EACE,8BAAI,oBAAC,OAAD,MAAJ,CADF,CADF,CADF,CAME,oBAAC,IAAD,EAAM,IAAI,KAAV,CAAW,EAAE,CAAE,EAAf,EACE,oBAAC,KAAD,EAAO,SAAS,CAAEoB,OAAO,CAACpB,KAA1B,EACE,oBAAC,IAAD,MACE,0EADF,CAEE,8BACE,4CADF,CAEE,+QAFF,CAKE,qcALF,CAUE,8BAVF,CAYE,6CAZF,CAaE,qQAbF,CAFF,CAmBE,8BAnBF,CAoBE,yCApBF,k1BAsCE,8BAtCF,CAuCE,8BAvCF,2OA0C0B,4CA1C1B,KA2CE,8BA3CF,CA4CE,8BA5CF,+MA+Ca,2CA/Cb,KAgDE,8BAhDF,CAiDE,8BAjDF,4FAmDE,8BAnDF,CAoDE,8BApDF,CAqDE,8BArDF,CAsDE,yCAtDF,6gBA6DE,8BA7DF,CA8DE,8BA9DF,wKAiEE,8BAjEF,qNAoEE,8BApEF,CAqEE,8BArEF,wUA0EE,8BA1EF,CA2EE,2BAAK,GAAG,CAAEP,WAAV,CAAuB,GAAG,CAAC,WAA3B,CAAuC,SAAS,CAAC,YAAjD,CAA8D,KAAK,CAAEa,QAArE,EA3EF,CA4EE,8BA5EF,CA6EE,8BA7EF,CA+EE,uEA/EF,CAgFE,8BAhFF,0IAmFE,8BAnFF,CAoFE,8BApFF,CAqFE,8BACE,oJADF,CAEE,iJAFF,CAGE,yHAHF,CArFF,CA0FE,8BA1FF,weAiGE,8BAjGF,CAkGE,8BAlGF,CAmGE,4EAnGF,CAoGE,8BApGF,qIAsGE,8BAtGF,CAuGE,oDAvGF,CAwGE,8BACE,yGADF,CAEE,4IAFF,CAIE,+OAJF,CAOE,2MAPF,CAxGF,CAkHE,8BAlHF,CAoHE,wDApHF,CAqHE,8BACE,yIADF,CAEE,sLAFF,CAIE,kKAJF,CAME,kKANF,CArHF,CA8HE,8BA9HF,CAgIE,qDAhIF,CAiIE,8BACE,gIADF,CAEE,6QAFF,CAjIF,CAuIE,8BAvIF,CAyIE,4EAzIF,CA0IE,8BACE,6LADF,CAGE,gJAHF,CAIE,sMAJF,CA1IF,CAiJE,8BAjJF,CAmJE,kDAnJF,CAoJE,8BACE,uIADF,CAEE,4FAFF,CAGE,yJAHF,CApJF,CA0JE,8BA1JF,CA4JE,8CA5JF,CA6JE,8BACE,4LACiE,oDADjE,KADF,CAGE,+QAHF,CAME,8BANF,CAOE,0CAPF,CAQE,8BACE,iGADF,CAEE,gHAFF,CARF,CAYE,8BAZF,CAaE,yCAbF,CAcE,8BACE,gKADF,CAGE,6FAHF,CAIE,kHAJF,CAdF,CA7JF,CAkLE,8BAlLF,CAoLE,kDApLF,CAqLE,8BACE,8IADF,CAGE,mOAHF,CAKE,0HALF,CAME,4KANF,CArLF,CA8LE,8BA9LF,CAgME,4DAhMF,sHAmME,8BAnMF,CAoME,8BApMF,CAqME,8PArMF,CAwME,8BAxMF,CAyME,8BAzMF,CA0ME,iEA1MF,CA2ME,8BACE,qGADF,CAEE,2GAFF,CAGE,iHAHF,CA3MF,CAgNE,8BAhNF,CAiNE,8BAjNF,CAmNE,2BAAK,KAAK,CAAEZ,MAAZ,EACE,oBAAC,SAAD,EACE,IAAI,CAAEe,OADR,CAEE,QAAQ,CAAC,IAFX,CAGE,OAAO,CAAE,CAAC,cAAD,CAHX,EADF,CAnNF,CA0NE,8BA1NF,CA4NE,iFA5NF,CA6NE,2BAAK,KAAK,CAAEf,MAAZ,EACE,oBAAC,SAAD,EACE,IAAI,CAAEiB,SADR,CAEE,QAAQ,CAAC,IAFX,CAGE,OAAO,CAAE,CAAC,cAAD,CAHX,EADF,CA7NF,CAoOE,sGApOF,CAqOE,8BArOF,CAuOE,6GAvOF,kDAyOE,2BAAK,KAAK,CAAEjB,MAAZ,EACE,oBAAC,SAAD,EACE,IAAI,CAAEkB,WADR,CAEE,QAAQ,CAAC,IAFX,CAGE,OAAO,CAAE,CAAC,cAAD,CAHX,EADF,CAzOF,CAgPE,4RAhPF,CAiPE,8BAjPF,CAkPE,8BAlPF,CAoPE,8DApPF,6SAsPE,2BAAK,KAAK,CAAElB,MAAZ,EACE,oBAAC,SAAD,EACE,IAAI,CAAEmB,UADR,CAEE,QAAQ,CAAC,IAFX,CAGE,OAAO,CAAE,CAAC,cAAD,CAHX,EADF,CAtPF,CA6PE,8BA7PF,CA+PE,iEA/PF,CAgQE,2BAAK,KAAK,CAAEnB,MAAZ,EACE,oBAAC,SAAD,EACE,IAAI,CAAEoB,UADR,CAEE,QAAQ,CAAC,IAFX,CAGE,OAAO,CAAE,CAAC,cAAD,CAHX,EADF,CAhQF,CAuQE,8BAvQF,CAyQE,uDAzQF,CA0QE,2BAAK,KAAK,CAAEpB,MAAZ,EACE,oBAAC,SAAD,EACE,IAAI,CAAEqB,OADR,CAEE,QAAQ,CAAC,IAFX,CAGE,OAAO,CAAE,CAAC,cAAD,CAHX,EADF,CA1QF,CAiRE,8BAjRF,CAmRE,4IAnRF,CAoRE,yDApRF,CAqRE,8BArRF,CAsRE,2BAAK,KAAK,CAAErB,MAAZ,EACE,oBAAC,SAAD,EACE,IAAI,CAAEsB,WADR,CAEE,QAAQ,CAAC,IAFX,CAGE,OAAO,CAAE,CAAC,cAAD,CAHX,EADF,CAtRF,CADF,CADF,CANF,CADF,CA2SD,C,6BAjT2B/B,S,EAoT9B,cAAgBI,CAAAA,UAAU,CAACS,MAAD,CAAV,CAAmBmB,eAAnB,CAAhB","sourcesContent":["import React, { Component } from 'react';\nimport Prism from \"prismjs\"\nimport { Grid, Paper, withStyles, List } from \"@material-ui/core\";\n\nimport '../../ReactJs/styles.css'\nimport Sidebar from '../sidebar';\nimport PrismCode from '../../ReactJs/prismCode';\n\nimport Overfitting from '../../../assets/ML/overfitting.png'\n\n\nconst titles = { backgroundColor: '#F0F8FF', padding: '1px', fontSize: '16px' }\n\nconst styles = theme => ({\n  paper: {\n    margin: theme.spacing(1),\n    padding: theme.spacing(1)\n  },\n  smMargin: {\n    margin: theme.spacing(1)\n  },\n  actionDiv: {\n    textAlign: \"center\"\n  }\n})\n\nconst redesign = {\n  height: 350,\n  width: 600\n}\n\n\nconst cluster = `\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n\nimport warnings                                                                     # Suppress Warnings for clean notebook\nwarnings.filterwarnings('ignore')\n\ndf = pd.read_csv('./Melbourne_housing_FULL.csv')\ndf.head()\ndf.nunique()\n\n# use limited columns which makes more sense for serving our purpose\ncols_to_use = ['Suburb', 'Rooms', 'Type', 'Method', 'SellerG', 'Regionname', 'Propertycount', \n               'Distance', 'CouncilArea', 'Bedroom2', 'Bathroom', 'Car', 'Landsize', 'BuildingArea', 'Price']\n\ndf = df[cols_to_use]\ndf.isna().sum()                                                                 \n\ncols_to_fill_zero = ['Propertycount', 'Distance', 'Bedroom2', 'Bathroom', 'Car']     \ndf[cols_to_fill_zero] = df[cols_to_fill_zero].fillna(0)\n\n# other continuous features can be imputed with mean for faster results since our focus is on Reducing overfitting\n# using Lasso and Ridge Regression\n\ndf['Landsize'] = df['Landsize'].fillna(df.Landsize.mean())\ndf['BuildingArea'] = df['BuildingArea'].fillna(df.BuildingArea.mean())\n\ndf.dropna(inplace=True)                                                        \ndf.shape\n`.trim();\n\nconst cagorical = `\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression  \n\ndf = pd.get_dummies(df, drop_first=True)\n\nX = df.drop('Price', axis=1)\ny = df['Price']\n\ntrain_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.3, random_state=2)                             \nmodel = LinearRegression()\nmodel.fit(train_X, train_y)\n\nmodel.score(test_X, test_y)\nmodel.score(train_X, train_y)\n`.trim();\n\nconst regularized = `\nfrom sklearn import linear_model\nfrom sklearn.linear_model import Ridge   \n\nlasso_reg = linear_model.Lasso(alpha=50, max_iter=100, tol=0.1)\nlasso_reg.fit(train_X, train_y)\n\nlasso_reg.score(test_X, test_y)\nlasso_reg.score(train_X, train_y)\n                                               \nridge_reg= Ridge(alpha=50, max_iter=100, tol=0.1)                                      #Using Ridge Regression Model.\nridge_reg.fit(train_X, train_y)\n\nridge_reg.score(test_X, test_y)\nridge_reg.score(train_X, train_y)\n`.trim();\n\nconst clustering = `\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\ndf = pd.read_csv(\"diabetes.csv\")\ndf.isnull().sum()\ndf.Outcome.value_counts()\n\n#There is slight imbalance in our dataset but since it is not major we will not worry about it!\nX = df.drop(\"Outcome\",axis=\"columns\")\ny = df.Outcome\n\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\nX_scaled[:3]\n\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y, stratify=y, random_state=10)\n\ny_train.value_counts()\ny_test.value_counts()\n`.trim();\n\nconst standAlone = `\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.tree import DecisionTreeClassifier\n\nscores = cross_val_score(DecisionTreeClassifier(), X, y, cv=5)\nscores.mean()\n`.trim();\n\nconst bagging = `\nfrom sklearn.ensemble import BaggingClassifier\n\nbag_model = BaggingClassifier(\n    base_estimator=DecisionTreeClassifier(), \n    n_estimators=100, \n    max_samples=0.8, \n    oob_score=True,\n    random_state=0\n)\nbag_model.fit(X_train, y_train)\nbag_model.oob_score_\n\nbag_model = BaggingClassifier(\n  base_estimator=DecisionTreeClassifier(), \n  n_estimators=100, \n  max_samples=0.8, \n  oob_score=True,\n  random_state=0\n)\n\nscores = cross_val_score(bag_model, X, y, cv=5)\nscores.mean()\n`.trim();\n\nconst improvement = `\nfrom sklearn.ensemble import RandomForestClassifier\n\nscores = cross_val_score(RandomForestClassifier(n_estimators=50), X, y, cv=5)\nscores.mean()\n`.trim();\n\n\nclass Regularizations extends Component {\n  componentDidMount() {\n    setTimeout(() => Prism.highlightAll(), 0)\n  }\n  render() {\n    const { classes } = this.props;\n    return (\n      <Grid container>\n        <Grid item xs={2}>\n          <Paper className={classes.paper}>\n            <h4><Sidebar /></h4>\n          </Paper>\n        </Grid>\n        <Grid item xs={10}>\n          <Paper className={classes.paper}>\n            <List>\n              <h1>1. What is overfitting and underfitting ?</h1>\n              <ul>\n                <b>Overfitting:</b>\n                <li>Overfitting refers to the scenario where a ML model can’t generalize or\n                  fit well on unseen dataset. A clear sign of ML overfitting is if its error on the\n                  testing/ validation dataset is much greater than the error on training dataset.</li>\n                <li>Overfitting happens when a model learns the detail and noise in the training dataset to the\n                  extent that it negatively impacts the performance of the model on a new dataset. This means that\n                  the noise/ random fluctuations in the training dataset is picked up and learned as concepts by\n                  the model. The problem is that these concepts do not apply to new datasets and negatively impact\n                  the model’s ability to generalize.</li>\n                <br />\n\n                <b>Underfitting:</b>\n                <li>Underfitting refers to a model that can neither model the training dataset nor generalize to new\n                  dataset. An underfit ML model is not a suitable model and will be obvious as it will\n                  have poor performance on the training dataset.</li>\n              </ul>\n              <br />\n              <b>Example: </b>\n              Let’s three students have prepared for a mathematics examination.\n\n              The first student has only studied Addition mathematic operations and skipped other mathematics operations\n              such as Subtraction, Division, Multiplication etc.\n\n              The second student has a particularly good memory. Thus, second student has memorized all the problems presented\n              in the textbook.\n\n              And the third student has studied all mathematical operations and is well prepared for the exam.\n\n              In the exam student one will only be able to solve the questions related to Addition and will fail in problems or\n              questions asked related to other mathematics operations.\n\n              Student two will only be able to answer questions if they happened to appear in the textbook and will not be able\n              to answer any other questions.\n\n              Student three will be able to solve all the exam problems reasonably well.\n              <br />\n              <br />\n              ML algorithms have similar behavior to our three students, sometimes the model generated by the\n              algorithm are similar to the first student. They learn from only from a small part of the training dataset, in\n              such cases the model is <b>Underfitting</b>.\n              <br />\n              <br />\n              Sometimes the model will memorize the entire training dataset, like the second student. They perform very\n              well on known instances, but faulter badly on unseen data. In such cases the model is\n              said to be <b>Overfitting</b>.\n              <br />\n              <br />\n              And when model does well in both the training dataset and unseen data, it is a good fit.\n              <br />\n              <br />\n              <br />\n              <b>Example: </b>\n              Consider you have visited a city “X” and took a ride in a taxi. On speaking to friends, you later realize that\n              the taxi driver charged you twice/ three times more than the standard fare. This occurred as you were new in\n              the city and driver quite literally took you for a ride.\n\n              Also, you purchased some items from a street vendor, and you again ended up paying more than they were worth. You\n              finally decide that the people in the city “X” are dishonest. Which is a human trait, people often generalize.\n              <br />\n              <br />\n              ML models also have this weakness if we are not careful to avoid bias during the development stages:\n              modeling, selecting algorithms, features, training dataset etc.\n              <br />\n              Suppose in the same city “X” another taxi driver charged you reasonably and as per the meter, but based\n              on experience, you consider that this driver has also charged more. This is called Overfitting.\n              <br />\n              <br />\n              So can say that, if model performs well on test or unseen dataset then it is a\n              best fit/ good model. And if did not perform well on test or unseen dataset but did well on training\n              dataset then it is an Overfit model. And any model that did not do well in the training dataset nor in\n              test dataset then it is a Underfit model.\n              <br />\n              <img src={Overfitting} alt=\"Equations\" className=\"responsive\" style={redesign} />\n              <br />\n              <br />\n\n              <b>Detecting Overfitting or Underfitting :</b>\n              <br />\n              A key challenge of detecting any kind of fit (underfitting or best fit or overfitting), is almost\n              impossible before you test the data.\n              <br />\n              <br />\n              <ul>\n                <li>If our model does much better on the training dataset than on the test dataset, then we’re likely overfitting.</li>\n                <li>If our model does much better on the test dataset than on the training dataset, then we are likely underfitting.</li>\n                <li>If our model does well on both the training and test datasets then we have the best fit.</li>\n              </ul>\n              <br />\n\n              Another simple way to detect this is by using cross-validation. This attempts to examine the trained\n              model with a new data set to check its predictive accuracy. Given a dataset, some portion of this is\n              held back (30%) while the rest is used in training the model. Once the model has been trained the\n              reserved data is then used to check the accuracy of the model compared to the accuracy of derived from\n              the data used in training. A significant variance in these two flags overfitting.\n              <br />\n              <br />\n              <b>How to Prevent Overfitting or Underfitting :</b>\n              <br />\n              Detecting overfitting or underfitting is useful, but it does not solve the problem. Here are a few of the most popular solutions.\n              <br />\n              <b>1. Cross-validation:</b>\n              <ul>\n                <li>Cross-validation is a powerful preventative measure against overfitting.</li>\n                <li>Use initial training data to generate multiple mini train-test splits. Use these splits to tune\n                  your model.</li>\n                <li>In standard k-fold cross-validation, we partition the data into k subsets, called folds. Then,\n                  we iteratively train the algorithm on k-1 folds while using the remaining fold as the test set or\n                  holdout fold.</li>\n                <li>Cross-validation allows to tune hyperparameters with only original training dataset.\n                  This allows to keep test dataset as a truly unseen dataset for selecting final model.</li>\n              </ul>\n              <br />\n\n              <b>2. Train with more data:</b>\n              <ul>\n                <li>It won’t work every time, but training with more data can help algorithms detect the signal better.</li>\n                <li>As the user feeds more training data into the model, it will be unable to overfit all the samples\n                  and will be forced to generalize to obtain results.</li>\n                <li>However, this method is considered expensive, and, therefore, users should ensure that the data\n                  being used is relevant and clean.</li>\n                <li>If we just add more noisy data, this technique will not\n                  help. That is why we should always ensure our data is clean and relevant.</li>\n              </ul>\n              <br />\n\n              <b>3. Data augmentation:</b>\n              <ul>\n                <li>If unable to continually collect more data, we can make the available data sets appear diverse.</li>\n                <li>Data augmentation makes a data sample look slightly different every time it is processed by the\n                  model. The process makes each data set appear unique to the model and prevents the model from\n                  learning the characteristics of the data sets.</li>\n              </ul>\n              <br />\n\n              <b>4. Reduce Complexity or Data Simplification:</b>\n              <ul>\n                <li>Overfitting can occur due to the complexity of a model, such that, even with large volumes of\n                  data, the model still manages to overfit the training dataset.</li>\n                <li>Reduce overfitting by decreasing the complexity of the model to make it simple enough that it does not overfit.</li>\n                <li>Some actions that can be implemented include pruning a decision tree, reducing the number\n                  of parameters in a Neural Networks, and using dropout on a Neural Networks.</li>\n              </ul>\n              <br />\n\n              <b>5. Regularization:</b>\n              <ul>\n                <li>Regularization refers to a broad range of techniques for artificially forcing our model to be simpler.</li>\n                <li>The method will depend on the type of learner we are using.</li>\n                <li>Oftentimes, the regularization method is a hyperparameter as well, which means it can be tuned through\n                  cross-validation.</li>\n              </ul>\n              <br />\n\n              <b>6. Ensembling:</b>\n              <ul>\n                <li>Ensembles are ML methods for combining predictions from multiple separate models. There are\n                  different methods for ensembling, but the two most common are. <b>Boosting and Bagging</b>.</li>\n                <li>Boosting works by using simple base models to increase their aggregate complexity. It trains a\n                  large number of weak learners arranged in a sequence, such that each learner in the sequence learns\n                  from the mistakes of the learner before it.</li>\n                <br />\n                <b>Boosting: </b>\n                <ul>\n                  <li>Attempts to improve the predictive flexibility of simple models.</li>\n                  <li>Combines all the weak learners in the sequence to bring out one strong learner.</li>\n                </ul>\n                <br />\n                <b>Bagging: </b>\n                <ul>\n                  <li>It works by training many strong learners arranged in a parallel pattern and then combining\n                    them to optimize their predictions.</li>\n                  <li>Attempts to reduce the chance of overfitting complex models.</li>\n                  <li>It combines all the strong learners together to \"smooth out\" their predictions.</li>\n                </ul>\n              </ul>\n              <br />\n\n              <b>7. Early Stopping:</b>\n              <ul>\n                <li>When training a learning algorithm iteratively, we can measure how well each iteration of the\n                  model performs.</li>\n                <li>Up until a certain number of iterations, new iterations improve the model. After that point,\n                  however, the model’s ability to generalize can weaken as it begins to overfit the training data.</li>\n                <li>Early stopping refers stopping the training process before the learner passes that point.</li>\n                <li>Today, this technique is mostly used in deep learning while other techniques (regularization)\n                  are preferred for classical machine learning.</li>\n              </ul>\n              <br />\n\n              <h3>2. L1 and L2 Regularization</h3>\n              We are going to use Melbourne House Price Dataset where we'll predict House Predictions based on\n              various features.\n              <br />\n              <br />\n              <i>housing price prediction where we will see a model overfit when we use simple linear regression.\n                Then we will use Lasso regression (L1 regularization) and ridge regression (L2 regression) to address\n                this overfitting issue.</i>\n              <br />\n              <br />\n              <b>Steps to handling Missing values:</b>\n              <ul>\n                <li>Some feature's missing values can be treated as zero (or NA values).</li>\n                <li>Like 0 for Propertycount, Bedroom2 will refer to other class of NA values.</li>\n                <li>like 0 for Car feature will mean that there's no car parking feature with house.</li>\n              </ul>\n              <br />\n              <br />\n\n              <div style={titles}>\n                <PrismCode\n                  code={cluster}\n                  language=\"js\"\n                  plugins={[\"line-numbers\"]}\n                />\n              </div>\n              <br />\n\n              <h3>3. Let's one hot encode the categorical features</h3>\n              <div style={titles}>\n                <PrismCode\n                  code={cagorical}\n                  language=\"js\"\n                  plugins={[\"line-numbers\"]}\n                />\n              </div>\n              <i>Here training score is 68% but test score is 13.85% which is very low.</i>\n              <br />\n\n              <h3>4. Normal Regression is clearly overfitting the data, let's try other models</h3>\n              Using Lasso (L1 Regularized) Regression Model.\n              <div style={titles}>\n                <PrismCode\n                  code={regularized}\n                  language=\"js\"\n                  plugins={[\"line-numbers\"]}\n                />\n              </div>\n              <i>We see that Lasso and Ridge Regularizations prove to be beneficial when our Simple Linear Regression Model overfits. These results may not be that contrast but significant in most cases.Also that L1 & L2 Regularizations are used in Neural Networks too.</i>\n              <br />\n              <br />\n\n              <h3>5. Ensemble Learning: Bagging</h3>\n              We will use pima indian diabetes dataset to predict if a person has a diabetes or not based on certain features such as blood pressure, skin thickness, age etc. We will train a standalone model first and then use bagging ensemble technique to check how it can improve the performance of the model.\n              <div style={titles}>\n                <PrismCode\n                  code={clustering}\n                  language=\"js\"\n                  plugins={[\"line-numbers\"]}\n                />\n              </div>\n              <br />\n\n              <h3>6. Train using stand alone model</h3>\n              <div style={titles}>\n                <PrismCode\n                  code={standAlone}\n                  language=\"js\"\n                  plugins={[\"line-numbers\"]}\n                />\n              </div>\n              <br />\n\n              <h3>7. Train using Bagging</h3>\n              <div style={titles}>\n                <PrismCode\n                  code={bagging}\n                  language=\"js\"\n                  plugins={[\"line-numbers\"]}\n                />\n              </div>\n              <br />\n\n              <h3>8. We can see some improvement in test score with bagging classifier as compared to a standalone classifier</h3>\n              <b>Train using Random Forest</b>\n              <br />\n              <div style={titles}>\n                <PrismCode\n                  code={improvement}\n                  language=\"js\"\n                  plugins={[\"line-numbers\"]}\n                />\n              </div>\n            </List>\n          </Paper>\n        </Grid>\n      </Grid>\n    )\n  }\n}\n\nexport default (withStyles(styles)(Regularizations));\n"]},"metadata":{},"sourceType":"module"}