{"ast":null,"code":"import _classCallCheck from\"/home/mukeshs/Projects/edurights/node_modules/@babel/runtime/helpers/esm/classCallCheck\";import _createClass from\"/home/mukeshs/Projects/edurights/node_modules/@babel/runtime/helpers/esm/createClass\";import _possibleConstructorReturn from\"/home/mukeshs/Projects/edurights/node_modules/@babel/runtime/helpers/esm/possibleConstructorReturn\";import _getPrototypeOf from\"/home/mukeshs/Projects/edurights/node_modules/@babel/runtime/helpers/esm/getPrototypeOf\";import _inherits from\"/home/mukeshs/Projects/edurights/node_modules/@babel/runtime/helpers/esm/inherits\";import React,{Component}from'react';import Prism from\"prismjs\";import{Grid,Paper,withStyles,List}from\"@material-ui/core\";import'../../ReactJs/styles.css';import Sidebar from'../sidebar';import PrismCode from'../../ReactJs/prismCode';import Activations from'../../../assets/AI/predicted.png';import logsvalues from'../../../assets/AI/logsvalues.PNG';import binarycross from'../../../assets/AI/binarycross.PNG';var titles={backgroundColor:'#F0F8FF',padding:'1px',fontSize:'16px'};var redesign={height:200,width:500};var styles=function styles(theme){return{paper:{margin:theme.spacing(1),padding:theme.spacing(1)},smMargin:{margin:theme.spacing(1)},actionDiv:{textAlign:\"center\"}};};var childsFile=\"\\ny_predicted = np.array([1,1,0,0,1])\\ny_true = np.array([0.30,0.7,1,0,0.5])\\n\\ndef mae(y_predicted, y_true):\\n    total_error = 0\\n    for yp, yt in zip(y_predicted, y_true):\\n        total_error += abs(yp - yt)\\n        \\n    print(\\\"Total error is:\\\",total_error)\\n    mae = total_error/len(y_predicted)\\n    print(\\\"Mean absolute error is:\\\",mae)\\n    return mae\\n    \\nmae(y_predicted, y_true)\\n\".trim();var easier=\"\\nnp.abs(y_predicted-y_true)\\nnp.mean(np.abs(y_predicted-y_true))\\n\\ndef mae_np(y_predicted, y_true):\\n    return np.mean(np.abs(y_predicted-y_true))\\n    \\nmae_np(y_predicted, y_true)\\n\".trim();var entropy=\"\\nnp.log([0])\\nepsilon = 1e-15\\nnp.log([1e-15])\\n\\ny_predicted\\ny_predicted_new = [max(i,epsilon) for i in y_predicted]\\ny_predicted_new\\n\\n1-epsilon\\ny_predicted_new = [min(i,1-epsilon) for i in y_predicted_new]\\ny_predicted_new\\n\\ny_predicted_new = np.array(y_predicted_new)\\nnp.log(y_predicted_new)\\n\\n-np.mean(y_true*np.log(y_predicted_new)+(1-y_true)*np.log(1-y_predicted_new))\\n\\ndef log_loss(y_true, y_predicted):\\n    y_predicted_new = [max(i,epsilon) for i in y_predicted]\\n    y_predicted_new = [min(i,1-epsilon) for i in y_predicted_new]\\n    y_predicted_new = np.array(y_predicted_new)\\n    return -np.mean(y_true*np.log(y_predicted_new)+(1-y_true)*np.log(1-y_predicted_new))\\n    \\nlog_loss(y_true, y_predicted)\\n\".trim();var childsFiles=\"\\nrevenue = np.array([[180,200,220],[24,36,40],[12,18,20]])\\nexpenses = np.array([[80,90,100],[10,16,20],[8,10,10]])\\n\\nprofit=revenue - expenses\\nprofit\\n\".trim();var sales=\"\\nprice_per_unit = np.array([1000,400,1200])\\nunits = np.array([[30,40,50],[5,10,15],[2,5,7]])\\n\\nprice_per_unit * units\\n\\nnp.dot(price_per_unit,units)\\n\".trim();var Loss=/*#__PURE__*/function(_Component){_inherits(Loss,_Component);function Loss(){_classCallCheck(this,Loss);return _possibleConstructorReturn(this,_getPrototypeOf(Loss).apply(this,arguments));}_createClass(Loss,[{key:\"componentDidMount\",value:function componentDidMount(){setTimeout(function(){return Prism.highlightAll();},0);}},{key:\"render\",value:function render(){var classes=this.props.classes;return React.createElement(Grid,{container:true},React.createElement(Grid,{item:true,xs:2},React.createElement(Paper,{className:classes.paper},React.createElement(\"h4\",null,React.createElement(Sidebar,null)))),React.createElement(Grid,{item:true,xs:10},React.createElement(Paper,{className:classes.paper},React.createElement(List,null,React.createElement(\"h3\",null,\"Loss or Cost Function\"),React.createElement(\"b\",null,\"Tensorflow loss value:\"),React.createElement(\"ul\",null,React.createElement(\"li\",null,\"sparse_categorical_cross entropy\"),React.createElement(\"li\",null,\"binary_cross entropy\"),React.createElement(\"li\",null,\"categorical_cross entropy\"),React.createElement(\"li\",null,\"mean_absolute_error\"),React.createElement(\"li\",null,\"mean_squared_error\")),React.createElement(\"br\",null),React.createElement(\"br\",null),React.createElement(\"b\",null,\"Most common loss functions for Machine Learning Regression:\"),React.createElement(\"ul\",null,React.createElement(\"li\",null,React.createElement(\"b\",null,\"Absolute Error(AE): \"),\"Refers to the magnitude of difference between the prediction of an observation and the true value of that observation.\"),React.createElement(\"li\",null,React.createElement(\"b\",null,\"Mean Absolute Error (MEA): \"),\"sum of Absolute Error / Total Number of Observation\"),React.createElement(\"ul\",null,React.createElement(\"li\",null,\"The MEA is an average of the all absolute errors.\")),React.createElement(\"br\",null),React.createElement(\"li\",null,React.createElement(\"b\",null,\"Mean Squared Error (MSE): \"),\"Take the difference between our model\\u2019s predictions and the ground truth, square it, and average it out across the whole dataset.\"),React.createElement(\"ul\",null,React.createElement(\"li\",null,\"=((AE)2 + (AE)2 + ... +(AE)2) / Total Number of Observation\"))),React.createElement(\"br\",null),React.createElement(\"i\",null,\"Implement Mean Absolute Error.\"),React.createElement(\"br\",null),React.createElement(\"div\",{style:titles},React.createElement(PrismCode,{code:childsFile,language:\"js\",plugins:[\"line-numbers\"]})),React.createElement(\"br\",null),React.createElement(\"br\",null),React.createElement(\"b\",null,\"Implement same thing using numpy in much easier way\"),React.createElement(\"br\",null),React.createElement(\"br\",null),React.createElement(\"div\",{style:titles},React.createElement(PrismCode,{code:easier,language:\"js\",plugins:[\"line-numbers\"]})),React.createElement(\"br\",null),React.createElement(\"ul\",null,React.createElement(\"li\",null,React.createElement(\"b\",null,\"Loss Function: \"),\"A function that associates a cost with a decision.\"),React.createElement(\"li\",null,React.createElement(\"b\",null,\"Ex. \"),\"Suppose a person moving from source to destinatios, Which have routes A, B and C with cost 10, 15 and 20 respectively.\"),React.createElement(\"br\",null),React.createElement(\"li\",null,React.createElement(\"b\",null,\"Log Loss/ Binary Cross Entropy: \")),React.createElement(\"li\",null,React.createElement(\"b\",null,\"Entropy: \"),\"Means randomness in our observations.\"),React.createElement(\"li\",null,\"Binary cross entropy compares each of the predicted probabilities to actual class output which can be either 0 or 1. It then calculates the score that penalizes the probabilities based on the distance from the expected value. That means how close or far from the actual value.\"),React.createElement(\"li\",null,React.createElement(\"b\",null,\"Binary Cross Entropy is the negative average of the log of corrected predicted probabilities.\"))),React.createElement(\"br\",null),React.createElement(\"b\",null,\"Predicted Probabilities: \"),\"Output given by the model that tells, the probability object belongs to class 1.\",React.createElement(\"br\",null),React.createElement(\"img\",{src:Activations,alt:\"Theata\",className:\"responsive2\",style:redesign}),React.createElement(\"br\",null),React.createElement(\"br\",null),React.createElement(\"b\",null,\"Corrected Probabilities: \"),\"probability that a particular observation belongs to its original class.  As shown in the above image, ID6 originally belongs to class 1 hence its predicted probability and corrected probability is the same i.e 0.94.\",React.createElement(\"br\",null),React.createElement(\"br\",null),\"On the other hand, the observation ID8  is from class 0. In this case, the predicted probability i.e the chances that ID8 belongs to class 1 is 0.56 whereas, the corrected probability means the chances that ID8 belongs to class 0 is ( 1-predicted_probability) is 0.44. In the same way, corrected probabilities of all the instances will be calculated.\",React.createElement(\"br\",null),React.createElement(\"br\",null),React.createElement(\"b\",null,\"Log(Corrected probabilities): \"),\"Now we will calculate the log value for each of the corrected probabilities. The reason behind using the log value is, the log value offers less penalty for small differences between predicted probability and corrected probability. when the difference is large the penalty will be higher.\",React.createElement(\"br\",null),React.createElement(\"img\",{src:logsvalues,alt:\"Theata\",className:\"responsive2\",style:redesign}),React.createElement(\"br\",null),\"Here we have calculated log values for all the corrected probabilities. Since all the corrected probabilities lie between 0 and 1, all the log values are negative.\",React.createElement(\"br\",null),React.createElement(\"br\",null),\"In order to compensate for this negative value, we will use a negative average of the values.\",React.createElement(\"br\",null),React.createElement(\"img\",{src:binarycross,alt:\"Theata\",className:\"responsive2\",style:redesign}),React.createElement(\"br\",null),\"The value of the negative average of corrected probabilities we calculate 0.214 which is our Log loss/ Binary cross-entropy for this example.\",React.createElement(\"h3\",null,\"Implement Log Loss or Binary Cross Entropy\"),React.createElement(\"div\",{style:titles},React.createElement(PrismCode,{code:entropy,language:\"js\",plugins:[\"line-numbers\"]})),React.createElement(\"br\",null),React.createElement(\"h3\",null),React.createElement(\"div\",{style:titles},React.createElement(PrismCode,{code:childsFile,language:\"js\",plugins:[\"line-numbers\"]})),React.createElement(\"br\",null),React.createElement(\"h3\",null,\"Matrix Math\"),React.createElement(\"b\",null,\"Calculate profit/ loss from revenue and expenses.\"),React.createElement(\"div\",{style:titles},React.createElement(PrismCode,{code:childsFiles,language:\"js\",plugins:[\"line-numbers\"]})),React.createElement(\"br\",null),React.createElement(\"h3\",null,\"Calculate total sales from units and price per unit using matrix multiplication\"),React.createElement(\"div\",{style:titles},React.createElement(PrismCode,{code:sales,language:\"js\",plugins:[\"line-numbers\"]})),React.createElement(\"br\",null),React.createElement(\"i\",null,\"In above case numpy is using broadcasting so it expands price_per_unit array from 1 row, 3 columns to 3 row and 3 columns. Correct way to do matrix multiplication is to use dot product as above.\")))));}}]);return Loss;}(Component);export default withStyles(styles)(Loss);","map":{"version":3,"sources":["/home/mukeshs/Projects/edurights/src/components/angularjs/deepAngularjs/loss.js"],"names":["React","Component","Prism","Grid","Paper","withStyles","List","Sidebar","PrismCode","Activations","logsvalues","binarycross","titles","backgroundColor","padding","fontSize","redesign","height","width","styles","theme","paper","margin","spacing","smMargin","actionDiv","textAlign","childsFile","trim","easier","entropy","childsFiles","sales","Loss","setTimeout","highlightAll","classes","props"],"mappings":"6kBAAA,MAAOA,CAAAA,KAAP,EAAgBC,SAAhB,KAAiC,OAAjC,CACA,MAAOC,CAAAA,KAAP,KAAkB,SAAlB,CACA,OAASC,IAAT,CAAeC,KAAf,CAAsBC,UAAtB,CAAkCC,IAAlC,KAA8C,mBAA9C,CAEA,MAAO,0BAAP,CACA,MAAOC,CAAAA,OAAP,KAAoB,YAApB,CACA,MAAOC,CAAAA,SAAP,KAAsB,yBAAtB,CAEA,MAAOC,CAAAA,WAAP,KAAwB,kCAAxB,CACA,MAAOC,CAAAA,UAAP,KAAuB,mCAAvB,CACA,MAAOC,CAAAA,WAAP,KAAwB,oCAAxB,CAGA,GAAMC,CAAAA,MAAM,CAAG,CAAEC,eAAe,CAAE,SAAnB,CAA8BC,OAAO,CAAE,KAAvC,CAA8CC,QAAQ,CAAE,MAAxD,CAAf,CAEA,GAAMC,CAAAA,QAAQ,CAAG,CACfC,MAAM,CAAE,GADO,CAEfC,KAAK,CAAE,GAFQ,CAAjB,CAKA,GAAMC,CAAAA,MAAM,CAAG,QAATA,CAAAA,MAAS,CAAAC,KAAK,QAAK,CACvBC,KAAK,CAAE,CACLC,MAAM,CAAEF,KAAK,CAACG,OAAN,CAAc,CAAd,CADH,CAELT,OAAO,CAAEM,KAAK,CAACG,OAAN,CAAc,CAAd,CAFJ,CADgB,CAKvBC,QAAQ,CAAE,CACRF,MAAM,CAAEF,KAAK,CAACG,OAAN,CAAc,CAAd,CADA,CALa,CAQvBE,SAAS,CAAE,CACTC,SAAS,CAAE,QADF,CARY,CAAL,EAApB,CAcA,GAAMC,CAAAA,UAAU,CAAG,mZAejBC,IAfiB,EAAnB,CAiBA,GAAMC,CAAAA,MAAM,CAAG,6LAQbD,IARa,EAAf,CAUA,GAAME,CAAAA,OAAO,CAAG,ytBAyBdF,IAzBc,EAAhB,CA2BA,GAAMG,CAAAA,WAAW,CAAG,8JAMlBH,IANkB,EAApB,CAQA,GAAMI,CAAAA,KAAK,CAAG,6JAOZJ,IAPY,EAAd,C,GAUMK,CAAAA,I,4QACgB,CAClBC,UAAU,CAAC,iBAAMhC,CAAAA,KAAK,CAACiC,YAAN,EAAN,EAAD,CAA6B,CAA7B,CAAV,CACD,C,uCACQ,IACCC,CAAAA,OADD,CACa,KAAKC,KADlB,CACCD,OADD,CAEP,MACE,qBAAC,IAAD,EAAM,SAAS,KAAf,EACE,oBAAC,IAAD,EAAM,IAAI,KAAV,CAAW,EAAE,CAAE,CAAf,EACE,oBAAC,KAAD,EAAO,SAAS,CAAEA,OAAO,CAACf,KAA1B,EACE,8BAAI,oBAAC,OAAD,MAAJ,CADF,CADF,CADF,CAME,oBAAC,IAAD,EAAM,IAAI,KAAV,CAAW,EAAE,CAAE,EAAf,EACE,oBAAC,KAAD,EAAO,SAAS,CAAEe,OAAO,CAACf,KAA1B,EACE,oBAAC,IAAD,MACE,sDADF,CAEE,sDAFF,CAGE,8BACE,iEADF,CAEE,qDAFF,CAGE,0DAHF,CAIE,oDAJF,CAKE,mDALF,CAHF,CAUE,8BAVF,CAWE,8BAXF,CAaE,2FAbF,CAcE,8BACE,8BAAI,oDAAJ,0HADF,CAEE,8BAAI,2DAAJ,uDAFF,CAGE,8BAAI,kFAAJ,CAHF,CAIE,8BAJF,CAKE,8BAAI,0DAAJ,0IALF,CAOE,8BAAI,4FAAJ,CAPF,CAdF,CAuBE,8BAvBF,CAyBE,8DAzBF,CA0BE,8BA1BF,CA4BE,2BAAK,KAAK,CAAET,MAAZ,EACE,oBAAC,SAAD,EACE,IAAI,CAAEe,UADR,CAEE,QAAQ,CAAC,IAFX,CAGE,OAAO,CAAE,CAAC,cAAD,CAHX,EADF,CA5BF,CAmCE,8BAnCF,CAoCE,8BApCF,CAsCE,mFAtCF,CAuCE,8BAvCF,CAwCE,8BAxCF,CAyCE,2BAAK,KAAK,CAAEf,MAAZ,EACE,oBAAC,SAAD,EACE,IAAI,CAAEiB,MADR,CAEE,QAAQ,CAAC,IAFX,CAGE,OAAO,CAAE,CAAC,cAAD,CAHX,EADF,CAzCF,CAgDE,8BAhDF,CAiDE,8BACE,8BAAI,+CAAJ,sDADF,CAEE,8BAAI,oCAAJ,0HAFF,CAIE,8BAJF,CAKE,8BAAI,gEAAJ,CALF,CAME,8BAAI,yCAAJ,yCANF,CAOE,qTAPF,CAUE,8BAAI,6HAAJ,CAVF,CAjDF,CA6DE,8BA7DF,CA8DE,yDA9DF,oFA+DE,8BA/DF,CAgEE,2BAAK,GAAG,CAAEpB,WAAV,CAAuB,GAAG,CAAC,QAA3B,CAAoC,SAAS,CAAC,aAA9C,CAA4D,KAAK,CAAEO,QAAnE,EAhEF,CAiEE,8BAjEF,CAkEE,8BAlEF,CAmEE,yDAnEF,4NAqEE,8BArEF,CAsEE,8BAtEF,kWA0EE,8BA1EF,CA2EE,8BA3EF,CA4EE,8DA5EF,oSAgFE,8BAhFF,CAiFE,2BAAK,GAAG,CAAEN,UAAV,CAAsB,GAAG,CAAC,QAA1B,CAAmC,SAAS,CAAC,aAA7C,CAA2D,KAAK,CAAEM,QAAlE,EAjFF,CAkFE,8BAlFF,uKAqFE,8BArFF,CAsFE,8BAtFF,iGAwFE,8BAxFF,CAyFE,2BAAK,GAAG,CAAEL,WAAV,CAAuB,GAAG,CAAC,QAA3B,CAAoC,SAAS,CAAC,aAA9C,CAA4D,KAAK,CAAEK,QAAnE,EAzFF,CA0FE,8BA1FF,iJA8FE,2EA9FF,CA+FE,2BAAK,KAAK,CAAEJ,MAAZ,EACE,oBAAC,SAAD,EACE,IAAI,CAAEkB,OADR,CAEE,QAAQ,CAAC,IAFX,CAGE,OAAO,CAAE,CAAC,cAAD,CAHX,EADF,CA/FF,CAsGE,8BAtGF,CAwGE,8BAxGF,CAyGE,2BAAK,KAAK,CAAElB,MAAZ,EACE,oBAAC,SAAD,EACE,IAAI,CAAEe,UADR,CAEE,QAAQ,CAAC,IAFX,CAGE,OAAO,CAAE,CAAC,cAAD,CAHX,EADF,CAzGF,CAgHE,8BAhHF,CAkHE,4CAlHF,CAmHE,iFAnHF,CAoHE,2BAAK,KAAK,CAAEf,MAAZ,EACE,oBAAC,SAAD,EACE,IAAI,CAAEmB,WADR,CAEE,QAAQ,CAAC,IAFX,CAGE,OAAO,CAAE,CAAC,cAAD,CAHX,EADF,CApHF,CA2HE,8BA3HF,CA6HE,gHA7HF,CA8HE,2BAAK,KAAK,CAAEnB,MAAZ,EACE,oBAAC,SAAD,EACE,IAAI,CAAEoB,KADR,CAEE,QAAQ,CAAC,IAFX,CAGE,OAAO,CAAE,CAAC,cAAD,CAHX,EADF,CA9HF,CAqIE,8BArIF,CAsIE,kOAtIF,CADF,CADF,CANF,CADF,CAsJD,C,kBA5JgB/B,S,EAgKnB,cAAgBI,CAAAA,UAAU,CAACc,MAAD,CAAV,CAAmBc,IAAnB,CAAhB","sourcesContent":["import React, { Component } from 'react';\nimport Prism from \"prismjs\"\nimport { Grid, Paper, withStyles, List } from \"@material-ui/core\";\n\nimport '../../ReactJs/styles.css'\nimport Sidebar from '../sidebar';\nimport PrismCode from '../../ReactJs/prismCode';\n\nimport Activations from '../../../assets/AI/predicted.png'\nimport logsvalues from '../../../assets/AI/logsvalues.PNG'\nimport binarycross from '../../../assets/AI/binarycross.PNG'\n\n\nconst titles = { backgroundColor: '#F0F8FF', padding: '1px', fontSize: '16px' }\n\nconst redesign = {\n  height: 200,\n  width: 500\n}\n\nconst styles = theme => ({\n  paper: {\n    margin: theme.spacing(1),\n    padding: theme.spacing(1)\n  },\n  smMargin: {\n    margin: theme.spacing(1)\n  },\n  actionDiv: {\n    textAlign: \"center\"\n  }\n})\n\n\nconst childsFile = `\ny_predicted = np.array([1,1,0,0,1])\ny_true = np.array([0.30,0.7,1,0,0.5])\n\ndef mae(y_predicted, y_true):\n    total_error = 0\n    for yp, yt in zip(y_predicted, y_true):\n        total_error += abs(yp - yt)\n        \n    print(\"Total error is:\",total_error)\n    mae = total_error/len(y_predicted)\n    print(\"Mean absolute error is:\",mae)\n    return mae\n    \nmae(y_predicted, y_true)\n`.trim();\n\nconst easier = `\nnp.abs(y_predicted-y_true)\nnp.mean(np.abs(y_predicted-y_true))\n\ndef mae_np(y_predicted, y_true):\n    return np.mean(np.abs(y_predicted-y_true))\n    \nmae_np(y_predicted, y_true)\n`.trim();\n\nconst entropy = `\nnp.log([0])\nepsilon = 1e-15\nnp.log([1e-15])\n\ny_predicted\ny_predicted_new = [max(i,epsilon) for i in y_predicted]\ny_predicted_new\n\n1-epsilon\ny_predicted_new = [min(i,1-epsilon) for i in y_predicted_new]\ny_predicted_new\n\ny_predicted_new = np.array(y_predicted_new)\nnp.log(y_predicted_new)\n\n-np.mean(y_true*np.log(y_predicted_new)+(1-y_true)*np.log(1-y_predicted_new))\n\ndef log_loss(y_true, y_predicted):\n    y_predicted_new = [max(i,epsilon) for i in y_predicted]\n    y_predicted_new = [min(i,1-epsilon) for i in y_predicted_new]\n    y_predicted_new = np.array(y_predicted_new)\n    return -np.mean(y_true*np.log(y_predicted_new)+(1-y_true)*np.log(1-y_predicted_new))\n    \nlog_loss(y_true, y_predicted)\n`.trim();\n\nconst childsFiles = `\nrevenue = np.array([[180,200,220],[24,36,40],[12,18,20]])\nexpenses = np.array([[80,90,100],[10,16,20],[8,10,10]])\n\nprofit=revenue - expenses\nprofit\n`.trim();\n\nconst sales = `\nprice_per_unit = np.array([1000,400,1200])\nunits = np.array([[30,40,50],[5,10,15],[2,5,7]])\n\nprice_per_unit * units\n\nnp.dot(price_per_unit,units)\n`.trim();\n\n\nclass Loss extends Component {\n  componentDidMount() {\n    setTimeout(() => Prism.highlightAll(), 0)\n  }\n  render() {\n    const { classes } = this.props;\n    return (\n      <Grid container>\n        <Grid item xs={2}>\n          <Paper className={classes.paper}>\n            <h4><Sidebar /></h4>\n          </Paper>\n        </Grid>\n        <Grid item xs={10}>\n          <Paper className={classes.paper}>\n            <List>\n              <h3>Loss or Cost Function</h3>\n              <b>Tensorflow loss value:</b>\n              <ul>\n                <li>sparse_categorical_cross entropy</li>\n                <li>binary_cross entropy</li>\n                <li>categorical_cross entropy</li>\n                <li>mean_absolute_error</li>\n                <li>mean_squared_error</li>\n              </ul>\n              <br />\n              <br />\n\n              <b>Most common loss functions for Machine Learning Regression:</b>\n              <ul>\n                <li><b>Absolute Error(AE): </b>Refers to the magnitude of difference between the prediction of an observation and the true value of that observation.</li>\n                <li><b>Mean Absolute Error (MEA): </b>sum of Absolute Error / Total Number of Observation</li>\n                <ul><li>The MEA is an average of the all absolute errors.</li></ul>\n                <br />\n                <li><b>Mean Squared Error (MSE): </b>Take the difference between our model’s predictions and the ground truth, square it, and\n                  average it out across the whole dataset.</li>\n                <ul><li>=((AE)2 + (AE)2 + ... +(AE)2) / Total Number of Observation</li></ul>\n              </ul>\n              <br />\n\n              <i>Implement Mean Absolute Error.</i>\n              <br />\n\n              <div style={titles}>\n                <PrismCode\n                  code={childsFile}\n                  language=\"js\"\n                  plugins={[\"line-numbers\"]}\n                />\n              </div>\n              <br />\n              <br />\n\n              <b>Implement same thing using numpy in much easier way</b>\n              <br />\n              <br />\n              <div style={titles}>\n                <PrismCode\n                  code={easier}\n                  language=\"js\"\n                  plugins={[\"line-numbers\"]}\n                />\n              </div>\n              <br />\n              <ul>\n                <li><b>Loss Function: </b>A function that associates a cost with a decision.</li>\n                <li><b>Ex. </b>Suppose a person moving from source to destinatios, Which have routes A, B and C with cost 10, 15 and 20\n                  respectively.</li>\n                <br />\n                <li><b>Log Loss/ Binary Cross Entropy: </b></li>\n                <li><b>Entropy: </b>Means randomness in our observations.</li>\n                <li>Binary cross entropy compares each of the predicted probabilities to actual class output which can be either 0 or 1. It\n                  then calculates the score that penalizes the probabilities based on the distance from the expected value. That means how\n                  close or far from the actual value.</li>\n                <li><b>Binary Cross Entropy is the negative average of the log of corrected predicted probabilities.</b></li>\n              </ul>\n              <br />\n              <b>Predicted Probabilities: </b>Output given by the model that tells, the probability object belongs to class 1.\n              <br />\n              <img src={Activations} alt=\"Theata\" className=\"responsive2\" style={redesign} />\n              <br />\n              <br />\n              <b>Corrected Probabilities: </b>probability that a particular observation belongs to its original class.  As shown in the above image, ID6 originally\n              belongs to class 1 hence its predicted probability and corrected probability is the same i.e 0.94.\n              <br />\n              <br />\n              On the other hand, the observation ID8  is from class 0. In this case, the predicted probability i.e the chances that ID8 belongs to class 1\n              is 0.56 whereas, the corrected probability means the chances that ID8 belongs to class 0 is ( 1-predicted_probability) is 0.44. In the same\n              way, corrected probabilities of all the instances will be calculated.\n              <br />\n              <br />\n              <b>Log(Corrected probabilities): </b>\n              Now we will calculate the log value for each of the corrected probabilities. The reason behind using the log value is, the log value offers\n              less penalty for small differences between predicted probability and corrected probability. when the difference is large the penalty will\n              be higher.\n              <br />\n              <img src={logsvalues} alt=\"Theata\" className=\"responsive2\" style={redesign} />\n              <br />\n              Here we have calculated log values for all the corrected probabilities. Since all the corrected probabilities lie between 0 and 1, all the\n              log values are negative.\n              <br />\n              <br />\n              In order to compensate for this negative value, we will use a negative average of the values.\n              <br />\n              <img src={binarycross} alt=\"Theata\" className=\"responsive2\" style={redesign} />\n              <br />\n              The value of the negative average of corrected probabilities we calculate 0.214 which is our Log loss/ Binary cross-entropy for this example.\n\n\n              <h3>Implement Log Loss or Binary Cross Entropy</h3>\n              <div style={titles}>\n                <PrismCode\n                  code={entropy}\n                  language=\"js\"\n                  plugins={[\"line-numbers\"]}\n                />\n              </div>\n              <br />\n\n              <h3></h3>\n              <div style={titles}>\n                <PrismCode\n                  code={childsFile}\n                  language=\"js\"\n                  plugins={[\"line-numbers\"]}\n                />\n              </div>\n              <br />\n\n              <h3>Matrix Math</h3>\n              <b>Calculate profit/ loss from revenue and expenses.</b>\n              <div style={titles}>\n                <PrismCode\n                  code={childsFiles}\n                  language=\"js\"\n                  plugins={[\"line-numbers\"]}\n                />\n              </div>\n              <br />\n\n              <h3>Calculate total sales from units and price per unit using matrix multiplication</h3>\n              <div style={titles}>\n                <PrismCode\n                  code={sales}\n                  language=\"js\"\n                  plugins={[\"line-numbers\"]}\n                />\n              </div>\n              <br />\n              <i>In above case numpy is using broadcasting so it expands price_per_unit array from 1 row, 3 columns to\n                3 row and 3 columns. Correct way to do matrix multiplication is to use dot product as above.</i>\n            </List>\n          </Paper>\n        </Grid>\n      </Grid>\n    )\n  }\n}\n\n\nexport default (withStyles(styles)(Loss));\n"]},"metadata":{},"sourceType":"module"}