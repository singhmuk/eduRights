{"ast":null,"code":"import _classCallCheck from\"/home/mukeshs/Projects/edurights/node_modules/@babel/runtime/helpers/esm/classCallCheck\";import _createClass from\"/home/mukeshs/Projects/edurights/node_modules/@babel/runtime/helpers/esm/createClass\";import _possibleConstructorReturn from\"/home/mukeshs/Projects/edurights/node_modules/@babel/runtime/helpers/esm/possibleConstructorReturn\";import _getPrototypeOf from\"/home/mukeshs/Projects/edurights/node_modules/@babel/runtime/helpers/esm/getPrototypeOf\";import _inherits from\"/home/mukeshs/Projects/edurights/node_modules/@babel/runtime/helpers/esm/inherits\";import React,{Component}from'react';import Prism from\"prismjs\";import{Grid,Paper,withStyles,List}from\"@material-ui/core\";import'../../ReactJs/styles.css';import Sidebar from'../sidebar';import PrismCode from'../../ReactJs/prismCode';import Logistic from'../../../assets/ML/navBayers.png';var titles={backgroundColor:'#F0F8FF',padding:'1px',fontSize:'16px'};var styles=function styles(theme){return{paper:{margin:theme.spacing(1),padding:theme.spacing(1)},smMargin:{margin:theme.spacing(1)},actionDiv:{textAlign:\"center\"}};};var predicting=\"\\nimport pandas as pd\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.naive_bayes import GaussianNB\\n\\ndf = pd.read_csv(\\\"titanic.csv\\\")\\n\\ndf.drop(['PassengerId','Name','SibSp','Parch','Ticket','Cabin','Embarked'],axis='columns',inplace=True)\\ninputs = df.drop('Survived',axis='columns')\\ntarget = df.Survived\\n\\n#inputs.Sex = inputs.Sex.map({'male': 1, 'female': 2})\\ndummies = pd.get_dummies(inputs.Sex)\\ninputs = pd.concat([inputs,dummies],axis='columns')\\n                \\ninputs.drop(['Sex','male'],axis='columns',inplace=True)                 #Dropping male column of dummy variable trap theory.\\n\\ninputs.columns[inputs.isna().any()]\\ninputs.Age[:10]\\ninputs.Age = inputs.Age.fillna(inputs.Age.mean())\\n\\nX_train, X_test, y_train, y_test = train_test_split(inputs,target,test_size=0.3)\\n\\nmodel = GaussianNB()\\nmodel.fit(X_train,y_train)\\n\\nmodel.score(X_test,y_test)\\nX_test[0:10]\\ny_test[0:10]\\n\\nmodel.predict(X_test[0:10])\\nmodel.predict_proba(X_test[:10])\\n\\nfrom sklearn.model_selection import cross_val_score\\ncross_val_score(GaussianNB(),X_train, y_train, cv=5)                  #Calculate the score using cross validation\\n\".trim();var naiveBease=\"\\nimport pandas as pd\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.feature_extraction.text import CountVectorizer\\nfrom sklearn.naive_bayes import MultinomialNB\\nfrom sklearn.pipeline import Pipeline\\n\\ndf = pd.read_csv(\\\"spam.csv\\\")\\ndf.groupby('Category').describe()\\ndf['spam']=df['Category'].apply(lambda x: 1 if x=='spam' else 0)\\n\\nX_train, X_test, y_train, y_test = train_test_split(df.Message,df.spam)\\n\\nv = CountVectorizer()\\nX_train_count = v.fit_transform(X_train.values)\\nX_train_count.toarray()[:2]\\n\\nmodel = MultinomialNB()\\nmodel.fit(X_train_count,y_train)\\n\\nemails = ['Hey mohan, can we get?','Upto Dont miss this reward!']\\nemails_count = v.transform(emails)\\nmodel.predict(emails_count)\\n\\nX_test_count = v.transform(X_test)\\nmodel.score(X_test_count, y_test)\\n\\nclf = Pipeline([('vectorizer', CountVectorizer()),('nb', MultinomialNB())])\\n\\nclf.fit(X_train, y_train)\\nclf.score(X_test,y_test)\\nclf.predict(emails)\\n\".trim();var NaiveBrs=/*#__PURE__*/function(_Component){_inherits(NaiveBrs,_Component);function NaiveBrs(){_classCallCheck(this,NaiveBrs);return _possibleConstructorReturn(this,_getPrototypeOf(NaiveBrs).apply(this,arguments));}_createClass(NaiveBrs,[{key:\"componentDidMount\",value:function componentDidMount(){setTimeout(function(){return Prism.highlightAll();},0);}},{key:\"render\",value:function render(){var classes=this.props.classes;return React.createElement(Grid,{container:true},React.createElement(Grid,{item:true,xs:2},React.createElement(Paper,{className:classes.paper},React.createElement(\"h4\",null,React.createElement(Sidebar,null)))),React.createElement(Grid,{item:true,xs:10},React.createElement(Paper,{className:classes.paper},React.createElement(List,null,React.createElement(\"h3\",null,\"Na\\xEFve Bayes (lassification technique based)\"),\"Na\\xEFve Bayes algorithms  applying Bayes\\u2019 theorem with a strong assumption that all the predictors are independent to each other.\",React.createElement(\"br\",null),React.createElement(\"b\",null,\"Ex.\"),\" a phone may be considered as smart if it is having touch screen, internet facility, good camera etc. Though all these features are dependent on each other, they contribute independently to the probability of that the phone is a smart phone.\",React.createElement(\"br\",null),React.createElement(\"br\",null),React.createElement(\"i\",null,\"In Bayesian classification, the main interest is find the posterior probabilities i.e.\"),React.createElement(\"br\",null),\"the probability of a label given some observed features, \\uD835\\uDC43(\\uD835\\uDC3F | \\uD835\\uDC53\\uD835\\uDC52\\uD835\\uDC4E\\uD835\\uDC61\\uD835\\uDC62\\uD835\\uDC5F\\uD835\\uDC52\\uD835\\uDC60). With the help of Bayes theorem, we can express this in quantitative form as follows \\u2212\",React.createElement(\"br\",null),React.createElement(\"b\",null,\"P(L|features)=P(L)P(features|L)P(features)\"),React.createElement(\"br\",null),React.createElement(\"br\",null),\"Here, (\\uD835\\uDC3F | \\uD835\\uDC53\\uD835\\uDC52\\uD835\\uDC4E\\uD835\\uDC61\\uD835\\uDC62\\uD835\\uDC5F\\uD835\\uDC52\\uD835\\uDC60) is the posterior probability of class.\",React.createElement(\"br\",null),\"\\uD835\\uDC43(\\uD835\\uDC3F) is the prior probability of class.\",React.createElement(\"br\",null),\"\\uD835\\uDC43(\\uD835\\uDC53\\uD835\\uDC52\\uD835\\uDC4E\\uD835\\uDC61\\uD835\\uDC62\\uD835\\uDC5F\\uD835\\uDC52\\uD835\\uDC60|\\uD835\\uDC3F) is the likelihood which is the probability of predictor given class.\",React.createElement(\"br\",null),\"\\uD835\\uDC43(\\uD835\\uDC53\\uD835\\uDC52\\uD835\\uDC4E\\uD835\\uDC61\\uD835\\uDC62\\uD835\\uDC5F\\uD835\\uDC52\\uD835\\uDC60) is the prior probability of predictor.\",React.createElement(\"br\",null),React.createElement(\"br\",null),\"Building model using Na\\xEFve Bayes in Python library, Scikit learn. We have the following three types of Na\\xEFve Bayes model under Scikit learn Python library \\u2212\",React.createElement(\"br\",null),React.createElement(\"h3\",null,\"Gaussian Na\\xEFve Bayes\"),\"It is the simplest Na\\xEFve Bayes classifier having the assumption that the data from each label is drawn from a simple Gaussian distribution.\",React.createElement(\"br\",null),React.createElement(\"br\",null),\"Multinomial Na\\xEFve Bayes Another useful Na\\xEFve Bayes classifier in which the features are assumed to be drawn from a simple Multinomial distribution. Such kind of Na\\xEFve Bayes are most appropriate for the features that represents discrete counts.\",React.createElement(\"br\",null),React.createElement(\"br\",null),\"Bernoulli Na\\xEFve Bayes Another important model in which features are assumed to be binary (0 and 1). Text classification with \\u2018bag of words\\u2019 model can be an application of Bernoulli Na\\xEFve Bayes.\",React.createElement(\"br\",null),React.createElement(\"br\",null),React.createElement(\"b\",null,\"Pros: \"),React.createElement(\"ul\",null,React.createElement(\"li\",null,\"Na\\xEFve Bayes classification is easy to implement and fast.\"),React.createElement(\"li\",null,\"It will converge faster than discriminative models like logistic regression.\"),React.createElement(\"li\",null,\"It requires less training data.\"),React.createElement(\"li\",null,\"It is highly scalable in nature, or they scale linearly with the number of predictors and data points.\"),React.createElement(\"li\",null,\"It can make probabilistic predictions and can handle continuous as well as discrete data. Na\\xEFve Bayes classification algorithm can be used for binary as well as multi-class classification problems both.\")),React.createElement(\"br\",null),React.createElement(\"b\",null,\"Cons: \"),\"Its strong feature independence because in real life it is almost impossible to have a set of features which are completely independent of each other. Its \\u2018zero frequency\\u2019 which means that if a categorial variable has a category but not being observed in training data set, then Na\\xEFve Bayes model will assign a zero probability to it and it will be unable to make a prediction.\",React.createElement(\"br\",null),React.createElement(\"br\",null),React.createElement(\"b\",null,\"Applications: \"),React.createElement(\"ul\",null,React.createElement(\"li\",null,React.createElement(\"b\",null,\"Real-time prediction: \"),\"Due to its ease of implementation and fast computation, it can be used to do prediction in real-time.\"),React.createElement(\"li\",null,React.createElement(\"b\",null,\"Multi-class prediction: \"),\"It can be used to predict posterior probability of multiple classes of target variable.\"),React.createElement(\"li\",null,React.createElement(\"b\",null,\"Text classification: \"),\"Due to the feature of multi-class prediction, Na\\xEFve Bayes classification algorithms are well suited for text classification. That is why it is also used to solve problems like spam-filtering and sentiment analysis.\"),React.createElement(\"li\",null,React.createElement(\"b\",null,\"Recommendation system: \"),\"Along with the algorithms like collaborative filtering, Na\\xEFve Bayes makes a Recommendation system which can be used to filter unseen information and to predict weather a user would like the given resource or not.\")),React.createElement(\"br\",null),React.createElement(\"h3\",null,\"Na\\xEFve Bayes Classifier Algorithm (supervised)\"),\"The Na\\xEFve Bayes algorithm is comprised of two words Na\\xEFve and Bayes.\",React.createElement(\"ul\",null,React.createElement(\"li\",null,\"It is mainly used in text classification that includes a high-dimensional training dataset.\"),React.createElement(\"li\",null,\"It is one of the simple and most effective Classification algorithms which helps in building the fast machine learning models that can make quick predictions.\"),React.createElement(\"li\",null,\"It is a probabilistic classifier, which means it predicts on the basis of the probability of an object.\"),React.createElement(\"b\",null,\"Ex.\"),\" Spam filtration, Sentimental analysis, and classifying articles.\",React.createElement(\"li\",null,React.createElement(\"b\",null,\"Na\\xEFve: \"),\"It is called Na\\xEFve because it assumes that the occurrence of a certain feature is independent of the occurrence of other features. Such as if the fruit is identified on the bases of color, shape, and taste, then red, spherical, and sweet fruit is recognized as an apple. Hence each feature individually contributes to identify that it is an apple without depending on each other.\"),React.createElement(\"li\",null,React.createElement(\"b\",null,\"Bayes: \"),\"It is called Bayes because it depends on the principle of Bayes' Theorem.\")),React.createElement(\"br\",null),React.createElement(\"b\",null,\"Bayes' Theorem:\"),React.createElement(\"br\",null),React.createElement(\"img\",{src:Logistic,alt:\"Equations\",className:\"responsive\",style:{width:300,height:50}}),React.createElement(\"br\",null),React.createElement(\"br\",null),\"Where,\",React.createElement(\"br\",null),React.createElement(\"b\",null,\"P(A|B) is Posterior probability: \"),\"Probability of hypothesis A on the observed event B.\",React.createElement(\"br\",null),React.createElement(\"b\",null,\"P(B|A) is Likelihood probability: \"),\"Probability of the evidence given that the probability of a hypothesis is true.\",React.createElement(\"br\",null),React.createElement(\"b\",null,\"P(A) is Prior Probability: \"),\"Probability of hypothesis before observing the evidence.\",React.createElement(\"br\",null),React.createElement(\"b\",null,\"P(B) is Marginal Probability: \"),\"Probability of Evidence.\",React.createElement(\"br\",null),React.createElement(\"b\",null,\"Advantages:\"),React.createElement(\"ul\",null,React.createElement(\"li\",null,\"It is one of the fast and easy ML algorithms to predict a class of datasets.\"),React.createElement(\"li\",null,\"It can be used for Binary as well as Multi-class Classifications.\"),React.createElement(\"li\",null,\"It performs well in Multi-class predictions as compared to the other Algorithms.\"),React.createElement(\"li\",null,\"It is the most popular choice for text classification problems.\")),React.createElement(\"br\",null),React.createElement(\"b\",null,\"Disadvantages:\"),\"Naive Bayes assumes that all features are independent or unrelated, so it cannot learn the relationship between features.\",React.createElement(\"br\",null),React.createElement(\"b\",null,\"Applications:\"),React.createElement(\"ul\",null,React.createElement(\"li\",null,\"It is used for Credit Scoring.\"),React.createElement(\"li\",null,\"It is used in medical data classification.\"),React.createElement(\"li\",null,\"It can be used in real-time predictions because Na\\xEFve Bayes Classifier is an eager learner.\"),React.createElement(\"li\",null,\"It is used in Text classification such as Spam filtering and Sentiment analysis.\")),React.createElement(\"br\",null),React.createElement(\"b\",null,\"Types of Na\\xEFve Bayes Model:\"),React.createElement(\"ul\",null,React.createElement(\"li\",null,React.createElement(\"b\",null,\"Gaussian :\"),\"The Gaussian model assumes that features follow a normal distribution. This means if predictors take continuous values instead of discrete, then the model assumes that these values are sampled from the Gaussian distribution.\"),React.createElement(\"li\",null,React.createElement(\"b\",null,\"Multinomial :\"),\"Used when the data is multinomial distributed. It is primarily used for document classification problems, it means a particular document belongs to which category such as Sports, Politics, education, etc.\",React.createElement(\"br\",null),\"The classifier uses the frequency of words for the predictors.\"),React.createElement(\"li\",null,React.createElement(\"b\",null,\"Bernoulli :\"),\"The Bernoulli classifier works similar to the Multinomial classifier, but the predictor variables are the independent Booleans variables. Such as if a particular word is present or not in a document. This model is also famous for document classification tasks.\")),React.createElement(\"br\",null),React.createElement(\"h3\",null,\"Predicting survival from titanic crash\"),React.createElement(\"div\",{style:titles},React.createElement(PrismCode,{code:predicting,language:\"js\",plugins:[\"line-numbers\"]})),React.createElement(\"br\",null),React.createElement(\"h3\",null,\"Naive Bayes 2\"),React.createElement(\"div\",{style:titles},React.createElement(PrismCode,{code:naiveBease,language:\"js\",plugins:[\"line-numbers\"]}))))));}}]);return NaiveBrs;}(Component);export default withStyles(styles)(NaiveBrs);","map":{"version":3,"sources":["/home/mukeshs/Projects/edurights/src/components/ml/deepMl/naiveBar.js"],"names":["React","Component","Prism","Grid","Paper","withStyles","List","Sidebar","PrismCode","Logistic","titles","backgroundColor","padding","fontSize","styles","theme","paper","margin","spacing","smMargin","actionDiv","textAlign","predicting","trim","naiveBease","NaiveBrs","setTimeout","highlightAll","classes","props","width","height"],"mappings":"6kBAAA,MAAOA,CAAAA,KAAP,EAAgBC,SAAhB,KAAiC,OAAjC,CACA,MAAOC,CAAAA,KAAP,KAAkB,SAAlB,CACA,OAASC,IAAT,CAAeC,KAAf,CAAsBC,UAAtB,CAAkCC,IAAlC,KAA8C,mBAA9C,CAEA,MAAO,0BAAP,CACA,MAAOC,CAAAA,OAAP,KAAoB,YAApB,CACA,MAAOC,CAAAA,SAAP,KAAsB,yBAAtB,CAEA,MAAOC,CAAAA,QAAP,KAAqB,kCAArB,CAGA,GAAMC,CAAAA,MAAM,CAAG,CAAEC,eAAe,CAAE,SAAnB,CAA8BC,OAAO,CAAE,KAAvC,CAA8CC,QAAQ,CAAE,MAAxD,CAAf,CAEA,GAAMC,CAAAA,MAAM,CAAG,QAATA,CAAAA,MAAS,CAAAC,KAAK,QAAK,CACvBC,KAAK,CAAE,CACLC,MAAM,CAAEF,KAAK,CAACG,OAAN,CAAc,CAAd,CADH,CAELN,OAAO,CAAEG,KAAK,CAACG,OAAN,CAAc,CAAd,CAFJ,CADgB,CAKvBC,QAAQ,CAAE,CACRF,MAAM,CAAEF,KAAK,CAACG,OAAN,CAAc,CAAd,CADA,CALa,CAQvBE,SAAS,CAAE,CACTC,SAAS,CAAE,QADF,CARY,CAAL,EAApB,CAaA,GAAMC,CAAAA,UAAU,CAAG,4oCAmCjBC,IAnCiB,EAAnB,CAqCA,GAAMC,CAAAA,UAAU,CAAG,m8BAgCjBD,IAhCiB,EAAnB,C,GAmCME,CAAAA,Q,gSACgB,CAClBC,UAAU,CAAC,iBAAMxB,CAAAA,KAAK,CAACyB,YAAN,EAAN,EAAD,CAA6B,CAA7B,CAAV,CACD,C,uCACQ,IACCC,CAAAA,OADD,CACa,KAAKC,KADlB,CACCD,OADD,CAEP,MACE,qBAAC,IAAD,EAAM,SAAS,KAAf,EACE,oBAAC,IAAD,EAAM,IAAI,KAAV,CAAW,EAAE,CAAE,CAAf,EACE,oBAAC,KAAD,EAAO,SAAS,CAAEA,OAAO,CAACZ,KAA1B,EACE,8BAAI,oBAAC,OAAD,MAAJ,CADF,CADF,CADF,CAME,oBAAC,IAAD,EAAM,IAAI,KAAV,CAAW,EAAE,CAAE,EAAf,EACE,oBAAC,KAAD,EAAO,SAAS,CAAEY,OAAO,CAACZ,KAA1B,EACE,oBAAC,IAAD,MACE,+EADF,2IAIE,8BAJF,CAKE,mCALF,qPAQE,8BARF,CASE,8BATF,CAUE,sHAVF,CAWE,8BAXF,sRAcE,8BAdF,CAeE,0EAfF,CAgBE,8BAhBF,CAiBE,8BAjBF,kKAkBsE,8BAlBtE,iEAmB2C,8BAnB3C,oMAqBE,8BArBF,yJAuBE,8BAvBF,CAwBE,8BAxBF,2KA2BE,8BA3BF,CA6BE,wDA7BF,kJAgCE,8BAhCF,CAiCE,8BAjCF,gQAqCE,8BArCF,CAsCE,8BAtCF,qNAyCE,8BAzCF,CA0CE,8BA1CF,CA2CE,sCA3CF,CA4CE,8BACE,6FADF,CAEE,6GAFF,CAGE,gEAHF,CAIE,uIAJF,CAKE,8OALF,CA5CF,CAoDE,8BApDF,CAqDE,sCArDF,0YA0DE,8BA1DF,CA2DE,8BA3DF,CA4DE,8CA5DF,CA6DE,8BACE,8BAAI,sDAAJ,yGADF,CAGE,8BAAI,wDAAJ,2FAHF,CAIE,8BAAI,qDAAJ,6NAJF,CAOE,8BAAI,uDAAJ,2NAPF,CA7DF,CAwEE,8BAxEF,CA0EE,iFA1EF,8EA4EE,8BACE,4HADF,CAEE,+LAFF,CAIE,wIAJF,CAKE,mCALF,qEAME,8BAAI,0CAAJ,kYANF,CAUE,8BAAI,uCAAJ,6EAVF,CA5EF,CAwFE,8BAxFF,CAyFE,+CAzFF,CA0FE,8BA1FF,CA2FE,2BAAK,GAAG,CAAEP,QAAV,CAAoB,GAAG,CAAC,WAAxB,CAAoC,SAAS,CAAC,YAA9C,CAA2D,KAAK,CAAE,CAAEqB,KAAK,CAAE,GAAT,CAAcC,MAAM,CAAE,EAAtB,CAAlE,EA3FF,CA4FE,8BA5FF,CA6FE,8BA7FF,UA+FE,8BA/FF,CAgGE,iEAhGF,wDAiGE,8BAjGF,CAkGE,kEAlGF,mFAmGE,8BAnGF,CAoGE,2DApGF,4DAqGE,8BArGF,CAsGE,8DAtGF,4BAuGE,8BAvGF,CAyGE,2CAzGF,CA0GE,8BACE,6GADF,CAEE,kGAFF,CAGE,iHAHF,CAIE,gGAJF,CA1GF,CAgHE,8BAhHF,CAkHE,8CAlHF,6HAqHE,8BArHF,CAuHE,6CAvHF,CAwHE,8BACE,+DADF,CAEE,2EAFF,CAGE,+HAHF,CAIE,iHAJF,CAxHF,CA8HE,8BA9HF,CAgIE,8DAhIF,CAiIE,8BACE,8BAAI,0CAAJ,oOADF,CAIE,8BAAI,6CAAJ,gNAGE,8BAHF,kEAJF,CASE,8BAAI,2CAAJ,wQATF,CAjIF,CA8IE,8BA9IF,CAgJE,uEAhJF,CAiJE,2BAAK,KAAK,CAAErB,MAAZ,EACE,oBAAC,SAAD,EACE,IAAI,CAAEY,UADR,CAEE,QAAQ,CAAC,IAFX,CAGE,OAAO,CAAE,CAAC,cAAD,CAHX,EADF,CAjJF,CAwJE,8BAxJF,CA0JE,8CA1JF,CA2JE,2BAAK,KAAK,CAAEZ,MAAZ,EACE,oBAAC,SAAD,EACE,IAAI,CAAEc,UADR,CAEE,QAAQ,CAAC,IAFX,CAGE,OAAO,CAAE,CAAC,cAAD,CAHX,EADF,CA3JF,CADF,CADF,CANF,CADF,CAgLD,C,sBAtLoBvB,S,EAyLvB,cAAgBI,CAAAA,UAAU,CAACS,MAAD,CAAV,CAAmBW,QAAnB,CAAhB","sourcesContent":["import React, { Component } from 'react';\nimport Prism from \"prismjs\"\nimport { Grid, Paper, withStyles, List } from \"@material-ui/core\";\n\nimport '../../ReactJs/styles.css'\nimport Sidebar from '../sidebar';\nimport PrismCode from '../../ReactJs/prismCode';\n\nimport Logistic from '../../../assets/ML/navBayers.png'\n\n\nconst titles = { backgroundColor: '#F0F8FF', padding: '1px', fontSize: '16px' }\n\nconst styles = theme => ({\n  paper: {\n    margin: theme.spacing(1),\n    padding: theme.spacing(1)\n  },\n  smMargin: {\n    margin: theme.spacing(1)\n  },\n  actionDiv: {\n    textAlign: \"center\"\n  }\n})\n\nconst predicting = `\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import GaussianNB\n\ndf = pd.read_csv(\"titanic.csv\")\n\ndf.drop(['PassengerId','Name','SibSp','Parch','Ticket','Cabin','Embarked'],axis='columns',inplace=True)\ninputs = df.drop('Survived',axis='columns')\ntarget = df.Survived\n\n#inputs.Sex = inputs.Sex.map({'male': 1, 'female': 2})\ndummies = pd.get_dummies(inputs.Sex)\ninputs = pd.concat([inputs,dummies],axis='columns')\n                \ninputs.drop(['Sex','male'],axis='columns',inplace=True)                 #Dropping male column of dummy variable trap theory.\n\ninputs.columns[inputs.isna().any()]\ninputs.Age[:10]\ninputs.Age = inputs.Age.fillna(inputs.Age.mean())\n\nX_train, X_test, y_train, y_test = train_test_split(inputs,target,test_size=0.3)\n\nmodel = GaussianNB()\nmodel.fit(X_train,y_train)\n\nmodel.score(X_test,y_test)\nX_test[0:10]\ny_test[0:10]\n\nmodel.predict(X_test[0:10])\nmodel.predict_proba(X_test[:10])\n\nfrom sklearn.model_selection import cross_val_score\ncross_val_score(GaussianNB(),X_train, y_train, cv=5)                  #Calculate the score using cross validation\n`.trim();\n\nconst naiveBease = `\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.pipeline import Pipeline\n\ndf = pd.read_csv(\"spam.csv\")\ndf.groupby('Category').describe()\ndf['spam']=df['Category'].apply(lambda x: 1 if x=='spam' else 0)\n\nX_train, X_test, y_train, y_test = train_test_split(df.Message,df.spam)\n\nv = CountVectorizer()\nX_train_count = v.fit_transform(X_train.values)\nX_train_count.toarray()[:2]\n\nmodel = MultinomialNB()\nmodel.fit(X_train_count,y_train)\n\nemails = ['Hey mohan, can we get?','Upto Dont miss this reward!']\nemails_count = v.transform(emails)\nmodel.predict(emails_count)\n\nX_test_count = v.transform(X_test)\nmodel.score(X_test_count, y_test)\n\nclf = Pipeline([('vectorizer', CountVectorizer()),('nb', MultinomialNB())])\n\nclf.fit(X_train, y_train)\nclf.score(X_test,y_test)\nclf.predict(emails)\n`.trim();\n\n\nclass NaiveBrs extends Component {\n  componentDidMount() {\n    setTimeout(() => Prism.highlightAll(), 0)\n  }\n  render() {\n    const { classes } = this.props;\n    return (\n      <Grid container>\n        <Grid item xs={2}>\n          <Paper className={classes.paper}>\n            <h4><Sidebar /></h4>\n          </Paper>\n        </Grid>\n        <Grid item xs={10}>\n          <Paper className={classes.paper}>\n            <List>\n              <h3>NaÃ¯ve Bayes (lassification technique based)</h3>\n              NaÃ¯ve Bayes algorithms  applying Bayesâ€™ theorem with a strong assumption that\n              all the predictors are independent to each other.\n              <br />\n              <b>Ex.</b> a phone may be considered as smart if it is having touch screen,\n              internet facility, good camera etc. Though all these features are dependent on each other, they contribute\n              independently to the probability of that the phone is a smart phone.\n              <br />\n              <br />\n              <i>In Bayesian classification, the main interest is find the posterior probabilities i.e.</i>\n              <br />\n              the probability of a label given some observed features, ğ‘ƒ(ğ¿ | ğ‘“ğ‘’ğ‘ğ‘¡ğ‘¢ğ‘Ÿğ‘’ğ‘ ). With the help of Bayes theorem, we can\n              express this in quantitative form as follows âˆ’\n              <br />\n              <b>P(L|features)=P(L)P(features|L)P(features)</b>\n              <br />\n              <br />\n              Here, (ğ¿ | ğ‘“ğ‘’ğ‘ğ‘¡ğ‘¢ğ‘Ÿğ‘’ğ‘ ) is the posterior probability of class.<br />\n              ğ‘ƒ(ğ¿) is the prior probability of class.<br />\n              ğ‘ƒ(ğ‘“ğ‘’ğ‘ğ‘¡ğ‘¢ğ‘Ÿğ‘’ğ‘ |ğ¿) is the likelihood which is the probability of predictor given class.\n              <br />\n              ğ‘ƒ(ğ‘“ğ‘’ğ‘ğ‘¡ğ‘¢ğ‘Ÿğ‘’ğ‘ ) is the prior probability of predictor.\n              <br />\n              <br />\n              Building model using NaÃ¯ve Bayes in Python library, Scikit learn. We have the following three types of NaÃ¯ve Bayes model under Scikit learn Python\n              library âˆ’\n              <br />\n\n              <h3>Gaussian NaÃ¯ve Bayes</h3>\n              It is the simplest NaÃ¯ve Bayes classifier having the assumption that the data from each label is drawn from a simple\n              Gaussian distribution.\n              <br />\n              <br />\n              Multinomial NaÃ¯ve Bayes Another useful NaÃ¯ve Bayes classifier in\n              which the features are assumed to be drawn from a simple Multinomial distribution. Such kind of NaÃ¯ve Bayes are most\n              appropriate for the features that represents discrete counts.\n              <br />\n              <br />\n              Bernoulli NaÃ¯ve Bayes Another important model in which features are assumed to be binary (0 and 1). Text classification with â€˜bag of wordsâ€™\n              model can be an application of Bernoulli NaÃ¯ve Bayes.\n              <br />\n              <br />\n              <b>Pros: </b>\n              <ul>\n                <li>NaÃ¯ve Bayes classification is easy to implement and fast.</li>\n                <li>It will converge faster than discriminative models like logistic regression.</li>\n                <li>It requires less training data.</li>\n                <li>It is highly scalable in nature, or they scale linearly with the number of predictors and data points.</li>\n                <li>It can make probabilistic predictions and can handle continuous as well as discrete data. NaÃ¯ve Bayes classification\n                  algorithm can be used for binary as well as multi-class classification problems both.</li>\n              </ul>\n              <br />\n              <b>Cons: </b>\n              Its strong feature independence because in real life it is almost impossible to have a set of features which are completely independent\n              of each other. Its â€˜zero frequencyâ€™ which means that if a categorial variable has a category but not being observed\n              in training data set, then NaÃ¯ve Bayes model will assign a zero probability to it and it will be unable to make a\n              prediction.\n              <br />\n              <br />\n              <b>Applications: </b>\n              <ul>\n                <li><b>Real-time prediction: </b>Due to its ease of implementation and\n                  fast computation, it can be used to do prediction in real-time.</li>\n                <li><b>Multi-class prediction: </b>It can be used to predict posterior probability of multiple classes of target variable.</li>\n                <li><b>Text classification: </b>Due to the feature of multi-class prediction, NaÃ¯ve Bayes classification algorithms are well\n                  suited for text classification. That is why it is also used to solve problems like spam-filtering and sentiment\n                  analysis.</li>\n                <li><b>Recommendation system: </b>Along with the algorithms like collaborative filtering, NaÃ¯ve Bayes makes a\n                  Recommendation system which can be used to filter unseen information and to predict weather a user would like the\n                  given resource or not.</li>\n              </ul>\n              <br />\n\n              <h3>NaÃ¯ve Bayes Classifier Algorithm (supervised)</h3>\n              The NaÃ¯ve Bayes algorithm is comprised of two words NaÃ¯ve and Bayes.\n              <ul>\n                <li>It is mainly used in text classification that includes a high-dimensional training dataset.</li>\n                <li>It is one of the simple and most effective Classification algorithms which helps in building the\n                  fast machine learning models that can make quick predictions.</li>\n                <li>It is a probabilistic classifier, which means it predicts on the basis of the probability of an object.</li>\n                <b>Ex.</b> Spam filtration, Sentimental analysis, and classifying articles.\n                <li><b>NaÃ¯ve: </b>It is called NaÃ¯ve because it assumes that the occurrence of a certain feature is\n                  independent of the occurrence of other features. Such as if the fruit is identified on the bases of\n                  color, shape, and taste, then red, spherical, and sweet fruit is recognized as an apple. Hence each\n                  feature individually contributes to identify that it is an apple without depending on each other.</li>\n                <li><b>Bayes: </b>It is called Bayes because it depends on the principle of Bayes' Theorem.</li>\n              </ul>\n              <br />\n              <b>Bayes' Theorem:</b>\n              <br />\n              <img src={Logistic} alt=\"Equations\" className=\"responsive\" style={{ width: 300, height: 50 }} />\n              <br />\n              <br />\n              Where,\n              <br />\n              <b>P(A|B) is Posterior probability: </b>Probability of hypothesis A on the observed event B.\n              <br />\n              <b>P(B|A) is Likelihood probability: </b>Probability of the evidence given that the probability of a hypothesis is true.\n              <br />\n              <b>P(A) is Prior Probability: </b>Probability of hypothesis before observing the evidence.\n              <br />\n              <b>P(B) is Marginal Probability: </b>Probability of Evidence.\n              <br />\n\n              <b>Advantages:</b>\n              <ul>\n                <li>It is one of the fast and easy ML algorithms to predict a class of datasets.</li>\n                <li>It can be used for Binary as well as Multi-class Classifications.</li>\n                <li>It performs well in Multi-class predictions as compared to the other Algorithms.</li>\n                <li>It is the most popular choice for text classification problems.</li>\n              </ul>\n              <br />\n\n              <b>Disadvantages:</b>\n              Naive Bayes assumes that all features are independent or unrelated, so it cannot learn the relationship\n              between features.\n              <br />\n\n              <b>Applications:</b>\n              <ul>\n                <li>It is used for Credit Scoring.</li>\n                <li>It is used in medical data classification.</li>\n                <li>It can be used in real-time predictions because NaÃ¯ve Bayes Classifier is an eager learner.</li>\n                <li>It is used in Text classification such as Spam filtering and Sentiment analysis.</li>\n              </ul>\n              <br />\n\n              <b>Types of NaÃ¯ve Bayes Model:</b>\n              <ul>\n                <li><b>Gaussian :</b>The Gaussian model assumes that features follow a normal distribution. This means\n                  if predictors take continuous values instead of discrete, then the model assumes that these values are\n                  sampled from the Gaussian distribution.</li>\n                <li><b>Multinomial :</b>Used when the data is multinomial distributed. It is primarily used for document\n                  classification problems, it means a particular document belongs to which category such as Sports, Politics,\n                  education, etc.\n                  <br />\n                  The classifier uses the frequency of words for the predictors.</li>\n                <li><b>Bernoulli :</b>The Bernoulli classifier works similar to the Multinomial classifier, but the\n                  predictor variables are the independent Booleans variables. Such as if a particular word is present\n                  or not in a document. This model is also famous for document classification tasks.</li>\n              </ul>\n              <br />\n\n              <h3>Predicting survival from titanic crash</h3>\n              <div style={titles}>\n                <PrismCode\n                  code={predicting}\n                  language=\"js\"\n                  plugins={[\"line-numbers\"]}\n                />\n              </div>\n              <br />\n\n              <h3>Naive Bayes 2</h3>\n              <div style={titles}>\n                <PrismCode\n                  code={naiveBease}\n                  language=\"js\"\n                  plugins={[\"line-numbers\"]}\n                />\n              </div>\n            </List>\n          </Paper>\n        </Grid>\n      </Grid>\n    )\n  }\n}\n\nexport default (withStyles(styles)(NaiveBrs));\n"]},"metadata":{},"sourceType":"module"}