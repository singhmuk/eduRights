{"ast":null,"code":"import _classCallCheck from\"/home/mukeshs/Projects/edurights/node_modules/@babel/runtime/helpers/esm/classCallCheck\";import _createClass from\"/home/mukeshs/Projects/edurights/node_modules/@babel/runtime/helpers/esm/createClass\";import _possibleConstructorReturn from\"/home/mukeshs/Projects/edurights/node_modules/@babel/runtime/helpers/esm/possibleConstructorReturn\";import _getPrototypeOf from\"/home/mukeshs/Projects/edurights/node_modules/@babel/runtime/helpers/esm/getPrototypeOf\";import _inherits from\"/home/mukeshs/Projects/edurights/node_modules/@babel/runtime/helpers/esm/inherits\";import React,{Component}from'react';import Prism from\"prismjs\";import{Grid,Paper,withStyles,List}from\"@material-ui/core\";import'../../ReactJs/styles.css';import Sidebar from'../sidebar';import PrismCode from'../../ReactJs/prismCode';var titles={backgroundColor:'#F0F8FF',padding:'1px',fontSize:'16px'};var styles=function styles(theme){return{paper:{margin:theme.spacing(1),padding:theme.spacing(1)},smMargin:{margin:theme.spacing(1)},actionDiv:{textAlign:\"center\"}};};var stack=\"\\nimport numpy as np\\n\\n# Decision stump used as weak classifier\\nclass DecisionStump:\\n    def __init__(self):\\n        self.polarity = 1\\n        self.feature_idx = None\\n        self.threshold = None\\n        self.alpha = None\\n\\n    def predict(self, X):\\n        n_samples = X.shape[0]\\n        X_column = X[:, self.feature_idx]\\n        predictions = np.ones(n_samples)\\n        if self.polarity == 1:\\n            predictions[X_column < self.threshold] = -1\\n        else:\\n            predictions[X_column > self.threshold] = -1\\n\\n        return predictions\\n\\n\\nclass Adaboost:\\n    def __init__(self, n_clf=5):\\n        self.n_clf = n_clf\\n        self.clfs = []\\n\\n    def fit(self, X, y):\\n        n_samples, n_features = X.shape\\n\\n        w = np.full(n_samples, (1 / n_samples))                          # Initialize weights to 1/N\\n\\n        self.clfs = []\\n        \\n        for _ in range(self.n_clf):                                      # Iterate through classifiers\\n            clf = DecisionStump()\\n            min_error = float(\\\"inf\\\")\\n\\n            for feature_i in range(n_features):                          # greedy search to find best threshold and feature\\n                X_column = X[:, feature_i]\\n                thresholds = np.unique(X_column)\\n\\n                for threshold in thresholds:\\n                    p = 1                                                # predict with polarity 1\\n                    predictions = np.ones(n_samples)\\n                    predictions[X_column < threshold] = -1\\n\\n                    misclassified = w[y != predictions]                  # Error = sum of weights of misclassified samples\\n                    error = sum(misclassified)\\n\\n                    if error > 0.5:\\n                        error = 1 - error\\n                        p = -1\\n\\n                    if error < min_error:                               # store the best configuration\\n                        clf.polarity = p\\n                        clf.threshold = threshold\\n                        clf.feature_idx = feature_i\\n                        min_error = error\\n\\n            # calculate alpha\\n            EPS = 1e-10\\n            clf.alpha = 0.5 * np.log((1.0 - min_error + EPS) / (min_error + EPS))\\n\\n            predictions = clf.predict(X)                                         # calculate predictions and update weights\\n\\n            w *= np.exp(-clf.alpha * y * predictions)\\n            # Normalize to one\\n            w /= np.sum(w)\\n\\n            self.clfs.append(clf)                                               # Save classifier\\n\\n    def predict(self, X):\\n        clf_preds = [clf.alpha * clf.predict(X) for clf in self.clfs]\\n        y_pred = np.sum(clf_preds, axis=0)\\n        y_pred = np.sign(y_pred)\\n\\n        return y_pred\\n\".trim();var testings=\"\\nif __name__ == \\\"__main__\\\":\\n    from sklearn import datasets\\n    from sklearn.model_selection import train_test_split\\n\\n    def accuracy(y_true, y_pred):\\n        accuracy = np.sum(y_true == y_pred) / len(y_true)   \\n        return accuracy\\n\\n    data = datasets.load_breast_cancer()\\n    X, y = data.data, data.target\\n\\n    y[y == 0] = -1\\n\\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=5)\\n\\n    # Adaboost classification with 5 weak classifiers\\n    clf = Adaboost(n_clf=5)\\n    clf.fit(X_train, y_train)\\n    y_pred = clf.predict(X_test)\\n\\n    acc = accuracy(y_test, y_pred)\\n    print(\\\"Accuracy:\\\", acc)\\n    \".trim();var performance=\"\\nPerformance of stump = 1/2 * log(1-Total Error) / Total Error\\n\\nNew weights = old weight * e(+-Performanse)\\n    where, + for Misclassification\\n           - for Right classification\\n\".trim();// const stack = ``.trim();\nvar Adaboots=/*#__PURE__*/function(_Component){_inherits(Adaboots,_Component);function Adaboots(){_classCallCheck(this,Adaboots);return _possibleConstructorReturn(this,_getPrototypeOf(Adaboots).apply(this,arguments));}_createClass(Adaboots,[{key:\"componentDidMount\",value:function componentDidMount(){setTimeout(function(){return Prism.highlightAll();},0);}},{key:\"render\",value:function render(){var classes=this.props.classes;return React.createElement(Grid,{container:true},React.createElement(Grid,{item:true,xs:2},React.createElement(Paper,{className:classes.paper},React.createElement(\"h4\",null,React.createElement(Sidebar,null)))),React.createElement(Grid,{item:true,xs:10},React.createElement(Paper,{className:classes.paper},React.createElement(List,null,React.createElement(\"h3\",null,\"Boosting Types\"),React.createElement(\"ul\",null,React.createElement(\"li\",null,React.createElement(\"b\",null,\"1. Adaboost: \")),React.createElement(\"li\",null,React.createElement(\"b\",null,\"2. Gradient Boosting: \"),\"Instead of Weights updation, here gradient (residuals, loss) is passed in next model.\"),React.createElement(\"br\",null),React.createElement(\"li\",null,React.createElement(\"b\",null,\"3. Extream Gradient Boosting: \")),React.createElement(\"ul\",null,React.createElement(\"li\",null,\"Much similar to GB.\"),React.createElement(\"li\",null,\"2nd order Derivatives of Loss function.\"),React.createElement(\"li\",null,\"High Performance.\"),React.createElement(\"li\",null,\"Fast training.\"),React.createElement(\"li\",null,\"Advanced L1 and L2 Loass Regularization.\"),React.createElement(\"li\",null,\"Parallel and Distributed computing (DMLC).\"),React.createElement(\"li\",null,\"It handle missing values.\"),React.createElement(\"li\",null,\"Cache Optimisation\"),React.createElement(\"li\",null,\"It has many hyperparameters. reg_alpha, reg_lambda.\"))),React.createElement(\"br\",null),React.createElement(\"h3\",null,\"Adaboost (Adaptive boosting)\"),React.createElement(\"ul\",null,React.createElement(\"li\",null,\"Used for Classification and Regression.\"),React.createElement(\"li\",null,\"Sequencial boosting.\"),React.createElement(\"li\",null,\"AdaBoost is one of the first boosting algorithms to be adapted in solving practices. Adaboost helps you combine multiple \\u201Cweak classifiers\\u201D into a single \\u201Cstrong classifier\\u201D.\"),React.createElement(\"li\",null,\"The weak learners in AdaBoost are decision trees with a single split, called \",React.createElement(\"b\",null,\"decision stumps\"),\".\"),React.createElement(\"li\",null,\"AdaBoost works by putting more weight on difficult to classify instances and less on those already handled well.\"),React.createElement(\"li\",null,\"Weight increase for misclassification and weight decreses for right classifications.\"),React.createElement(\"li\",null,\"Used to exploit dependency between models.\"),React.createElement(\"li\",null,\"Stagewise additive MultiModeling using Multiclass Exponential Loss Function.\"),React.createElement(\"li\",null,\"Can handle missing values and outliner.\"),React.createElement(\"li\",null,\"Can handles mixed predictors as well (Quantitive and Qualitative).\")),React.createElement(\"br\",null),React.createElement(\"br\",null),React.createElement(\"b\",null,\"Steps for Adaboost Algoritham:\"),React.createElement(\"ul\",null,React.createElement(\"li\",null,\"1. Initialize the weights as 1/n to every n observations.\"),React.createElement(\"li\",null,\"2. Select the 1 Feature according to Lowest Gini/Highest information Gain and calculate the total error.\"),React.createElement(\"li\",null,\"3. Calculate the Performance of the stump.\"),React.createElement(\"li\",null,\"4. Calculate the new weights for each misclassification(increase) and right classification(decrease).\"),React.createElement(\"li\",null,\"5. Normalize the new weights so that sum of weight is 1.\"),React.createElement(\"li\",null,\"6. Repeat from step 2 to till configured number of estimators reacfied the accuracy achieved.\")),React.createElement(\"br\",null),React.createElement(\"div\",{style:titles},React.createElement(PrismCode,{code:performance,language:\"js\",plugins:[\"line-numbers\"]})),React.createElement(\"br\",null),React.createElement(\"div\",{style:titles},React.createElement(PrismCode,{code:stack,language:\"js\",plugins:[\"line-numbers\"]})),React.createElement(\"br\",null),React.createElement(\"h3\",null,\"Testing\"),React.createElement(\"div\",{style:titles},React.createElement(PrismCode,{code:testings,language:\"js\",plugins:[\"line-numbers\"]}))))));}}]);return Adaboots;}(Component);export default withStyles(styles)(Adaboots);","map":{"version":3,"sources":["/home/mukeshs/Projects/edurights/src/components/ml/deepMl/adaboost.js"],"names":["React","Component","Prism","Grid","Paper","withStyles","List","Sidebar","PrismCode","titles","backgroundColor","padding","fontSize","styles","theme","paper","margin","spacing","smMargin","actionDiv","textAlign","stack","trim","testings","performance","Adaboots","setTimeout","highlightAll","classes","props"],"mappings":"6kBAAA,MAAOA,CAAAA,KAAP,EAAgBC,SAAhB,KAAiC,OAAjC,CACA,MAAOC,CAAAA,KAAP,KAAkB,SAAlB,CACA,OAASC,IAAT,CAAeC,KAAf,CAAsBC,UAAtB,CAAkCC,IAAlC,KAA8C,mBAA9C,CAEA,MAAO,0BAAP,CACA,MAAOC,CAAAA,OAAP,KAAoB,YAApB,CACA,MAAOC,CAAAA,SAAP,KAAsB,yBAAtB,CAGA,GAAMC,CAAAA,MAAM,CAAG,CAAEC,eAAe,CAAE,SAAnB,CAA8BC,OAAO,CAAE,KAAvC,CAA8CC,QAAQ,CAAE,MAAxD,CAAf,CAEA,GAAMC,CAAAA,MAAM,CAAG,QAATA,CAAAA,MAAS,CAAAC,KAAK,QAAK,CACvBC,KAAK,CAAE,CACLC,MAAM,CAAEF,KAAK,CAACG,OAAN,CAAc,CAAd,CADH,CAELN,OAAO,CAAEG,KAAK,CAACG,OAAN,CAAc,CAAd,CAFJ,CADgB,CAKvBC,QAAQ,CAAE,CACRF,MAAM,CAAEF,KAAK,CAACG,OAAN,CAAc,CAAd,CADA,CALa,CAQvBE,SAAS,CAAE,CACTC,SAAS,CAAE,QADF,CARY,CAAL,EAApB,CAcA,GAAMC,CAAAA,KAAK,CAAG,qwFA+EZC,IA/EY,EAAd,CAiFA,GAAMC,CAAAA,QAAQ,CAAG,gqBAuBXD,IAvBW,EAAjB,CAyBA,GAAME,CAAAA,WAAW,CAAG,8LAMlBF,IANkB,EAApB,CAQA;GAGMG,CAAAA,Q,gSACgB,CAClBC,UAAU,CAAC,iBAAMxB,CAAAA,KAAK,CAACyB,YAAN,EAAN,EAAD,CAA6B,CAA7B,CAAV,CACD,C,uCACQ,IACCC,CAAAA,OADD,CACa,KAAKC,KADlB,CACCD,OADD,CAEP,MACE,qBAAC,IAAD,EAAM,SAAS,KAAf,EACE,oBAAC,IAAD,EAAM,IAAI,KAAV,CAAW,EAAE,CAAE,CAAf,EACE,oBAAC,KAAD,EAAO,SAAS,CAAEA,OAAO,CAACb,KAA1B,EACE,8BAAI,oBAAC,OAAD,MAAJ,CADF,CADF,CADF,CAME,oBAAC,IAAD,EAAM,IAAI,KAAV,CAAW,EAAE,CAAE,EAAf,EACE,oBAAC,KAAD,EAAO,SAAS,CAAEa,OAAO,CAACb,KAA1B,EACE,oBAAC,IAAD,MACE,+CADF,CAEE,8BACE,8BAAI,6CAAJ,CADF,CAEE,8BAAI,sDAAJ,yFAFF,CAGE,8BAHF,CAIE,8BAAI,8DAAJ,CAJF,CAKE,8BACE,oDADF,CAEE,wEAFF,CAGE,kDAHF,CAIE,+CAJF,CAKE,yEALF,CAME,2EANF,CAOE,0DAPF,CAQE,mDARF,CASE,oFATF,CALF,CAFF,CAoBE,8BApBF,CAsBE,6DAtBF,CAuBE,8BACE,wEADF,CAEE,qDAFF,CAGE,mOAHF,CAKE,8GAAiF,+CAAjF,KALF,CAME,iJANF,CAOE,qHAPF,CAQE,2EARF,CASE,6GATF,CAUE,wEAVF,CAWE,mGAXF,CAvBF,CAoCE,8BApCF,CAqCE,8BArCF,CAuCE,8DAvCF,CAwCE,8BACE,0FADF,CAEE,yIAFF,CAGE,2EAHF,CAIE,sIAJF,CAKE,yFALF,CAME,8HANF,CAxCF,CAgDE,8BAhDF,CAkDE,2BAAK,KAAK,CAAEN,MAAZ,EACE,oBAAC,SAAD,EACE,IAAI,CAAEe,WADR,CAEE,QAAQ,CAAC,IAFX,CAGE,OAAO,CAAE,CAAC,cAAD,CAHX,EADF,CAlDF,CAyDE,8BAzDF,CA2DE,2BAAK,KAAK,CAAEf,MAAZ,EACE,oBAAC,SAAD,EACE,IAAI,CAAEY,KADR,CAEE,QAAQ,CAAC,IAFX,CAGE,OAAO,CAAE,CAAC,cAAD,CAHX,EADF,CA3DF,CAkEE,8BAlEF,CAoEE,wCApEF,CAqEE,2BAAK,KAAK,CAAEZ,MAAZ,EACE,oBAAC,SAAD,EACE,IAAI,CAAEc,QADR,CAEE,QAAQ,CAAC,IAFX,CAGE,OAAO,CAAE,CAAC,cAAD,CAHX,EADF,CArEF,CADF,CADF,CANF,CADF,CA8GD,C,sBApHoBtB,S,EAuHvB,cAAgBI,CAAAA,UAAU,CAACQ,MAAD,CAAV,CAAmBY,QAAnB,CAAhB","sourcesContent":["import React, { Component } from 'react';\nimport Prism from \"prismjs\"\nimport { Grid, Paper, withStyles, List } from \"@material-ui/core\";\n\nimport '../../ReactJs/styles.css'\nimport Sidebar from '../sidebar';\nimport PrismCode from '../../ReactJs/prismCode';\n\n\nconst titles = { backgroundColor: '#F0F8FF', padding: '1px', fontSize: '16px' }\n\nconst styles = theme => ({\n  paper: {\n    margin: theme.spacing(1),\n    padding: theme.spacing(1)\n  },\n  smMargin: {\n    margin: theme.spacing(1)\n  },\n  actionDiv: {\n    textAlign: \"center\"\n  }\n})\n\n\nconst stack = `\nimport numpy as np\n\n# Decision stump used as weak classifier\nclass DecisionStump:\n    def __init__(self):\n        self.polarity = 1\n        self.feature_idx = None\n        self.threshold = None\n        self.alpha = None\n\n    def predict(self, X):\n        n_samples = X.shape[0]\n        X_column = X[:, self.feature_idx]\n        predictions = np.ones(n_samples)\n        if self.polarity == 1:\n            predictions[X_column < self.threshold] = -1\n        else:\n            predictions[X_column > self.threshold] = -1\n\n        return predictions\n\n\nclass Adaboost:\n    def __init__(self, n_clf=5):\n        self.n_clf = n_clf\n        self.clfs = []\n\n    def fit(self, X, y):\n        n_samples, n_features = X.shape\n\n        w = np.full(n_samples, (1 / n_samples))                          # Initialize weights to 1/N\n\n        self.clfs = []\n        \n        for _ in range(self.n_clf):                                      # Iterate through classifiers\n            clf = DecisionStump()\n            min_error = float(\"inf\")\n\n            for feature_i in range(n_features):                          # greedy search to find best threshold and feature\n                X_column = X[:, feature_i]\n                thresholds = np.unique(X_column)\n\n                for threshold in thresholds:\n                    p = 1                                                # predict with polarity 1\n                    predictions = np.ones(n_samples)\n                    predictions[X_column < threshold] = -1\n\n                    misclassified = w[y != predictions]                  # Error = sum of weights of misclassified samples\n                    error = sum(misclassified)\n\n                    if error > 0.5:\n                        error = 1 - error\n                        p = -1\n\n                    if error < min_error:                               # store the best configuration\n                        clf.polarity = p\n                        clf.threshold = threshold\n                        clf.feature_idx = feature_i\n                        min_error = error\n\n            # calculate alpha\n            EPS = 1e-10\n            clf.alpha = 0.5 * np.log((1.0 - min_error + EPS) / (min_error + EPS))\n\n            predictions = clf.predict(X)                                         # calculate predictions and update weights\n\n            w *= np.exp(-clf.alpha * y * predictions)\n            # Normalize to one\n            w /= np.sum(w)\n\n            self.clfs.append(clf)                                               # Save classifier\n\n    def predict(self, X):\n        clf_preds = [clf.alpha * clf.predict(X) for clf in self.clfs]\n        y_pred = np.sum(clf_preds, axis=0)\n        y_pred = np.sign(y_pred)\n\n        return y_pred\n`.trim();\n\nconst testings = `\nif __name__ == \"__main__\":\n    from sklearn import datasets\n    from sklearn.model_selection import train_test_split\n\n    def accuracy(y_true, y_pred):\n        accuracy = np.sum(y_true == y_pred) / len(y_true)   \n        return accuracy\n\n    data = datasets.load_breast_cancer()\n    X, y = data.data, data.target\n\n    y[y == 0] = -1\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=5)\n\n    # Adaboost classification with 5 weak classifiers\n    clf = Adaboost(n_clf=5)\n    clf.fit(X_train, y_train)\n    y_pred = clf.predict(X_test)\n\n    acc = accuracy(y_test, y_pred)\n    print(\"Accuracy:\", acc)\n    `.trim();\n\nconst performance = `\nPerformance of stump = 1/2 * log(1-Total Error) / Total Error\n\nNew weights = old weight * e(+-Performanse)\n    where, + for Misclassification\n           - for Right classification\n`.trim();\n\n// const stack = ``.trim();\n\n\nclass Adaboots extends Component {\n  componentDidMount() {\n    setTimeout(() => Prism.highlightAll(), 0)\n  }\n  render() {\n    const { classes } = this.props;\n    return (\n      <Grid container>\n        <Grid item xs={2}>\n          <Paper className={classes.paper}>\n            <h4><Sidebar /></h4>\n          </Paper>\n        </Grid>\n        <Grid item xs={10}>\n          <Paper className={classes.paper}>\n            <List>\n              <h3>Boosting Types</h3>\n              <ul>\n                <li><b>1. Adaboost: </b></li>\n                <li><b>2. Gradient Boosting: </b>Instead of Weights updation, here gradient (residuals, loss) is passed in next model.</li>\n                <br />\n                <li><b>3. Extream Gradient Boosting: </b></li>\n                <ul>\n                  <li>Much similar to GB.</li>\n                  <li>2nd order Derivatives of Loss function.</li>\n                  <li>High Performance.</li>\n                  <li>Fast training.</li>\n                  <li>Advanced L1 and L2 Loass Regularization.</li>\n                  <li>Parallel and Distributed computing (DMLC).</li>\n                  <li>It handle missing values.</li>\n                  <li>Cache Optimisation</li>\n                  <li>It has many hyperparameters. reg_alpha, reg_lambda.</li>\n                </ul>\n              </ul>\n\n              <br />\n\n              <h3>Adaboost (Adaptive boosting)</h3>\n              <ul>\n                <li>Used for Classification and Regression.</li>\n                <li>Sequencial boosting.</li>\n                <li>AdaBoost is one of the first boosting algorithms to be adapted in solving practices. Adaboost helps you combine\n                  multiple “weak classifiers” into a single “strong classifier”.</li>\n                <li>The weak learners in AdaBoost are decision trees with a single split, called <b>decision stumps</b>.</li>\n                <li>AdaBoost works by putting more weight on difficult to classify instances and less on those already handled well.</li>\n                <li>Weight increase for misclassification and weight decreses for right classifications.</li>\n                <li>Used to exploit dependency between models.</li>\n                <li>Stagewise additive MultiModeling using Multiclass Exponential Loss Function.</li>\n                <li>Can handle missing values and outliner.</li>\n                <li>Can handles mixed predictors as well (Quantitive and Qualitative).</li>\n              </ul>\n              <br />\n              <br />\n\n              <b>Steps for Adaboost Algoritham:</b>\n              <ul>\n                <li>1. Initialize the weights as 1/n to every n observations.</li>\n                <li>2. Select the 1 Feature according to Lowest Gini/Highest information Gain and calculate the total error.</li>\n                <li>3. Calculate the Performance of the stump.</li>\n                <li>4. Calculate the new weights for each misclassification(increase) and right classification(decrease).</li>\n                <li>5. Normalize the new weights so that sum of weight is 1.</li>\n                <li>6. Repeat from step 2 to till configured number of estimators reacfied the accuracy achieved.</li>\n              </ul>\n              <br />\n\n              <div style={titles}>\n                <PrismCode\n                  code={performance}\n                  language=\"js\"\n                  plugins={[\"line-numbers\"]}\n                />\n              </div>\n              <br />\n\n              <div style={titles}>\n                <PrismCode\n                  code={stack}\n                  language=\"js\"\n                  plugins={[\"line-numbers\"]}\n                />\n              </div>\n              <br />\n\n              <h3>Testing</h3>\n              <div style={titles}>\n                <PrismCode\n                  code={testings}\n                  language=\"js\"\n                  plugins={[\"line-numbers\"]}\n                />\n              </div>\n              {/* <br />\n\n              <h3></h3>\n              <div style={titles}>\n                <PrismCode\n                  code={stack}\n                  language=\"js\"\n                  plugins={[\"line-numbers\"]}\n                />\n              </div>\n              <br />\n\n              <h3></h3>\n              <div style={titles}>\n                <PrismCode\n                  code={stack}\n                  language=\"js\"\n                  plugins={[\"line-numbers\"]}\n                />\n              </div> */}\n            </List>\n          </Paper>\n        </Grid>\n      </Grid>\n    )\n  }\n}\n\nexport default (withStyles(styles)(Adaboots));\n"]},"metadata":{},"sourceType":"module"}