{"ast":null,"code":"import _classCallCheck from\"/home/mukeshs/Projects/edurights/node_modules/@babel/runtime/helpers/esm/classCallCheck\";import _createClass from\"/home/mukeshs/Projects/edurights/node_modules/@babel/runtime/helpers/esm/createClass\";import _possibleConstructorReturn from\"/home/mukeshs/Projects/edurights/node_modules/@babel/runtime/helpers/esm/possibleConstructorReturn\";import _getPrototypeOf from\"/home/mukeshs/Projects/edurights/node_modules/@babel/runtime/helpers/esm/getPrototypeOf\";import _inherits from\"/home/mukeshs/Projects/edurights/node_modules/@babel/runtime/helpers/esm/inherits\";import React,{Component}from'react';import Prism from\"prismjs\";import{Grid,Paper,withStyles,List}from\"@material-ui/core\";import'../../ReactJs/styles.css';import Sidebar from'../sidebar';import PrismCode from'../../ReactJs/prismCode';var titles={backgroundColor:'#F0F8FF',padding:'1px',fontSize:'16px'};var styles=function styles(theme){return{paper:{margin:theme.spacing(1),padding:theme.spacing(1)},smMargin:{margin:theme.spacing(1)},actionDiv:{textAlign:\"center\"}};};var rnmFor=\"\\nimport pandas as pd\\nimport seaborn as sn\\nfrom sklearn.datasets import load_digits\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.ensemble import RandomForestClassifier\\nfrom sklearn.metrics import confusion_matrix \\nimport matplotlib.pyplot as plt\\n%matplotlib inline\\n\\ndigits = load_digits()\\ndir(digits)\\n\\nplt.gray() \\nfor i in range(4):\\n    plt.matshow(digits.images[i]) \\n    \\ndf = pd.DataFrame(digits.data)\\ndf['target'] = digits.target\\n\\nX = df.drop('target',axis='columns')\\ny = df.target\\n\\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2)\\nmodel = RandomForestClassifier(n_estimators=20)\\nmodel.fit(X_train, y_train)\\nmodel.score(X_test, y_test)\\n\\ny_predicted = model.predict(X_test)\\ncm = confusion_matrix(y_test, y_predicted)\\n\\nplt.figure(figsize=(10,7))\\nsn.heatmap(cm, annot=True)\\n\\nplt.xlabel('Predicted')\\nplt.ylabel('Truth')\\n\".trim();var stack=\"\\nimport numpy as np\\nfrom collections import Counter\\nfrom .decision_tree import DecisionTree\\n\\n\\ndef bootstrap_sample(X, y):\\n    n_samples = X.shape[0]\\n    idxs = np.random.choice(n_samples, n_samples, replace=True)\\n    return X[idxs], y[idxs]\\n\\n\\ndef most_common_label(y):\\n    counter = Counter(y)\\n    most_common = counter.most_common(1)[0][0]\\n    return most_common\\n\\n\\nclass RandomForest:\\n    def __init__(self, n_trees=10, min_samples_split=2, max_depth=100, n_feats=None):\\n        self.n_trees = n_trees\\n        self.min_samples_split = min_samples_split\\n        self.max_depth = max_depth\\n        self.n_feats = n_feats\\n        self.trees = []\\n\\n    def fit(self, X, y):\\n        self.trees = []\\n        for _ in range(self.n_trees):\\n            tree = DecisionTree(\\n                min_samples_split=self.min_samples_split,\\n                max_depth=self.max_depth,\\n                n_feats=self.n_feats,\\n            )\\n            X_samp, y_samp = bootstrap_sample(X, y)\\n            tree.fit(X_samp, y_samp)\\n            self.trees.append(tree)\\n\\n    def predict(self, X):\\n        tree_preds = np.array([tree.predict(X) for tree in self.trees])\\n        tree_preds = np.swapaxes(tree_preds, 0, 1)\\n        y_pred = [most_common_label(tree_pred) for tree_pred in tree_preds]\\n        return np.array(y_pred)\\n\".trim();var testings=\"\\nif __name__ == \\\"__main__\\\":\\n    from sklearn import datasets\\n    from sklearn.model_selection import train_test_split\\n\\n    def accuracy(y_true, y_pred):\\n        accuracy = np.sum(y_true == y_pred) / len(y_true)\\n        return accuracy\\n\\n    data = datasets.load_breast_cancer()\\n    X = data.data\\n    y = data.target\\n\\n    X_train, X_test, y_train, y_test = train_test_split(\\n        X, y, test_size=0.2, random_state=1234\\n    )\\n\\n    clf = RandomForest(n_trees=3, max_depth=10)\\n\\n    clf.fit(X_train, y_train)\\n    y_pred = clf.predict(X_test)\\n    acc = accuracy(y_test, y_pred)\\n\\n    print(\\\"Accuracy:\\\", acc)\\n\".trim();var RandomForest=/*#__PURE__*/function(_Component){_inherits(RandomForest,_Component);function RandomForest(){_classCallCheck(this,RandomForest);return _possibleConstructorReturn(this,_getPrototypeOf(RandomForest).apply(this,arguments));}_createClass(RandomForest,[{key:\"componentDidMount\",value:function componentDidMount(){setTimeout(function(){return Prism.highlightAll();},0);}},{key:\"render\",value:function render(){var classes=this.props.classes;return React.createElement(Grid,{container:true},React.createElement(Grid,{item:true,xs:2},React.createElement(Paper,{className:classes.paper},React.createElement(\"h4\",null,React.createElement(Sidebar,null)))),React.createElement(Grid,{item:true,xs:10},React.createElement(Paper,{className:classes.paper},React.createElement(List,null,React.createElement(\"h3\",null,\"Random Forest (supervised learning algorithm)\"),\"Random forest is used for both classification as well as regression. But however, it is mainly used for classification problems. As we know that a forest is made up of trees and more trees means more robust forest. Similarly, random forest algorithm creates decision trees on data samples and then gets the prediction from each of them and finally selects the best solution by means of voting. It is an ensemble method which is better than a single decision tree because it reduces the over-fitting by averaging the result. Working of Random Forest Algorithm.\",React.createElement(\"br\",null),React.createElement(\"br\",null),React.createElement(\"ul\",null,React.createElement(\"li\",null,\"1. Start with the selection of random samples from a given dataset. \"),React.createElement(\"li\",null,\"2. This algorithm will construct a decision tree for every sample. Then it will get the prediction result from every decision tree.\"),React.createElement(\"li\",null,\"3. Voting will be performed for every predicted result.\"),React.createElement(\"li\",null,\"4. At last, select the most voted prediction result as the final prediction result.\")),React.createElement(\"br\",null),React.createElement(\"b\",null,\"Pros: \"),React.createElement(\"ul\",null,React.createElement(\"li\",null,\"It overcomes the problem of overfitting by averaging or combining the results of different decision trees.\"),React.createElement(\"li\",null,\"Work well for a large range of data items than a single decision tree does. Random forest has less variance then single decision tree.\"),React.createElement(\"li\",null,\"Very flexible and possess very high accuracy.\"),React.createElement(\"li\",null,\"Scaling of data does not require in random forest algorithm. It maintains good accuracy even after providing data without scaling.\"),React.createElement(\"li\",null,\"Maintains good accuracy even a large proportion of the data is missing.\"),React.createElement(\"li\",null)),React.createElement(\"br\",null),React.createElement(\"b\",null,\"Cons: \"),React.createElement(\"ul\",null,React.createElement(\"li\",null,\"Complexity is the main disadvantage of Random forest algorithms.\"),React.createElement(\"li\",null,\"Construction of Random forests are much harder and time-consuming than decision trees. More computational resources are required to implement Random Forest algorithm.\"),React.createElement(\"li\",null,\"It is less intuitive in case when we have a large collection of decision trees. The prediction process using random forests is very time-consuming in comparison with other algorithms.\")),React.createElement(\"br\",null),React.createElement(\"h3\",null,\"Random Forest(Supervised)\"),\"Used widely in Classification and Regression problems.It builds decision trees on different samples and takes their majority vote for classification and average in case of regression.\",React.createElement(\"br\",null),React.createElement(\"br\",null),\"One of the most important features of the Random Forest Algorithm is that it can handle the data set containing continuous variables in regression and categorical variables in classification(better results).             for classification problems.\",React.createElement(\"br\",null),React.createElement(\"br\",null),\"A student named X wants to choose a course after his 10+2, and he is confused about the choice of course based on his skill set. So he decides to consult various people like his cousins, teachers, parents, degree students, and working people. He asks them varied questions like why he should choose, job opportunities with that course, course fee, etc. Finally, after consulting various people about the course he decides to take the course suggested by most of the people.\",React.createElement(\"br\",null),React.createElement(\"br\",null),React.createElement(\"b\",null,\"Ensemble uses two types of methods: \"),React.createElement(\"ul\",null,React.createElement(\"li\",null,\"Bagging (Parallel), Random forest works on the Bagging principle.\"),React.createElement(\"li\",null,\"Boosting (Sequential)\")),React.createElement(\"br\",null),React.createElement(\"b\",null,\"Steps involved in random forest algorithm:\"),React.createElement(\"ul\",null,React.createElement(\"li\",null,\"n number of random records are taken from the data set having k number of records.\"),React.createElement(\"li\",null,\"Individual decision trees are constructed for each sample.\"),React.createElement(\"li\",null,\"Each decision tree will generate an output.\"),React.createElement(\"li\",null,\"Final output is considered based on Majority Voting or Averaging for Classification and regression respectively.\")),React.createElement(\"br\",null),React.createElement(\"b\",null,\"Important Features of Random Forest: \"),React.createElement(\"ul\",null,React.createElement(\"li\",null,React.createElement(\"b\",null,\"Diversity: \"),\"Not all attributes/variables/features are considered while making an individual tree, each tree is different.\"),React.createElement(\"li\",null,React.createElement(\"b\",null,\"Immune to the curse of dimensionality: \"),\"Since each tree does not consider all the features, the feature space is reduced.\"),React.createElement(\"li\",null,React.createElement(\"b\",null,\"Parallelization: \"),\"Each tree is created independently out of different data and attributes. This means that we can make full use of the CPU to build random forests.\"),React.createElement(\"li\",null,React.createElement(\"b\",null,\"Train-Test split: \")),React.createElement(\"li\",null,React.createElement(\"b\",null,\"Stability: \"),\"Stability arises because the result is based on majority voting/ averaging.\")),React.createElement(\"br\",null),React.createElement(\"b\",null,\"Difference Between Decision Tree & Random Forest\"),React.createElement(\"br\",null),\"Random forest is a collection of decision trees.\",React.createElement(\"table\",null,React.createElement(\"tr\",null,React.createElement(\"th\",null,\"Decision trees\"),React.createElement(\"th\",null,\"Random Forest\")),React.createElement(\"tr\",null,React.createElement(\"td\",null,\"Normally suffer from the problem of overfitting if it\\u2019s allowed to grow without any control.\"),React.createElement(\"td\",null,\"Created from subsets of data and the final output is based on average or majority ranking and hence the problem of overfitting is taken care of.\")),React.createElement(\"tr\",null,React.createElement(\"td\",null,\"A single decision tree is faster in computation.\"),React.createElement(\"td\",null,\"Comparatively slower.\")),React.createElement(\"tr\",null,React.createElement(\"td\",null,\"When a data set with features is taken as input by a decision tree it will formulate some set of rules to do prediction.\"),React.createElement(\"td\",null,\"Randomly selects observations, builds a decision tree and the average result is taken. It doesn\\u2019t use any set of formulas.\"))),React.createElement(\"br\",null),React.createElement(\"i\",null,\"Thus random forests are much more successful than decision trees only if the trees are diverse and acceptable.\"),React.createElement(\"br\",null),React.createElement(\"h3\",null,\"Important Hyperparameters\"),\"Hyperparameters are used in random forests to either enhance the performance and predictive power of models or to make the model faster.\",React.createElement(\"ul\",null,React.createElement(\"li\",null,React.createElement(\"b\",null,\"n_estimators: \"),\"number of trees the algorithm builds before averaging the predictions.\"),React.createElement(\"li\",null,React.createElement(\"b\",null,\"max_features: \"),\"max. number of features random forest considers splitting a node.\"),React.createElement(\"li\",null,React.createElement(\"b\",null,\"mini_sample_leaf: \"),\"determines the min. number of leaves required to split an internal node.\")),React.createElement(\"br\",null),React.createElement(\"b\",null,\"Following hyperparameters increases the speed: \"),React.createElement(\"ul\",null,React.createElement(\"li\",null,React.createElement(\"b\",null,\"n_jobs: \"),\" It tells the engine how many processors it is allowed to use. If the value is 1, it can use only one processor but if the value is -1 there is no limit.\"),React.createElement(\"li\",null,React.createElement(\"b\",null,\"random_state: \"),\"Controls randomness of the sample. The model will always produce the same results if it has a definite value of random state and if it has been given the same hyperparameters and the same training data.\"),React.createElement(\"li\",null,React.createElement(\"b\",null,\"oob_score : \"),\"OOB means out of the bag. It is a random forest cross-validation method. In this one-third of the sample is not used to train the data instead used to evaluate its performance. These samples are called out of bag samples.\")),React.createElement(\"br\",null),React.createElement(\"b\",null,\"Advantages:\"),React.createElement(\"ul\",null,React.createElement(\"li\",null,\"It can be used in classification and regression problems.\"),React.createElement(\"li\",null,\"It solves the problem of overfitting as output is based on majority voting or averaging.\"),React.createElement(\"li\",null,\"It performs well even if the data contains null/missing values.\"),React.createElement(\"li\",null,\"Each decision tree created is independent of the other thus it shows the property of parallelization.\"),React.createElement(\"li\",null,\"It is highly stable as the average answers given by a large number of trees are taken.\"),React.createElement(\"li\",null,\"It maintains diversity as all the attributes are not considered while making each decision tree though it is not true in all cases.\"),React.createElement(\"li\",null,\"It is immune to the curse of dimensionality. Since each tree does not consider all the attributes, feature space is reduced.\"),React.createElement(\"li\",null,\"We don\\u2019t have to segregate data into train and test as there will always be 30% of the data which is not seen by the decision tree made out of bootstrap.\")),React.createElement(\"br\",null),React.createElement(\"b\",null,\"Disadvantages:\"),React.createElement(\"ul\",null,React.createElement(\"li\",null,\"Random forest is highly complex when compared to decision trees where decisions can be made by following the path of the tree.\"),React.createElement(\"li\",null,\"Training time is more compared to other models due to its complexity. Whenever it has to make a prediction each decision tree has to generate output for the given input data.\")),React.createElement(\"br\",null),React.createElement(\"h3\",null,\"Example\"),React.createElement(\"b\",null,\"Digits dataset from sklearn\"),React.createElement(\"br\",null),React.createElement(\"br\",null),React.createElement(\"div\",{style:titles},React.createElement(PrismCode,{code:rnmFor,language:\"js\",plugins:[\"line-numbers\"]})),React.createElement(\"br\",null),React.createElement(\"h3\",null,\"Random Forest 2\"),React.createElement(\"div\",{style:titles},React.createElement(PrismCode,{code:stack,language:\"js\",plugins:[\"line-numbers\"]})),React.createElement(\"br\",null),React.createElement(\"h3\",null,\"Testing\"),React.createElement(\"div\",{style:titles},React.createElement(PrismCode,{code:testings,language:\"js\",plugins:[\"line-numbers\"]}))))));}}]);return RandomForest;}(Component);export default withStyles(styles)(RandomForest);","map":{"version":3,"sources":["/home/mukeshs/Projects/edurights/src/components/ml/deepMl/randomForest.js"],"names":["React","Component","Prism","Grid","Paper","withStyles","List","Sidebar","PrismCode","titles","backgroundColor","padding","fontSize","styles","theme","paper","margin","spacing","smMargin","actionDiv","textAlign","rnmFor","trim","stack","testings","RandomForest","setTimeout","highlightAll","classes","props"],"mappings":"6kBAAA,MAAOA,CAAAA,KAAP,EAAgBC,SAAhB,KAAiC,OAAjC,CACA,MAAOC,CAAAA,KAAP,KAAkB,SAAlB,CACA,OAASC,IAAT,CAAeC,KAAf,CAAsBC,UAAtB,CAAkCC,IAAlC,KAA8C,mBAA9C,CAEA,MAAO,0BAAP,CACA,MAAOC,CAAAA,OAAP,KAAoB,YAApB,CACA,MAAOC,CAAAA,SAAP,KAAsB,yBAAtB,CAGA,GAAMC,CAAAA,MAAM,CAAG,CAAEC,eAAe,CAAE,SAAnB,CAA8BC,OAAO,CAAE,KAAvC,CAA8CC,QAAQ,CAAE,MAAxD,CAAf,CAEA,GAAMC,CAAAA,MAAM,CAAG,QAATA,CAAAA,MAAS,CAAAC,KAAK,QAAK,CACvBC,KAAK,CAAE,CACLC,MAAM,CAAEF,KAAK,CAACG,OAAN,CAAc,CAAd,CADH,CAELN,OAAO,CAAEG,KAAK,CAACG,OAAN,CAAc,CAAd,CAFJ,CADgB,CAKvBC,QAAQ,CAAE,CACRF,MAAM,CAAEF,KAAK,CAACG,OAAN,CAAc,CAAd,CADA,CALa,CAQvBE,SAAS,CAAE,CACTC,SAAS,CAAE,QADF,CARY,CAAL,EAApB,CAaA,GAAMC,CAAAA,MAAM,CAAG,o4BAoCbC,IApCa,EAAf,CAsCA,GAAMC,CAAAA,KAAK,CAAG,k0CA2CZD,IA3CY,EAAd,CA6CA,GAAME,CAAAA,QAAQ,CAAG,0nBAwBfF,IAxBe,EAAjB,C,GA2BMG,CAAAA,Y,oTACgB,CAClBC,UAAU,CAAC,iBAAMxB,CAAAA,KAAK,CAACyB,YAAN,EAAN,EAAD,CAA6B,CAA7B,CAAV,CACD,C,uCACQ,IACCC,CAAAA,OADD,CACa,KAAKC,KADlB,CACCD,OADD,CAEP,MACE,qBAAC,IAAD,EAAM,SAAS,KAAf,EACE,oBAAC,IAAD,EAAM,IAAI,KAAV,CAAW,EAAE,CAAE,CAAf,EACE,oBAAC,KAAD,EAAO,SAAS,CAAEA,OAAO,CAACb,KAA1B,EACE,8BAAI,oBAAC,OAAD,MAAJ,CADF,CADF,CADF,CAME,oBAAC,IAAD,EAAM,IAAI,KAAV,CAAW,EAAE,CAAE,EAAf,EACE,oBAAC,KAAD,EAAO,SAAS,CAAEa,OAAO,CAACb,KAA1B,EACE,oBAAC,IAAD,MACE,8EADF,mjBAQE,8BARF,CASE,8BATF,CAUE,8BACE,qGADF,CAEE,oKAFF,CAIE,wFAJF,CAKE,oHALF,CAVF,CAiBE,8BAjBF,CAmBE,sCAnBF,CAoBE,8BACE,2IADF,CAEE,uKAFF,CAGE,8EAHF,CAIE,mKAJF,CAKE,wGALF,CAME,8BANF,CApBF,CA4BE,8BA5BF,CA6BE,sCA7BF,CA8BE,8BACE,iGADF,CAEE,uMAFF,CAIE,wNAJF,CA9BF,CAsCE,8BAtCF,CAuCE,0DAvCF,2LA0CE,8BA1CF,CA2CE,8BA3CF,4PA8CE,8BA9CF,CA+CE,8BA/CF,6dAqDE,8BArDF,CAsDE,8BAtDF,CAuDE,oEAvDF,CAwDE,8BACE,kGADF,CAEE,sDAFF,CAxDF,CA4DE,8BA5DF,CA6DE,0EA7DF,CA8DE,8BACE,mHADF,CAEE,2FAFF,CAGE,4EAHF,CAIE,iJAJF,CA9DF,CAoEE,8BApEF,CAqEE,qEArEF,CAsEE,8BACE,8BAAI,2CAAJ,iHADF,CAGE,8BAAI,uEAAJ,qFAHF,CAKE,8BAAI,iDAAJ,qJALF,CAOE,8BAAI,kDAAJ,CAPF,CAQE,8BAAI,2CAAJ,+EARF,CAtEF,CAgFE,8BAhFF,CAiFE,gFAjFF,CAkFE,8BAlFF,oDAoFE,iCACE,8BACE,+CADF,CAEE,8CAFF,CADF,CAKE,8BACE,kIADF,CAEE,iLAFF,CALF,CAUE,8BACE,iFADF,CAEE,sDAFF,CAVF,CAcE,8BACE,yJADF,CAGE,gKAHF,CAdF,CApFF,CAyGE,8BAzGF,CA0GE,8IA1GF,CA2GE,8BA3GF,CA6GE,0DA7GF,4IAgHE,8BACE,8BAAI,8CAAJ,0EADF,CAEE,8BAAI,8CAAJ,qEAFF,CAGE,8BAAI,kDAAJ,4EAHF,CAhHF,CAqHE,8BArHF,CAuHE,+EAvHF,CAwHE,8BACE,8BAAI,wCAAJ,6JADF,CAGE,8BAAI,8CAAJ,8MAHF,CAME,8BAAI,4CAAJ,iOANF,CAxHF,CAkIE,8BAlIF,CAoIE,2CApIF,CAqIE,8BACE,0FADF,CAEE,yHAFF,CAGE,gGAHF,CAIE,sIAJF,CAKE,uHALF,CAME,oKANF,CAQE,6JARF,CAUE,+LAVF,CArIF,CAkJE,8BAlJF,CAoJE,8CApJF,CAqJE,8BACE,+JADF,CAGE,+MAHF,CArJF,CA2JE,8BA3JF,CA6JE,wCA7JF,CA8JE,2DA9JF,CA+JE,8BA/JF,CAgKE,8BAhKF,CAkKE,2BAAK,KAAK,CAAEN,MAAZ,EACE,oBAAC,SAAD,EACE,IAAI,CAAEY,MADR,CAEE,QAAQ,CAAC,IAFX,CAGE,OAAO,CAAE,CAAC,cAAD,CAHX,EADF,CAlKF,CAyKE,8BAzKF,CA2KE,gDA3KF,CA4KE,2BAAK,KAAK,CAAEZ,MAAZ,EACE,oBAAC,SAAD,EACE,IAAI,CAAEc,KADR,CAEE,QAAQ,CAAC,IAFX,CAGE,OAAO,CAAE,CAAC,cAAD,CAHX,EADF,CA5KF,CAmLE,8BAnLF,CAqLE,wCArLF,CAsLE,2BAAK,KAAK,CAAEd,MAAZ,EACE,oBAAC,SAAD,EACE,IAAI,CAAEe,QADR,CAEE,QAAQ,CAAC,IAFX,CAGE,OAAO,CAAE,CAAC,cAAD,CAHX,EADF,CAtLF,CADF,CADF,CANF,CADF,CA2MD,C,0BAjNwBvB,S,EAoN3B,cAAgBI,CAAAA,UAAU,CAACQ,MAAD,CAAV,CAAmBY,YAAnB,CAAhB","sourcesContent":["import React, { Component } from 'react';\nimport Prism from \"prismjs\"\nimport { Grid, Paper, withStyles, List } from \"@material-ui/core\";\n\nimport '../../ReactJs/styles.css'\nimport Sidebar from '../sidebar';\nimport PrismCode from '../../ReactJs/prismCode';\n\n\nconst titles = { backgroundColor: '#F0F8FF', padding: '1px', fontSize: '16px' }\n\nconst styles = theme => ({\n  paper: {\n    margin: theme.spacing(1),\n    padding: theme.spacing(1)\n  },\n  smMargin: {\n    margin: theme.spacing(1)\n  },\n  actionDiv: {\n    textAlign: \"center\"\n  }\n})\n\nconst rnmFor = `\nimport pandas as pd\nimport seaborn as sn\nfrom sklearn.datasets import load_digits\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix \nimport matplotlib.pyplot as plt\n%matplotlib inline\n\ndigits = load_digits()\ndir(digits)\n\nplt.gray() \nfor i in range(4):\n    plt.matshow(digits.images[i]) \n    \ndf = pd.DataFrame(digits.data)\ndf['target'] = digits.target\n\nX = df.drop('target',axis='columns')\ny = df.target\n\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2)\nmodel = RandomForestClassifier(n_estimators=20)\nmodel.fit(X_train, y_train)\nmodel.score(X_test, y_test)\n\ny_predicted = model.predict(X_test)\ncm = confusion_matrix(y_test, y_predicted)\n\nplt.figure(figsize=(10,7))\nsn.heatmap(cm, annot=True)\n\nplt.xlabel('Predicted')\nplt.ylabel('Truth')\n`.trim();\n\nconst stack = `\nimport numpy as np\nfrom collections import Counter\nfrom .decision_tree import DecisionTree\n\n\ndef bootstrap_sample(X, y):\n    n_samples = X.shape[0]\n    idxs = np.random.choice(n_samples, n_samples, replace=True)\n    return X[idxs], y[idxs]\n\n\ndef most_common_label(y):\n    counter = Counter(y)\n    most_common = counter.most_common(1)[0][0]\n    return most_common\n\n\nclass RandomForest:\n    def __init__(self, n_trees=10, min_samples_split=2, max_depth=100, n_feats=None):\n        self.n_trees = n_trees\n        self.min_samples_split = min_samples_split\n        self.max_depth = max_depth\n        self.n_feats = n_feats\n        self.trees = []\n\n    def fit(self, X, y):\n        self.trees = []\n        for _ in range(self.n_trees):\n            tree = DecisionTree(\n                min_samples_split=self.min_samples_split,\n                max_depth=self.max_depth,\n                n_feats=self.n_feats,\n            )\n            X_samp, y_samp = bootstrap_sample(X, y)\n            tree.fit(X_samp, y_samp)\n            self.trees.append(tree)\n\n    def predict(self, X):\n        tree_preds = np.array([tree.predict(X) for tree in self.trees])\n        tree_preds = np.swapaxes(tree_preds, 0, 1)\n        y_pred = [most_common_label(tree_pred) for tree_pred in tree_preds]\n        return np.array(y_pred)\n`.trim();\n\nconst testings = `\nif __name__ == \"__main__\":\n    from sklearn import datasets\n    from sklearn.model_selection import train_test_split\n\n    def accuracy(y_true, y_pred):\n        accuracy = np.sum(y_true == y_pred) / len(y_true)\n        return accuracy\n\n    data = datasets.load_breast_cancer()\n    X = data.data\n    y = data.target\n\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, random_state=1234\n    )\n\n    clf = RandomForest(n_trees=3, max_depth=10)\n\n    clf.fit(X_train, y_train)\n    y_pred = clf.predict(X_test)\n    acc = accuracy(y_test, y_pred)\n\n    print(\"Accuracy:\", acc)\n`.trim();\n\n\nclass RandomForest extends Component {\n  componentDidMount() {\n    setTimeout(() => Prism.highlightAll(), 0)\n  }\n  render() {\n    const { classes } = this.props;\n    return (\n      <Grid container>\n        <Grid item xs={2}>\n          <Paper className={classes.paper}>\n            <h4><Sidebar /></h4>\n          </Paper>\n        </Grid>\n        <Grid item xs={10}>\n          <Paper className={classes.paper}>\n            <List>\n              <h3>Random Forest (supervised learning algorithm)</h3>\n              Random forest is used for both classification as well as regression. But\n              however, it is mainly used for classification problems. As we know that a forest is made up of trees and more trees\n              means more robust forest. Similarly, random forest algorithm creates decision trees on data samples and then gets the\n              prediction from each of them and finally selects the best solution by means of voting. It is an ensemble method which\n              is better than a single decision tree because it reduces the over-fitting by averaging the result. Working of Random\n              Forest Algorithm.\n              <br />\n              <br />\n              <ul>\n                <li>1. Start with the selection of random samples from a given dataset. </li>\n                <li>2. This algorithm will construct a decision tree for every sample. Then it will get the prediction result from\n                  every decision tree.</li>\n                <li>3. Voting will be performed for every predicted result.</li>\n                <li>4. At last, select the most voted prediction result as the final prediction result.</li>\n              </ul>\n              <br />\n\n              <b>Pros: </b>\n              <ul>\n                <li>It overcomes the problem of overfitting by averaging or combining the results of different decision trees.</li>\n                <li>Work well for a large range of data items than a single decision tree does. Random forest has less variance then single decision tree.</li>\n                <li>Very flexible and possess very high accuracy.</li>\n                <li>Scaling of data does not require in random forest algorithm. It maintains good accuracy even after providing data without scaling.</li>\n                <li>Maintains good accuracy even a large proportion of the data is missing.</li>\n                <li></li>\n              </ul>\n              <br />\n              <b>Cons: </b>\n              <ul>\n                <li>Complexity is the main disadvantage of Random forest algorithms.</li>\n                <li>Construction of Random forests are much harder and time-consuming than decision trees. More computational resources\n                  are required to implement Random Forest algorithm.</li>\n                <li>It is less intuitive in case when we have a large collection of decision trees. The prediction process\n                  using random forests is very time-consuming in comparison with other algorithms.</li>\n              </ul>\n\n              <br />\n              <h3>Random Forest(Supervised)</h3>\n              Used widely in Classification and Regression problems.It builds decision trees on different samples and\n              takes their majority vote for classification and average in case of regression.\n              <br />\n              <br />\n              One of the most important features of the Random Forest Algorithm is that it can handle the data set\n              containing continuous variables in regression and categorical variables in classification(better results).             for classification problems.\n              <br />\n              <br />\n              A student named X wants to choose a course after his 10+2, and he is confused about the choice of\n              course based on his skill set. So he decides to consult various people like his cousins, teachers,\n              parents, degree students, and working people. He asks them varied questions like why he should choose,\n              job opportunities with that course, course fee, etc. Finally, after consulting various people about the\n              course he decides to take the course suggested by most of the people.\n              <br />\n              <br />\n              <b>Ensemble uses two types of methods: </b>\n              <ul>\n                <li>Bagging (Parallel), Random forest works on the Bagging principle.</li>\n                <li>Boosting (Sequential)</li>\n              </ul>\n              <br />\n              <b>Steps involved in random forest algorithm:</b>\n              <ul>\n                <li>n number of random records are taken from the data set having k number of records.</li>\n                <li>Individual decision trees are constructed for each sample.</li>\n                <li>Each decision tree will generate an output.</li>\n                <li>Final output is considered based on Majority Voting or Averaging for Classification and regression respectively.</li>\n              </ul>\n              <br />\n              <b>Important Features of Random Forest: </b>\n              <ul>\n                <li><b>Diversity: </b>Not all attributes/variables/features are considered while making an individual\n                  tree, each tree is different.</li>\n                <li><b>Immune to the curse of dimensionality: </b>Since each tree does not consider all the features,\n                  the feature space is reduced.</li>\n                <li><b>Parallelization: </b>Each tree is created independently out of different data and attributes.\n                  This means that we can make full use of the CPU to build random forests.</li>\n                <li><b>Train-Test split: </b></li>\n                <li><b>Stability: </b>Stability arises because the result is based on majority voting/ averaging.</li>\n              </ul>\n              <br />\n              <b>Difference Between Decision Tree & Random Forest</b>\n              <br />\n              Random forest is a collection of decision trees.\n              <table>\n                <tr>\n                  <th>Decision trees</th>\n                  <th>Random Forest</th>\n                </tr>\n                <tr>\n                  <td>Normally suffer from the problem of overfitting if it’s allowed to grow without any control.</td>\n                  <td>Created from subsets of data and the final output is based on average or majority ranking and\n                    hence the problem of overfitting is taken care of.</td>\n                </tr>\n                <tr>\n                  <td>A single decision tree is faster in computation.</td>\n                  <td>Comparatively slower.</td>\n                </tr>\n                <tr>\n                  <td>When a data set with features is taken as input by a decision tree it will formulate some set\n                    of rules to do prediction.</td>\n                  <td>Randomly selects observations, builds a decision tree and the average result is taken. It\n                    doesn’t use any set of formulas.</td>\n                </tr>\n              </table>\n              <br />\n              <i>Thus random forests are much more successful than decision trees only if the trees are diverse and acceptable.</i>\n              <br />\n\n              <h3>Important Hyperparameters</h3>\n              Hyperparameters are used in random forests to either enhance the performance and predictive power of models or\n              to make the model faster.\n              <ul>\n                <li><b>n_estimators: </b>number of trees the algorithm builds before averaging the predictions.</li>\n                <li><b>max_features: </b>max. number of features random forest considers splitting a node.</li>\n                <li><b>mini_sample_leaf: </b>determines the min. number of leaves required to split an internal node.</li>\n              </ul>\n              <br />\n\n              <b>Following hyperparameters increases the speed: </b>\n              <ul>\n                <li><b>n_jobs: </b> It tells the engine how many processors it is allowed to use. If the value is 1,\n                  it can use only one processor but if the value is -1 there is no limit.</li>\n                <li><b>random_state: </b>Controls randomness of the sample. The model will always produce the same\n                  results if it has a definite value of random state and if it has been given the same hyperparameters\n                  and the same training data.</li>\n                <li><b>oob_score : </b>OOB means out of the bag. It is a random forest cross-validation method. In\n                  this one-third of the sample is not used to train the data instead used to evaluate its performance.\n                  These samples are called out of bag samples.</li>\n              </ul>\n              <br />\n\n              <b>Advantages:</b>\n              <ul>\n                <li>It can be used in classification and regression problems.</li>\n                <li>It solves the problem of overfitting as output is based on majority voting or averaging.</li>\n                <li>It performs well even if the data contains null/missing values.</li>\n                <li>Each decision tree created is independent of the other thus it shows the property of parallelization.</li>\n                <li>It is highly stable as the average answers given by a large number of trees are taken.</li>\n                <li>It maintains diversity as all the attributes are not considered while making each decision tree\n                  though it is not true in all cases.</li>\n                <li>It is immune to the curse of dimensionality. Since each tree does not consider all the attributes,\n                  feature space is reduced.</li>\n                <li>We don’t have to segregate data into train and test as there will always be 30% of the data which\n                  is not seen by the decision tree made out of bootstrap.</li>\n              </ul>\n              <br />\n\n              <b>Disadvantages:</b>\n              <ul>\n                <li>Random forest is highly complex when compared to decision trees where decisions can be made by following\n                  the path of the tree.</li>\n                <li>Training time is more compared to other models due to its complexity. Whenever it has to make a prediction\n                  each decision tree has to generate output for the given input data.</li>\n              </ul>\n              <br />\n\n              <h3>Example</h3>\n              <b>Digits dataset from sklearn</b>\n              <br />\n              <br />\n\n              <div style={titles}>\n                <PrismCode\n                  code={rnmFor}\n                  language=\"js\"\n                  plugins={[\"line-numbers\"]}\n                />\n              </div>\n              <br />\n\n              <h3>Random Forest 2</h3>\n              <div style={titles}>\n                <PrismCode\n                  code={stack}\n                  language=\"js\"\n                  plugins={[\"line-numbers\"]}\n                />\n              </div>\n              <br />\n\n              <h3>Testing</h3>\n              <div style={titles}>\n                <PrismCode\n                  code={testings}\n                  language=\"js\"\n                  plugins={[\"line-numbers\"]}\n                />\n              </div>\n            </List>\n          </Paper>\n        </Grid>\n      </Grid>\n    )\n  }\n}\n\nexport default (withStyles(styles)(RandomForest));\n"]},"metadata":{},"sourceType":"module"}