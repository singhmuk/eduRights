{"map":{"version":3,"sources":["static/js/24.905cb1cd.chunk.js"],"names":["this","push","344","module","__webpack_exports__","__webpack_require__","d","Sidebar","react__WEBPACK_IMPORTED_MODULE_0__","react__WEBPACK_IMPORTED_MODULE_0___default","n","react_router_dom__WEBPACK_IMPORTED_MODULE_1__","_material_ui_core_MenuItem__WEBPACK_IMPORTED_MODULE_2__","_material_ui_core_MenuList__WEBPACK_IMPORTED_MODULE_3__","_material_ui_core_styles__WEBPACK_IMPORTED_MODULE_4__","useStyles","Object","theme","root","display","paper","marginRight","spacing","line","textDecoration","classes","a","createElement","className","to","356","exports","p","466","r","_home_mukeshs_Projects_edurights_node_modules_babel_runtime_helpers_esm_classCallCheck__WEBPACK_IMPORTED_MODULE_0__","_home_mukeshs_Projects_edurights_node_modules_babel_runtime_helpers_esm_createClass__WEBPACK_IMPORTED_MODULE_1__","_home_mukeshs_Projects_edurights_node_modules_babel_runtime_helpers_esm_possibleConstructorReturn__WEBPACK_IMPORTED_MODULE_2__","_home_mukeshs_Projects_edurights_node_modules_babel_runtime_helpers_esm_getPrototypeOf__WEBPACK_IMPORTED_MODULE_3__","_home_mukeshs_Projects_edurights_node_modules_babel_runtime_helpers_esm_inherits__WEBPACK_IMPORTED_MODULE_4__","react__WEBPACK_IMPORTED_MODULE_5__","react__WEBPACK_IMPORTED_MODULE_5___default","prismjs__WEBPACK_IMPORTED_MODULE_6__","prismjs__WEBPACK_IMPORTED_MODULE_6___default","_material_ui_core__WEBPACK_IMPORTED_MODULE_7__","_material_ui_core__WEBPACK_IMPORTED_MODULE_8__","_material_ui_core__WEBPACK_IMPORTED_MODULE_9__","_material_ui_core__WEBPACK_IMPORTED_MODULE_10__","_sidebar__WEBPACK_IMPORTED_MODULE_12__","_ReactJs_prismCode__WEBPACK_IMPORTED_MODULE_13__","_assets_AI_nn_png__WEBPACK_IMPORTED_MODULE_14__","_assets_AI_nn_png__WEBPACK_IMPORTED_MODULE_14___default","titles","backgroundColor","padding","fontSize","redesign","height","width","childsFile","trim","keras","weights","prediction","descent","implementse","GradientNeural","_Component","apply","arguments","key","value","setTimeout","highlightAll","props","container","item","xs","style","code","language","plugins","src","alt","margin","smMargin","actionDiv","textAlign"],"mappings":"CAACA,KAAK,iCAAmCA,KAAK,kCAAoC,IAAIC,KAAK,CAAC,CAAC,IAAI,CAE3FC,IACA,SAAUC,EAAQC,EAAqBC,GAE7C,aAC+BA,EAAoBC,EAAEF,EAAqB,KAAK,WAAa,OAAOG,KAC9E,IAAIC,EAAqCH,EAAoB,GACzDI,EAA0DJ,EAAoBK,EAAEF,GAChFG,EAAgDN,EAAoB,IACpEO,EAA0DP,EAAoB,IAC9EQ,EAA0DR,EAAoB,IAC9ES,EAAwDT,EAAoB,IACjGU,EAAUC,OAAOF,EAAuE,EAA9EE,EAAiF,SAASC,GAAO,MAAM,CAACC,KAAK,CAACC,QAAQ,QAAQC,MAAM,CAACC,YAAYJ,EAAMK,QAAQ,IAAIC,KAAK,CAACC,eAAe,YAAY,SAASjB,IAAU,IAAIkB,EAAQV,IAAY,OAAON,EAA2CiB,EAAEC,cAAc,MAAM,CAACC,UAAUH,EAAQP,MAAMT,EAA2CiB,EAAEC,cAAcd,EAAyE,EAAE,KAAKJ,EAA2CiB,EAAEC,cAAcf,EAAyE,EAAE,KAAKH,EAA2CiB,EAAEC,cAAchB,EAA4D,EAAE,CAACkB,GAAG,gBAAgBD,UAAUH,EAAQF,MAAM,OAAOd,EAA2CiB,EAAEC,cAAcf,EAAyE,EAAE,KAAKH,EAA2CiB,EAAEC,cAAchB,EAA4D,EAAE,CAACkB,GAAG,cAAcD,UAAUH,EAAQF,MAAM,eAAed,EAA2CiB,EAAEC,cAAcf,EAAyE,EAAE,KAAKH,EAA2CiB,EAAEC,cAAchB,EAA4D,EAAE,CAACkB,GAAG,WAAWD,UAAUH,EAAQF,MAAM,iBAAiBd,EAA2CiB,EAAEC,cAAcf,EAAyE,EAAE,KAAKH,EAA2CiB,EAAEC,cAAchB,EAA4D,EAAE,CAACkB,GAAG,eAAeD,UAAUH,EAAQF,MAAM,aAAad,EAA2CiB,EAAEC,cAAcf,EAAyE,EAAE,KAAKH,EAA2CiB,EAAEC,cAAchB,EAA4D,EAAE,CAACkB,GAAG,UAAUD,UAAUH,EAAQF,MAAM,gBAAgBd,EAA2CiB,EAAEC,cAAcf,EAAyE,EAAE,KAAKH,EAA2CiB,EAAEC,cAAchB,EAA4D,EAAE,CAACkB,GAAG,uBAAuBD,UAAUH,EAAQF,MAAM,oBAAoBd,EAA2CiB,EAAEC,cAAcf,EAAyE,EAAE,KAAKH,EAA2CiB,EAAEC,cAAchB,EAA4D,EAAE,CAACkB,GAAG,QAAQD,UAAUH,EAAQF,MAAM,SAASd,EAA2CiB,EAAEC,cAAcf,EAAyE,EAAE,KAAKH,EAA2CiB,EAAEC,cAAchB,EAA4D,EAAE,CAACkB,GAAG,kBAAkBD,UAAUH,EAAQF,MAAM,mBAAmBd,EAA2CiB,EAAEC,cAAcf,EAAyE,EAAE,KAAKH,EAA2CiB,EAAEC,cAAchB,EAA4D,EAAE,CAACkB,GAAG,cAAcD,UAAUH,EAAQF,MAAM,eAAed,EAA2CiB,EAAEC,cAAcf,EAAyE,EAAE,KAAKH,EAA2CiB,EAAEC,cAAchB,EAA4D,EAAE,CAACkB,GAAG,gBAAgBD,UAAUH,EAAQF,MAAM,iBAAiBd,EAA2CiB,EAAEC,cAAcf,EAAyE,EAAE,KAAKH,EAA2CiB,EAAEC,cAAchB,EAA4D,EAAE,CAACkB,GAAG,YAAYD,UAAUH,EAAQF,MAAM,aAAad,EAA2CiB,EAAEC,cAAcf,EAAyE,EAAE,KAAKH,EAA2CiB,EAAEC,cAAchB,EAA4D,EAAE,CAACkB,GAAG,sBAAsBD,UAAUH,EAAQF,MAAM,wBAAwBd,EAA2CiB,EAAEC,cAAcf,EAAyE,EAAE,KAAKH,EAA2CiB,EAAEC,cAAchB,EAA4D,EAAE,CAACkB,GAAG,cAAcD,UAAUH,EAAQF,MAAM,eAAed,EAA2CiB,EAAEC,cAAcf,EAAyE,EAAE,KAAKH,EAA2CiB,EAAEC,cAAchB,EAA4D,EAAE,CAACkB,GAAG,eAAeD,UAAUH,EAAQF,MAAM,gBAAgBd,EAA2CiB,EAAEC,cAAcf,EAAyE,EAAE,KAAKH,EAA2CiB,EAAEC,cAAchB,EAA4D,EAAE,CAACkB,GAAG,kBAAkBD,UAAUH,EAAQF,MAAM,mBAAmBd,EAA2CiB,EAAEC,cAAcf,EAAyE,EAAE,KAAKH,EAA2CiB,EAAEC,cAAchB,EAA4D,EAAE,CAACkB,GAAG,qBAAqBD,UAAUH,EAAQF,MAAM,sBAAsBd,EAA2CiB,EAAEC,cAAcf,EAAyE,EAAE,KAAKH,EAA2CiB,EAAEC,cAAchB,EAA4D,EAAE,CAACkB,GAAG,YAAYD,UAAUH,EAAQF,MAAM,aAAad,EAA2CiB,EAAEC,cAAcf,EAAyE,EAAE,KAAKH,EAA2CiB,EAAEC,cAAchB,EAA4D,EAAE,CAACkB,GAAG,kBAAkBD,UAAUH,EAAQF,MAAM,eAAed,EAA2CiB,EAAEC,cAAc,MAAM,SAItpMG,IACA,SAAU3B,EAAQ4B,EAAS1B,GAEjCF,EAAO4B,QAAU1B,EAAoB2B,EAAI,gCAInCC,IACA,SAAU9B,EAAQC,EAAqBC,GAE7C,aACAA,EAAoB6B,EAAE9B,GACD,IAAI+B,EAAsH9B,EAAoB,GAC1I+B,EAAmH/B,EAAoB,GACvIgC,EAAiIhC,EAAoB,GACrJiC,EAAsHjC,EAAoB,GAC1IkC,EAAgHlC,EAAoB,GACpImC,EAAqCnC,EAAoB,GACzDoC,EAA0DpC,EAAoBK,EAAE8B,GAChFE,EAAuCrC,EAAoB,GAC3DsC,EAA4DtC,EAAoBK,EAAEgC,GAClFE,EAAiDvC,EAAoB,GACrEwC,EAAiDxC,EAAoB,GACrEyC,EAAiDzC,EAAoB,IACrE0C,EAAkD1C,EAAoB,IAGtE2C,GAFoD3C,EAAoB,IAE/BA,EAAoB,MAC7D4C,EAAmD5C,EAAoB,GACvE6C,EAAkD7C,EAAoB,KACtE8C,EAAuE9C,EAAoBK,EAAEwC,GAClHE,EAAO,CAACC,gBAAgB,UAAUC,QAAQ,MAAMC,SAAS,QAAYC,EAAS,CAACC,OAAO,IAAIC,MAAM,KAAgLC,EAAW,qpBAAqpBC,OAAWC,EAAM,6bAA6bD,OAAWE,EAAQ,wIAAwIF,OAAWG,EAAW,qNAAqNH,OAAWI,EAAQ,maAAmaJ,OAAWK,EAAY,s4BAAs4BL,OAAWM,EAA4B,SAASC,GAAgL,SAASD,IAAqL,OAApKlD,OAAOmB,EAAqI,EAA5InB,CAA+IhB,KAAKkE,GAAuBlD,OAAOqB,EAAgJ,EAAvJrB,CAA0JhB,KAAKgB,OAAOsB,EAAqI,EAA5ItB,CAA+IkD,GAAgBE,MAAMpE,KAAKqE,YAA03M,OAA5iOrD,OAAOuB,EAA+H,EAAtIvB,CAAyIkD,EAAeC,GAAuiBnD,OAAOoB,EAAkI,EAAzIpB,CAA4IkD,EAAe,CAAC,CAACI,IAAI,oBAAoBC,MAAM,WAA6BC,YAAW,WAAW,OAAO7B,EAA6CjB,EAAE+C,iBAAiB,KAAM,CAACH,IAAI,SAASC,MAAM,WAAkB,IAAI9C,EAAQzB,KAAK0E,MAAMjD,QAAQ,OAAOgB,EAA2Cf,EAAEC,cAAciB,EAAgE,EAAE,CAAC+B,WAAU,GAAMlC,EAA2Cf,EAAEC,cAAciB,EAAgE,EAAE,CAACgC,MAAK,EAAKC,GAAG,GAAGpC,EAA2Cf,EAAEC,cAAckB,EAAgE,EAAE,CAACjB,UAAUH,EAAQL,OAAOqB,EAA2Cf,EAAEC,cAAc,KAAK,KAAKc,EAA2Cf,EAAEC,cAAcqB,EAAwD,EAAE,SAASP,EAA2Cf,EAAEC,cAAciB,EAAgE,EAAE,CAACgC,MAAK,EAAKC,GAAG,IAAIpC,EAA2Cf,EAAEC,cAAckB,EAAgE,EAAE,CAACjB,UAAUH,EAAQL,OAAOqB,EAA2Cf,EAAEC,cAAcmB,EAAgE,EAAE,KAAKL,EAA2Cf,EAAEC,cAAc,KAAK,KAAK,0EAA0E,6HAA6Hc,EAA2Cf,EAAEC,cAAc,KAAK,MAAMc,EAA2Cf,EAAEC,cAAc,KAAK,MAAMc,EAA2Cf,EAAEC,cAAc,IAAI,KAAK,8FAA8Fc,EAA2Cf,EAAEC,cAAc,KAAK,MAAMc,EAA2Cf,EAAEC,cAAc,KAAK,MAAMc,EAA2Cf,EAAEC,cAAc,MAAM,CAACmD,MAAM1B,GAAQX,EAA2Cf,EAAEC,cAAcsB,EAAkE,EAAE,CAAC8B,KAAKpB,EAAWqB,SAAS,KAAKC,QAAQ,CAAC,mBAAmBxC,EAA2Cf,EAAEC,cAAc,KAAK,MAAMc,EAA2Cf,EAAEC,cAAc,KAAK,MAAMc,EAA2Cf,EAAEC,cAAc,IAAI,KAAK,qRAAqRc,EAA2Cf,EAAEC,cAAc,KAAK,MAAMc,EAA2Cf,EAAEC,cAAc,KAAK,MAAMc,EAA2Cf,EAAEC,cAAc,MAAM,CAACuD,IAAI/B,EAAwDzB,EAAEyD,IAAI,SAASvD,UAAU,cAAckD,MAAMtB,IAAWf,EAA2Cf,EAAEC,cAAc,KAAK,MAAMc,EAA2Cf,EAAEC,cAAc,KAAK,MAAMc,EAA2Cf,EAAEC,cAAc,MAAM,CAACmD,MAAM1B,GAAQX,EAA2Cf,EAAEC,cAAcsB,EAAkE,EAAE,CAAC8B,KAAKlB,EAAMmB,SAAS,KAAKC,QAAQ,CAAC,mBAAmBxC,EAA2Cf,EAAEC,cAAc,KAAK,MAAMc,EAA2Cf,EAAEC,cAAc,KAAK,KAAK,wDAAwDc,EAA2Cf,EAAEC,cAAc,MAAM,CAACmD,MAAM1B,GAAQX,EAA2Cf,EAAEC,cAAcsB,EAAkE,EAAE,CAAC8B,KAAKjB,EAAQkB,SAAS,KAAKC,QAAQ,CAAC,mBAAmBxC,EAA2Cf,EAAEC,cAAc,KAAK,MAAMc,EAA2Cf,EAAEC,cAAc,KAAK,KAAK,yFAAyFc,EAA2Cf,EAAEC,cAAc,MAAM,CAACmD,MAAM1B,GAAQX,EAA2Cf,EAAEC,cAAcsB,EAAkE,EAAE,CAAC8B,KAAKhB,EAAWiB,SAAS,KAAKC,QAAQ,CAAC,mBAAmBxC,EAA2Cf,EAAEC,cAAc,KAAK,MAAMc,EAA2Cf,EAAEC,cAAc,KAAK,MAAMc,EAA2Cf,EAAEC,cAAc,IAAI,KAAK,2PAA2Pc,EAA2Cf,EAAEC,cAAc,KAAK,MAAMc,EAA2Cf,EAAEC,cAAc,KAAK,MAAMc,EAA2Cf,EAAEC,cAAc,IAAI,KAAK,uEAAuEc,EAA2Cf,EAAEC,cAAc,KAAK,MAAMc,EAA2Cf,EAAEC,cAAc,MAAM,CAACmD,MAAM1B,GAAQX,EAA2Cf,EAAEC,cAAcsB,EAAkE,EAAE,CAAC8B,KAAKf,EAAQgB,SAAS,KAAKC,QAAQ,CAAC,mBAAmBxC,EAA2Cf,EAAEC,cAAc,KAAK,MAAMc,EAA2Cf,EAAEC,cAAc,KAAK,MAAMc,EAA2Cf,EAAEC,cAAc,IAAI,KAAK,iFAAiFc,EAA2Cf,EAAEC,cAAc,MAAM,CAACmD,MAAM1B,GAAQX,EAA2Cf,EAAEC,cAAcsB,EAAkE,EAAE,CAAC8B,KAAKd,EAAYe,SAAS,KAAKC,QAAQ,CAAC,mBAAmBxC,EAA2Cf,EAAEC,cAAc,KAAK,MAAMc,EAA2Cf,EAAEC,cAAc,IAAI,KAAK,kKAA0KuC,EAAxkO,CAAylO1B,EAA8C,WAAgCpC,EAA6B,QAAKY,OAAO+B,EAAiE,EAAxE/B,EAAvtU,SAAgBC,GAAO,MAAM,CAACG,MAAM,CAACgE,OAAOnE,EAAMK,QAAQ,GAAGgC,QAAQrC,EAAMK,QAAQ,IAAI+D,SAAS,CAACD,OAAOnE,EAAMK,QAAQ,IAAIgE,UAAU,CAACC,UAAU,aAAukU,CAAoFrB"},"code":"(this[\"webpackJsonpmern-stack-client\"]=this[\"webpackJsonpmern-stack-client\"]||[]).push([[24],{344:function(e,n,t){\"use strict\";t.d(n,\"a\",(function(){return m}));var a=t(0),l=t.n(a),r=t(12),i=t(14),s=t(50),c=t(49),o=Object(c.a)((function(e){return{root:{display:\"flex\"},paper:{marginRight:e.spacing(2)},line:{textDecoration:\"none\"}}}));function m(){var e=o();return l.a.createElement(\"div\",{className:e.root},l.a.createElement(s.a,null,l.a.createElement(i.a,null,l.a.createElement(r.b,{to:\"/introAngular\",className:e.line},\"AI\")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:\"/tensorflow\",className:e.line},\"Tensorflow\")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:\"/tensors\",className:e.line},\"Tensorboards\")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:\"/angCompiler\",className:e.line},\"Compiler\")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:\"/neural\",className:e.line},\"NeuralKeras\")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:\"/activationFunctions\",className:e.line},\"activationFuncs\")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:\"/loss\",className:e.line},\"Loss\")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:\"/gradientNeural\",className:e.line},\"GradientNeural\")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:\"/stochastic\",className:e.line},\"Stochastic\")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:\"/benchmarking\",className:e.line},\"Benchmarking\")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:\"/customer\",className:e.line},\"Customer\")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:\"/regularizationDeep\",className:e.line},\"Regularization Deep\")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:\"/imbalanced\",className:e.line},\"Imbalanced\")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:\"/imbalanced2\",className:e.line},\"Imbalanced2\")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:\"/convolutionals\",className:e.line},\"Convolutionals\")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:\"/data_augmentation\",className:e.line},\"data Augmentation\")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:\"/transfer\",className:e.line},\"Transfer\")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:\"/word_embedding\",className:e.line},\"Embedding\"))),l.a.createElement(\"div\",null))}},356:function(e,n,t){e.exports=t.p+\"static/media/nn.73ebaea6.png\"},466:function(e,n,t){\"use strict\";t.r(n);var a=t(4),l=t(3),r=t(5),i=t(6),s=t(8),c=t(0),o=t.n(c),m=t(7),d=t.n(m),u=t(2),p=t(9),g=t(13),b=t(11),_=(t(16),t(344)),E=t(1),f=t(356),h=t.n(f),w={backgroundColor:\"#F0F8FF\",padding:\"1px\",fontSize:\"16px\"},y={height:200,width:500},v=\"\\nimport numpy as np\\nimport pandas as pd\\nimport tensorflow as tf\\nfrom tensorflow import keras\\nfrom sklearn.model_selection import train_test_split\\nfrom matplotlib import pyplot as plt\\n%matplotlib inline\\n\\ndf = pd.read_csv(\\\"insurance_data.csv\\\")\\n\\nX_train, X_test, y_train, y_test = train_test_split(df[['age','affordibility']],df.bought_insurance,test_size=0.2, \\n    random_state=25)\\n\\n\\n#Preprocessing: Scale the data so both age and affordibility are in same scaling range.\\nX_train_scaled = X_train.copy()\\nX_train_scaled['age'] = X_train_scaled['age'] / 100\\n\\nX_test_scaled = X_test.copy()\\nX_test_scaled['age'] = X_test_scaled['age'] / 100\\n\".trim(),N=\"\\nmodel = keras.Sequential([\\n  keras.layers.Dense(1, input_shape=(2,), activation='sigmoid', kernel_initializer='ones', bias_initializer='zeros')\\n])\\n\\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\\nmodel.fit(X_train_scaled, y_train, epochs=5000)\\n\\nmodel.evaluate(X_test_scaled,y_test)                                            #Evaluate the model on test set.\\nmodel.predict(X_test_scaled)\\n\\ny_test\\n\".trim(),k=\"\\nimport math\\n\\ncoef, intercept = model.get_weights()\\n\\ndef sigmoid(x):\\n    return 1 / (1 + math.exp(-x))\\nsigmoid(18)\\n\\nX_test\\n\".trim(),X=\"\\ndef prediction_function(age, affordibility):\\n    weighted_sum = coef[0]*age + coef[1]*affordibility + intercept\\n    return sigmoid(weighted_sum)\\n\\nprediction_function(.47, 1)\\nprediction_function(.18, 1)\\n\".trim(),j=\"\\ndef sigmoid_numpy(X):\\n   return 1/(1+np.exp(-X))\\n\\nsigmoid_numpy(np.array([12,0,1]))\\n\\ndef log_loss(y_true, y_predicted):\\n    epsilon = 1e-15\\n    y_predicted_new = [max(i,epsilon) for i in y_predicted]\\n    y_predicted_new = [min(i,1-epsilon) for i in y_predicted_new]\\n    y_predicted_new = np.array(y_predicted_new)\\n    return -np.mean(y_true*np.log(y_predicted_new)+(1-y_true)*np.log(1-y_predicted_new))\\n\".trim(),x=\"\\ndef gradient_descent(age, affordability, y_true, epochs, loss_thresold):\\n    w1 = w2 = 1\\n    bias = 0\\n    rate = 0.5\\n    n = len(age)\\n    for i in range(epochs):\\n        weighted_sum = w1 * age + w2 * affordability + bias\\n        y_predicted = sigmoid_numpy(weighted_sum)\\n        loss = log_loss(y_true, y_predicted)\\n\\n        w1d = (1/n)*np.dot(np.transpose(age),(y_predicted-y_true)) \\n        w2d = (1/n)*np.dot(np.transpose(affordability),(y_predicted-y_true)) \\n\\n        bias_d = np.mean(y_predicted-y_true)\\n        w1 = w1 - rate * w1d\\n        w2 = w2 - rate * w2d\\n        bias = bias - rate * bias_d\\n\\n        print (f'Epoch:{i}, w1:{w1}, w2:{w2}, bias:{bias}, loss:{loss}')\\n\\n        if loss<=loss_thresold:\\n            break\\n\\n    return w1, w2, bias\\n    \\ngradient_descent(X_train_scaled['age'],X_train_scaled['affordibility'],y_train,1000, 0.4631)\\n\\ncoef, intercept\\n\".trim(),z=function(e){function n(){return Object(a.a)(this,n),Object(r.a)(this,Object(i.a)(n).apply(this,arguments))}return Object(s.a)(n,e),Object(l.a)(n,[{key:\"componentDidMount\",value:function(){setTimeout((function(){return d.a.highlightAll()}),0)}},{key:\"render\",value:function(){var e=this.props.classes;return o.a.createElement(u.a,{container:!0},o.a.createElement(u.a,{item:!0,xs:2},o.a.createElement(p.a,{className:e.paper},o.a.createElement(\"h4\",null,o.a.createElement(_.a,null)))),o.a.createElement(u.a,{item:!0,xs:10},o.a.createElement(p.a,{className:e.paper},o.a.createElement(g.a,null,o.a.createElement(\"h3\",null,\"Implement Gradient Descent For Neural Network (or Logistic Regression)\"),\"An optimization algorithm used to train machine learning models by minimizing errors between predicted and actual results.\",o.a.createElement(\"br\",null),o.a.createElement(\"br\",null),o.a.createElement(\"b\",null,\"Predicting if a person would buy life insurnace based on his age using logistic regression\"),o.a.createElement(\"br\",null),o.a.createElement(\"br\",null),o.a.createElement(\"div\",{style:w},o.a.createElement(E.a,{code:v,language:\"js\",plugins:[\"line-numbers\"]})),o.a.createElement(\"br\",null),o.a.createElement(\"br\",null),o.a.createElement(\"b\",null,\"Model Building: First build a model in keras/tensorflow and see what weights and bias values it comes up with. We will than try to reproduce same weights and bias in our plain python implementation of gradient descent. Below is the architecture of our simple neural network\"),o.a.createElement(\"br\",null),o.a.createElement(\"br\",null),o.a.createElement(\"img\",{src:h.a,alt:\"Theata\",className:\"responsive2\",style:y}),o.a.createElement(\"br\",null),o.a.createElement(\"br\",null),o.a.createElement(\"div\",{style:w},o.a.createElement(E.a,{code:N,language:\"js\",plugins:[\"line-numbers\"]})),o.a.createElement(\"br\",null),o.a.createElement(\"h3\",null,\"Now get the value of weights and bias from the model\"),o.a.createElement(\"div\",{style:w},o.a.createElement(E.a,{code:k,language:\"js\",plugins:[\"line-numbers\"]})),o.a.createElement(\"br\",null),o.a.createElement(\"h3\",null,\"Instead of model.predict, write our own prediction function that uses w1,w2 and bias.\"),o.a.createElement(\"div\",{style:w},o.a.createElement(E.a,{code:X,language:\"js\",plugins:[\"line-numbers\"]})),o.a.createElement(\"br\",null),o.a.createElement(\"br\",null),o.a.createElement(\"b\",null,\"Now we start implementing gradient descent in plain python. Again the goal is to come up with same w1, w2 and bias that keras model calculated. We want to show how keras/tensorflow would have computed these values internally using gradient descent\"),o.a.createElement(\"br\",null),o.a.createElement(\"br\",null),o.a.createElement(\"i\",null,\"First write couple of helper routines such as sigmoid and log_loss.\"),o.a.createElement(\"br\",null),o.a.createElement(\"div\",{style:w},o.a.createElement(E.a,{code:j,language:\"js\",plugins:[\"line-numbers\"]})),o.a.createElement(\"br\",null),o.a.createElement(\"br\",null),o.a.createElement(\"b\",null,\"All right now comes the time to implement our final gradient descent function\"),o.a.createElement(\"div\",{style:w},o.a.createElement(E.a,{code:x,language:\"js\",plugins:[\"line-numbers\"]})),o.a.createElement(\"br\",null),o.a.createElement(\"i\",null,\"This shows that in the end we were able to come up with same value of w1,w2 and bias using a plain python implementation of gradient descent function.\")))))}}]),n}(c.Component);n.default=Object(b.a)((function(e){return{paper:{margin:e.spacing(1),padding:e.spacing(1)},smMargin:{margin:e.spacing(1)},actionDiv:{textAlign:\"center\"}}}))(z)}}]);","extractedComments":[]}