{"map":{"version":3,"sources":["static/js/25.7b028bfc.chunk.js"],"names":["this","push","344","module","__webpack_exports__","__webpack_require__","d","Sidebar","react__WEBPACK_IMPORTED_MODULE_0__","react__WEBPACK_IMPORTED_MODULE_0___default","n","react_router_dom__WEBPACK_IMPORTED_MODULE_1__","_material_ui_core_MenuItem__WEBPACK_IMPORTED_MODULE_2__","_material_ui_core_MenuList__WEBPACK_IMPORTED_MODULE_3__","_material_ui_core_styles__WEBPACK_IMPORTED_MODULE_4__","useStyles","Object","theme","root","display","paper","marginRight","spacing","line","textDecoration","classes","a","createElement","className","to","357","exports","p","467","r","_home_mukeshs_Projects_edurights_node_modules_babel_runtime_helpers_esm_classCallCheck__WEBPACK_IMPORTED_MODULE_0__","_home_mukeshs_Projects_edurights_node_modules_babel_runtime_helpers_esm_createClass__WEBPACK_IMPORTED_MODULE_1__","_home_mukeshs_Projects_edurights_node_modules_babel_runtime_helpers_esm_possibleConstructorReturn__WEBPACK_IMPORTED_MODULE_2__","_home_mukeshs_Projects_edurights_node_modules_babel_runtime_helpers_esm_getPrototypeOf__WEBPACK_IMPORTED_MODULE_3__","_home_mukeshs_Projects_edurights_node_modules_babel_runtime_helpers_esm_inherits__WEBPACK_IMPORTED_MODULE_4__","react__WEBPACK_IMPORTED_MODULE_5__","react__WEBPACK_IMPORTED_MODULE_5___default","prismjs__WEBPACK_IMPORTED_MODULE_6__","prismjs__WEBPACK_IMPORTED_MODULE_6___default","_material_ui_core__WEBPACK_IMPORTED_MODULE_7__","_material_ui_core__WEBPACK_IMPORTED_MODULE_8__","_material_ui_core__WEBPACK_IMPORTED_MODULE_9__","_material_ui_core__WEBPACK_IMPORTED_MODULE_10__","_sidebar__WEBPACK_IMPORTED_MODULE_12__","_ReactJs_prismCode__WEBPACK_IMPORTED_MODULE_13__","_assets_AI_hp_jpg__WEBPACK_IMPORTED_MODULE_14__","_assets_AI_hp_jpg__WEBPACK_IMPORTED_MODULE_14___default","titles","backgroundColor","padding","fontSize","redesign","height","width","childsFile","trim","batch","predictions","stochastics","Stochastic","_Component","apply","arguments","key","value","setTimeout","highlightAll","props","container","item","xs","style","code","language","plugins","src","alt","margin","smMargin","actionDiv","textAlign"],"mappings":"CAACA,KAAK,iCAAmCA,KAAK,kCAAoC,IAAIC,KAAK,CAAC,CAAC,IAAI,CAE3FC,IACA,SAAUC,EAAQC,EAAqBC,GAE7C,aAC+BA,EAAoBC,EAAEF,EAAqB,KAAK,WAAa,OAAOG,KAC9E,IAAIC,EAAqCH,EAAoB,GACzDI,EAA0DJ,EAAoBK,EAAEF,GAChFG,EAAgDN,EAAoB,IACpEO,EAA0DP,EAAoB,IAC9EQ,EAA0DR,EAAoB,IAC9ES,EAAwDT,EAAoB,IACjGU,EAAUC,OAAOF,EAAuE,EAA9EE,EAAiF,SAASC,GAAO,MAAM,CAACC,KAAK,CAACC,QAAQ,QAAQC,MAAM,CAACC,YAAYJ,EAAMK,QAAQ,IAAIC,KAAK,CAACC,eAAe,YAAY,SAASjB,IAAU,IAAIkB,EAAQV,IAAY,OAAON,EAA2CiB,EAAEC,cAAc,MAAM,CAACC,UAAUH,EAAQP,MAAMT,EAA2CiB,EAAEC,cAAcd,EAAyE,EAAE,KAAKJ,EAA2CiB,EAAEC,cAAcf,EAAyE,EAAE,KAAKH,EAA2CiB,EAAEC,cAAchB,EAA4D,EAAE,CAACkB,GAAG,gBAAgBD,UAAUH,EAAQF,MAAM,OAAOd,EAA2CiB,EAAEC,cAAcf,EAAyE,EAAE,KAAKH,EAA2CiB,EAAEC,cAAchB,EAA4D,EAAE,CAACkB,GAAG,cAAcD,UAAUH,EAAQF,MAAM,eAAed,EAA2CiB,EAAEC,cAAcf,EAAyE,EAAE,KAAKH,EAA2CiB,EAAEC,cAAchB,EAA4D,EAAE,CAACkB,GAAG,WAAWD,UAAUH,EAAQF,MAAM,iBAAiBd,EAA2CiB,EAAEC,cAAcf,EAAyE,EAAE,KAAKH,EAA2CiB,EAAEC,cAAchB,EAA4D,EAAE,CAACkB,GAAG,eAAeD,UAAUH,EAAQF,MAAM,aAAad,EAA2CiB,EAAEC,cAAcf,EAAyE,EAAE,KAAKH,EAA2CiB,EAAEC,cAAchB,EAA4D,EAAE,CAACkB,GAAG,UAAUD,UAAUH,EAAQF,MAAM,gBAAgBd,EAA2CiB,EAAEC,cAAcf,EAAyE,EAAE,KAAKH,EAA2CiB,EAAEC,cAAchB,EAA4D,EAAE,CAACkB,GAAG,uBAAuBD,UAAUH,EAAQF,MAAM,oBAAoBd,EAA2CiB,EAAEC,cAAcf,EAAyE,EAAE,KAAKH,EAA2CiB,EAAEC,cAAchB,EAA4D,EAAE,CAACkB,GAAG,QAAQD,UAAUH,EAAQF,MAAM,SAASd,EAA2CiB,EAAEC,cAAcf,EAAyE,EAAE,KAAKH,EAA2CiB,EAAEC,cAAchB,EAA4D,EAAE,CAACkB,GAAG,kBAAkBD,UAAUH,EAAQF,MAAM,mBAAmBd,EAA2CiB,EAAEC,cAAcf,EAAyE,EAAE,KAAKH,EAA2CiB,EAAEC,cAAchB,EAA4D,EAAE,CAACkB,GAAG,cAAcD,UAAUH,EAAQF,MAAM,eAAed,EAA2CiB,EAAEC,cAAcf,EAAyE,EAAE,KAAKH,EAA2CiB,EAAEC,cAAchB,EAA4D,EAAE,CAACkB,GAAG,gBAAgBD,UAAUH,EAAQF,MAAM,iBAAiBd,EAA2CiB,EAAEC,cAAcf,EAAyE,EAAE,KAAKH,EAA2CiB,EAAEC,cAAchB,EAA4D,EAAE,CAACkB,GAAG,YAAYD,UAAUH,EAAQF,MAAM,aAAad,EAA2CiB,EAAEC,cAAcf,EAAyE,EAAE,KAAKH,EAA2CiB,EAAEC,cAAchB,EAA4D,EAAE,CAACkB,GAAG,sBAAsBD,UAAUH,EAAQF,MAAM,wBAAwBd,EAA2CiB,EAAEC,cAAcf,EAAyE,EAAE,KAAKH,EAA2CiB,EAAEC,cAAchB,EAA4D,EAAE,CAACkB,GAAG,cAAcD,UAAUH,EAAQF,MAAM,eAAed,EAA2CiB,EAAEC,cAAcf,EAAyE,EAAE,KAAKH,EAA2CiB,EAAEC,cAAchB,EAA4D,EAAE,CAACkB,GAAG,eAAeD,UAAUH,EAAQF,MAAM,gBAAgBd,EAA2CiB,EAAEC,cAAcf,EAAyE,EAAE,KAAKH,EAA2CiB,EAAEC,cAAchB,EAA4D,EAAE,CAACkB,GAAG,kBAAkBD,UAAUH,EAAQF,MAAM,mBAAmBd,EAA2CiB,EAAEC,cAAcf,EAAyE,EAAE,KAAKH,EAA2CiB,EAAEC,cAAchB,EAA4D,EAAE,CAACkB,GAAG,qBAAqBD,UAAUH,EAAQF,MAAM,sBAAsBd,EAA2CiB,EAAEC,cAAcf,EAAyE,EAAE,KAAKH,EAA2CiB,EAAEC,cAAchB,EAA4D,EAAE,CAACkB,GAAG,YAAYD,UAAUH,EAAQF,MAAM,aAAad,EAA2CiB,EAAEC,cAAcf,EAAyE,EAAE,KAAKH,EAA2CiB,EAAEC,cAAchB,EAA4D,EAAE,CAACkB,GAAG,kBAAkBD,UAAUH,EAAQF,MAAM,eAAed,EAA2CiB,EAAEC,cAAc,MAAM,SAItpMG,IACA,SAAU3B,EAAQ4B,EAAS1B,GAEjCF,EAAO4B,QAAU1B,EAAoB2B,EAAI,gCAInCC,IACA,SAAU9B,EAAQC,EAAqBC,GAE7C,aACAA,EAAoB6B,EAAE9B,GACD,IAAI+B,EAAsH9B,EAAoB,GAC1I+B,EAAmH/B,EAAoB,GACvIgC,EAAiIhC,EAAoB,GACrJiC,EAAsHjC,EAAoB,GAC1IkC,EAAgHlC,EAAoB,GACpImC,EAAqCnC,EAAoB,GACzDoC,EAA0DpC,EAAoBK,EAAE8B,GAChFE,EAAuCrC,EAAoB,GAC3DsC,EAA4DtC,EAAoBK,EAAEgC,GAClFE,EAAiDvC,EAAoB,GACrEwC,EAAiDxC,EAAoB,GACrEyC,EAAiDzC,EAAoB,IACrE0C,EAAkD1C,EAAoB,IAGtE2C,GAFoD3C,EAAoB,IAE/BA,EAAoB,MAC7D4C,EAAmD5C,EAAoB,GACvE6C,EAAkD7C,EAAoB,KACtE8C,EAAuE9C,EAAoBK,EAAEwC,GAClHE,EAAO,CAACC,gBAAgB,UAAUC,QAAQ,MAAMC,SAAS,QAAYC,EAAS,CAACC,OAAO,IAAIC,MAAM,KAAgLC,EAAW,qaAAqaC,OAAWC,EAAM,+jCAAmkCD,OAAWE,EAAY,oRAAoRF,OAAWG,EAAY,w7CAA47CH,OAClhHI,EAAwB,SAASC,GAA4K,SAASD,IAA6K,OAAhKhD,OAAOmB,EAAqI,EAA5InB,CAA+IhB,KAAKgE,GAAmBhD,OAAOqB,EAAgJ,EAAvJrB,CAA0JhB,KAAKgB,OAAOsB,EAAqI,EAA5ItB,CAA+IgD,GAAYE,MAAMlE,KAAKmE,YAAywM,OAA36NnD,OAAOuB,EAA+H,EAAtIvB,CAAyIgD,EAAWC,GAA2hBjD,OAAOoB,EAAkI,EAAzIpB,CAA4IgD,EAAW,CAAC,CAACI,IAAI,oBAAoBC,MAAM,WAA6BC,YAAW,WAAW,OAAO3B,EAA6CjB,EAAE6C,iBAAiB,KAAM,CAACH,IAAI,SAASC,MAAM,WAAkB,IAAI5C,EAAQzB,KAAKwE,MAAM/C,QAAQ,OAAOgB,EAA2Cf,EAAEC,cAAciB,EAAgE,EAAE,CAAC6B,WAAU,GAAMhC,EAA2Cf,EAAEC,cAAciB,EAAgE,EAAE,CAAC8B,MAAK,EAAKC,GAAG,GAAGlC,EAA2Cf,EAAEC,cAAckB,EAAgE,EAAE,CAACjB,UAAUH,EAAQL,OAAOqB,EAA2Cf,EAAEC,cAAc,KAAK,KAAKc,EAA2Cf,EAAEC,cAAcqB,EAAwD,EAAE,SAASP,EAA2Cf,EAAEC,cAAciB,EAAgE,EAAE,CAAC8B,MAAK,EAAKC,GAAG,IAAIlC,EAA2Cf,EAAEC,cAAckB,EAAgE,EAAE,CAACjB,UAAUH,EAAQL,OAAOqB,EAA2Cf,EAAEC,cAAcmB,EAAgE,EAAE,KAAKL,EAA2Cf,EAAEC,cAAc,KAAK,KAAK,uEAAuEc,EAA2Cf,EAAEC,cAAc,IAAI,KAAK,8HAA8Hc,EAA2Cf,EAAEC,cAAc,IAAI,KAAK,OAAO,qIAAqIc,EAA2Cf,EAAEC,cAAc,IAAI,KAAK,qBAAqB,6FAA6Fc,EAA2Cf,EAAEC,cAAc,KAAK,MAAMc,EAA2Cf,EAAEC,cAAc,KAAK,MAAMc,EAA2Cf,EAAEC,cAAc,KAAK,KAAKc,EAA2Cf,EAAEC,cAAc,KAAK,KAAKc,EAA2Cf,EAAEC,cAAc,IAAI,KAAK,4BAA4B,yFAAyFc,EAA2Cf,EAAEC,cAAc,KAAK,MAAMc,EAA2Cf,EAAEC,cAAc,MAAM,CAACiD,MAAMxB,GAAQX,EAA2Cf,EAAEC,cAAcsB,EAAkE,EAAE,CAAC4B,KAAKlB,EAAWmB,SAAS,KAAKC,QAAQ,CAAC,mBAAmBtC,EAA2Cf,EAAEC,cAAc,KAAK,MAAMc,EAA2Cf,EAAEC,cAAc,IAAI,KAAK,wJAAwJc,EAA2Cf,EAAEC,cAAc,KAAK,MAAMc,EAA2Cf,EAAEC,cAAc,MAAM,CAACqD,IAAI7B,EAAwDzB,EAAEuD,IAAI,SAASrD,UAAU,cAAcgD,MAAMpB,IAAWf,EAA2Cf,EAAEC,cAAc,KAAK,KAAK,+CAA+Cc,EAA2Cf,EAAEC,cAAc,KAAK,KAAKc,EAA2Cf,EAAEC,cAAc,KAAK,KAAK,wHAAwHc,EAA2Cf,EAAEC,cAAc,MAAM,CAACiD,MAAMxB,GAAQX,EAA2Cf,EAAEC,cAAcsB,EAAkE,EAAE,CAAC4B,KAAKhB,EAAMiB,SAAS,KAAKC,QAAQ,CAAC,mBAAmBtC,EAA2Cf,EAAEC,cAAc,KAAK,MAAMc,EAA2Cf,EAAEC,cAAc,KAAK,KAAK,kCAAkCc,EAA2Cf,EAAEC,cAAc,KAAK,KAAKc,EAA2Cf,EAAEC,cAAc,KAAK,KAAK,wDAAwDc,EAA2Cf,EAAEC,cAAc,KAAK,KAAK,gEAAgEc,EAA2Cf,EAAEC,cAAc,KAAK,KAAK,wBAAwBc,EAA2Cf,EAAEC,cAAc,KAAK,KAAK,4BAA4Bc,EAA2Cf,EAAEC,cAAc,KAAK,KAAK,uBAAuBc,EAA2Cf,EAAEC,cAAc,KAAK,KAAK,+JAA+Jc,EAA2Cf,EAAEC,cAAc,MAAM,CAACiD,MAAMxB,GAAQX,EAA2Cf,EAAEC,cAAcsB,EAAkE,EAAE,CAAC4B,KAAKf,EAAYgB,SAAS,KAAKC,QAAQ,CAAC,mBAAmBtC,EAA2Cf,EAAEC,cAAc,KAAK,MAAMc,EAA2Cf,EAAEC,cAAc,KAAK,KAAK,8CAA8Cc,EAA2Cf,EAAEC,cAAc,IAAI,KAAK,2IAA2Ic,EAA2Cf,EAAEC,cAAc,KAAK,MAAMc,EAA2Cf,EAAEC,cAAc,MAAM,CAACiD,MAAMxB,GAAQX,EAA2Cf,EAAEC,cAAcsB,EAAkE,EAAE,CAAC4B,KAAKd,EAAYe,SAAS,KAAKC,QAAQ,CAAC,mBAAmBtC,EAA2Cf,EAAEC,cAAc,KAAK,MAAMc,EAA2Cf,EAAEC,cAAc,IAAI,KAAK,8GAAsHqC,EAAv8N,CAAo9NxB,EAA8C,WAAgCpC,EAA6B,QAAKY,OAAO+B,EAAiE,EAAxE/B,EAD5+N,SAAgBC,GAAO,MAAM,CAACG,MAAM,CAAC8D,OAAOjE,EAAMK,QAAQ,GAAGgC,QAAQrC,EAAMK,QAAQ,IAAI6D,SAAS,CAACD,OAAOjE,EAAMK,QAAQ,IAAI8D,UAAU,CAACC,UAAU,aAC41N,CAAoFrB"},"code":"(this[\"webpackJsonpmern-stack-client\"]=this[\"webpackJsonpmern-stack-client\"]||[]).push([[25],{344:function(e,a,n){\"use strict\";n.d(a,\"a\",(function(){return m}));var t=n(0),l=n.n(t),r=n(12),s=n(14),c=n(50),i=n(49),o=Object(i.a)((function(e){return{root:{display:\"flex\"},paper:{marginRight:e.spacing(2)},line:{textDecoration:\"none\"}}}));function m(){var e=o();return l.a.createElement(\"div\",{className:e.root},l.a.createElement(c.a,null,l.a.createElement(s.a,null,l.a.createElement(r.b,{to:\"/introAngular\",className:e.line},\"AI\")),l.a.createElement(s.a,null,l.a.createElement(r.b,{to:\"/tensorflow\",className:e.line},\"Tensorflow\")),l.a.createElement(s.a,null,l.a.createElement(r.b,{to:\"/tensors\",className:e.line},\"Tensorboards\")),l.a.createElement(s.a,null,l.a.createElement(r.b,{to:\"/angCompiler\",className:e.line},\"Compiler\")),l.a.createElement(s.a,null,l.a.createElement(r.b,{to:\"/neural\",className:e.line},\"NeuralKeras\")),l.a.createElement(s.a,null,l.a.createElement(r.b,{to:\"/activationFunctions\",className:e.line},\"activationFuncs\")),l.a.createElement(s.a,null,l.a.createElement(r.b,{to:\"/loss\",className:e.line},\"Loss\")),l.a.createElement(s.a,null,l.a.createElement(r.b,{to:\"/gradientNeural\",className:e.line},\"GradientNeural\")),l.a.createElement(s.a,null,l.a.createElement(r.b,{to:\"/stochastic\",className:e.line},\"Stochastic\")),l.a.createElement(s.a,null,l.a.createElement(r.b,{to:\"/benchmarking\",className:e.line},\"Benchmarking\")),l.a.createElement(s.a,null,l.a.createElement(r.b,{to:\"/customer\",className:e.line},\"Customer\")),l.a.createElement(s.a,null,l.a.createElement(r.b,{to:\"/regularizationDeep\",className:e.line},\"Regularization Deep\")),l.a.createElement(s.a,null,l.a.createElement(r.b,{to:\"/imbalanced\",className:e.line},\"Imbalanced\")),l.a.createElement(s.a,null,l.a.createElement(r.b,{to:\"/imbalanced2\",className:e.line},\"Imbalanced2\")),l.a.createElement(s.a,null,l.a.createElement(r.b,{to:\"/convolutionals\",className:e.line},\"Convolutionals\")),l.a.createElement(s.a,null,l.a.createElement(r.b,{to:\"/data_augmentation\",className:e.line},\"data Augmentation\")),l.a.createElement(s.a,null,l.a.createElement(r.b,{to:\"/transfer\",className:e.line},\"Transfer\")),l.a.createElement(s.a,null,l.a.createElement(r.b,{to:\"/word_embedding\",className:e.line},\"Embedding\"))),l.a.createElement(\"div\",null))}},357:function(e,a,n){e.exports=n.p+\"static/media/hp.2e354048.jpg\"},467:function(e,a,n){\"use strict\";n.r(a);var t=n(4),l=n(3),r=n(5),s=n(6),c=n(8),i=n(0),o=n.n(i),m=n(7),d=n.n(m),p=n(2),u=n(9),_=n(13),b=n(11),g=(n(16),n(344)),h=n(1),E=n(357),w=n.n(E),f={backgroundColor:\"#F0F8FF\",padding:\"1px\",fontSize:\"16px\"},y={height:200,width:500},v=\"\\nimport pandas as pd\\nimport numpy as np\\nfrom sklearn import preprocessing\\nfrom matplotlib import pyplot as plt\\n%matplotlib inline\\n\\ndf = pd.read_csv(\\\"homeprices_banglore.csv\\\")\\n\\nsx = preprocessing.MinMaxScaler()\\nsy = preprocessing.MinMaxScaler()\\n\\nscaled_X = sx.fit_transform(df.drop('price',axis='columns'))\\nscaled_y = sy.fit_transform(df['price'].values.reshape(df.shape[0],1))\\n\\nscaled_y.reshape(20,)\\n\".trim(),N='\\ndef batch_gradient_descent(X, y_true, epochs, learning_rate = 0.01):\\n    number_of_features = X.shape[1]\\n    w = np.ones(shape=(number_of_features)) \\n    b = 0\\n    total_samples = X.shape[0]                                                  # number of rows in X\\n    cost_list = []\\n    epoch_list = []\\n    \\n    for i in range(epochs):        \\n        y_predicted = np.dot(w, X.T) + b\\n\\n        w_grad = -(2/total_samples)*(X.T.dot(y_true-y_predicted))\\n        b_grad = -(2/total_samples)*np.sum(y_true-y_predicted)\\n        \\n        w = w - learning_rate * w_grad\\n        b = b - learning_rate * b_grad\\n        \\n        cost = np.mean(np.square(y_true-y_predicted))                          # MSE (Mean Squared Error)\\n        \\n        if i%10==0:\\n            cost_list.append(cost)\\n            epoch_list.append(i)\\n    return w, b, cost, cost_list, epoch_list\\n\\nw, b, cost, cost_list, epoch_list = batch_gradient_descent(scaled_X,scaled_y.reshape(scaled_y.shape[0],),500)\\nw, b, cost\\n\\nplt.xlabel(\"epoch\")\\nplt.ylabel(\"cost\")\\nplt.plot(epoch_list,cost_list)\\n\\n'.trim(),x=\"\\ndef predict(area,bedrooms,w,b):\\n    scaled_X = sx.transform([[area, bedrooms]])[0]\\n    scaled_price = w[0] * scaled_X[0] + w[1] * scaled_X[1] + b\\n    return sy.inverse_transform([[scaled_price]])[0][0]\\n\\npredict(2600,4,w,b)\\npredict(1000,2,w,b)\\npredict(1500,3,w,b)\\n\".trim(),X='\\nimport random\\nrandom.randint(0,6)                              # randit gives random number between two numbers specified in the argument.\\n\\ndef stochastic_gradient_descent(X, y_true, epochs, learning_rate = 0.01):\\n    number_of_features = X.shape[1]\\n    w = np.ones(shape=(number_of_features)) \\n    b = 0\\n    total_samples = X.shape[0]\\n    \\n    cost_list = []\\n    epoch_list = []\\n    \\n    for i in range(epochs):    \\n        random_index = random.randint(0,total_samples-1) # random index from total samples\\n        sample_x = X[random_index]\\n        sample_y = y_true[random_index]\\n        y_predicted = np.dot(w, sample_x.T) + b\\n    \\n        w_grad = -(2/total_samples)*(sample_x.T.dot(sample_y-y_predicted))\\n        b_grad = -(2/total_samples)*(sample_y-y_predicted)\\n        \\n        w = w - learning_rate * w_grad\\n        b = b - learning_rate * b_grad\\n        cost = np.square(sample_y-y_predicted)\\n        \\n        if i%100==0: # at every 100th iteration record the cost and epoch value\\n            cost_list.append(cost)\\n            epoch_list.append(i)\\n    return w, b, cost, cost_list, epoch_list\\n\\nw_sgd, b_sgd, cost_sgd, cost_list_sgd, epoch_list_sgd = SGD(scaled_X,scaled_y.reshape(scaled_y.shape[0],),10000)\\nw_sgd, b_sgd, cost_sgd\\n\\nw , b \\nplt.xlabel(\"epoch\")\\nplt.ylabel(\"cost\")\\nplt.plot(epoch_list_sgd,cost_list_sgd)\\n\\npredict(2600,4,w_sgd, b_sgd) \\npredict(1000,2,w_sgd, b_sgd)\\npredict(1500,3,w_sgd, b_sgd)\\n'.trim(),j=function(e){function a(){return Object(t.a)(this,a),Object(r.a)(this,Object(s.a)(a).apply(this,arguments))}return Object(c.a)(a,e),Object(l.a)(a,[{key:\"componentDidMount\",value:function(){setTimeout((function(){return d.a.highlightAll()}),0)}},{key:\"render\",value:function(){var e=this.props.classes;return o.a.createElement(p.a,{container:!0},o.a.createElement(p.a,{item:!0,xs:2},o.a.createElement(u.a,{className:e.paper},o.a.createElement(\"h4\",null,o.a.createElement(g.a,null)))),o.a.createElement(p.a,{item:!0,xs:10},o.a.createElement(u.a,{className:e.paper},o.a.createElement(_.a,null,o.a.createElement(\"h3\",null,\"Implementation of stochastic and batch grandient descent in python.\"),o.a.createElement(\"i\",null,\"We will use home prices data set to implement batch and stochastic gradient descent in python. Batch gradient descent uses \",o.a.createElement(\"b\",null,\"all\"),\" training samples in forward pass to calculate cumulitive error and than we adjust weights using derivaties. In stochastic GD, we \",o.a.createElement(\"b\",null,\"randomly pick one\"),\" training sample, perform forward pass, compute the error and immidiately adjust weights.\"),o.a.createElement(\"br\",null),o.a.createElement(\"br\",null),o.a.createElement(\"ul\",null,o.a.createElement(\"li\",null,o.a.createElement(\"b\",null,\"Preprocessing/ Scaling: \"),\"Since our columns are on different sacle it is important to perform scaling on them.\")),o.a.createElement(\"br\",null),o.a.createElement(\"div\",{style:f},o.a.createElement(h.a,{code:v,language:\"js\",plugins:[\"line-numbers\"]})),o.a.createElement(\"br\",null),o.a.createElement(\"i\",null,\"We should convert target column (price) into one dimensional array. It has become 2D due to scaling that we did above but now we should change to 1D\"),o.a.createElement(\"br\",null),o.a.createElement(\"img\",{src:w.a,alt:\"Theata\",className:\"responsive2\",style:y}),o.a.createElement(\"h3\",null,\"Now implement mini batch gradient descent. \"),o.a.createElement(\"ul\",null,o.a.createElement(\"li\",null,\"numpy array with 1 row and columns equal to number of features. In our case number_of_features = 2 (area, bedroom).\")),o.a.createElement(\"div\",{style:f},o.a.createElement(h.a,{code:N,language:\"js\",plugins:[\"line-numbers\"]})),o.a.createElement(\"br\",null),o.a.createElement(\"h3\",null,\"Lets do some predictions now. \"),o.a.createElement(\"ul\",null,o.a.createElement(\"li\",null,\"Here w1 = w[0] , w2 = w[1], w3 = w[2] and bias is b.\"),o.a.createElement(\"li\",null,\"Equation for price is w1*area + w2*bedrooms + w3*age + bias.\"),o.a.createElement(\"li\",null,\"scaled_X[0] is area.\"),o.a.createElement(\"li\",null,\"scaled_X[1] is bedrooms.\"),o.a.createElement(\"li\",null,\"scaled_X[2] is age.\"),o.a.createElement(\"li\",null,\"Once we get price prediction we need to to rescal it back to original value also since it returns 2D array, to get single value we need to do value[0][0].\")),o.a.createElement(\"div\",{style:f},o.a.createElement(h.a,{code:x,language:\"js\",plugins:[\"line-numbers\"]})),o.a.createElement(\"br\",null),o.a.createElement(\"h3\",null,\"Stochastic Gradient Descent Implementation\"),o.a.createElement(\"i\",null,\"Stochastic GD will use randomly picked single training sample to calculate error and using this error we backpropage to adjust weights.\"),o.a.createElement(\"br\",null),o.a.createElement(\"div\",{style:f},o.a.createElement(h.a,{code:X,language:\"js\",plugins:[\"line-numbers\"]})),o.a.createElement(\"br\",null),o.a.createElement(\"i\",null,\"Compare this with weights and bias that we got using gradient descent. They both of quite similar.\")))))}}]),a}(i.Component);a.default=Object(b.a)((function(e){return{paper:{margin:e.spacing(1),padding:e.spacing(1)},smMargin:{margin:e.spacing(1)},actionDiv:{textAlign:\"center\"}}}))(j)}}]);","extractedComments":[]}