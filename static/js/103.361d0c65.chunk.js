(this["webpackJsonpmern-stack-client"]=this["webpackJsonpmern-stack-client"]||[]).push([[103],{140:function(e,n,t){"use strict";t.d(n,"a",(function(){return d}));var a=t(45),l=t(28),r=t(136),s=t(137),i=t(139),c=t(0),o=t.n(c),u=t(138),m=t.n(u),d=(t(59),function(e){function n(e){var t;return Object(a.a)(this,n),(t=Object(r.a)(this,Object(s.a)(n).call(this,e))).highlight=function(){t.ref&&t.ref.current&&m.a.highlightElement(t.ref.current)},t.ref=o.a.createRef(),t}return Object(i.a)(n,e),Object(l.a)(n,[{key:"componentDidMount",value:function(){this.highlight()}},{key:"componentDidUpdate",value:function(){this.highlight()}},{key:"render",value:function(){var e=this.props,n=e.code,t=(e.plugins,e.language);return o.a.createElement("pre",{className:"code-prism"},o.a.createElement("code",{ref:this.ref,className:"language-".concat(t)},n.trim()))}}]),n}(o.a.Component))},141:function(e,n,t){},150:function(e,n,t){"use strict";t.d(n,"a",(function(){return u}));var a=t(0),l=t.n(a),r=t(26),s=t(297),i=t(295),c=t(114),o=Object(c.a)((function(e){return{root:{display:"flex"},paper:{marginRight:e.spacing(2)},line:{textDecoration:"none"}}}));function u(){var e=o();return l.a.createElement("div",{className:e.root},l.a.createElement(i.a,null,l.a.createElement(s.a,null,l.a.createElement(r.b,{to:"/introAngular",className:e.line},"AI")),l.a.createElement(s.a,null,l.a.createElement(r.b,{to:"/tensorflow",className:e.line},"Tensorflow")),l.a.createElement(s.a,null,l.a.createElement(r.b,{to:"/tensors",className:e.line},"Tensorboards")),l.a.createElement(s.a,null,l.a.createElement(r.b,{to:"/angCompiler",className:e.line},"Compiler")),l.a.createElement(s.a,null,l.a.createElement(r.b,{to:"/neural",className:e.line},"NeuralKeras")),l.a.createElement(s.a,null,l.a.createElement(r.b,{to:"/activationFunctions",className:e.line},"activationFuncs")),l.a.createElement(s.a,null,l.a.createElement(r.b,{to:"/loss",className:e.line},"Loss")),l.a.createElement(s.a,null,l.a.createElement(r.b,{to:"/gradientNeural",className:e.line},"GradientNeural")),l.a.createElement(s.a,null,l.a.createElement(r.b,{to:"/stochastic",className:e.line},"Stochastic")),l.a.createElement(s.a,null,l.a.createElement(r.b,{to:"/benchmarking",className:e.line},"Benchmarking")),l.a.createElement(s.a,null,l.a.createElement(r.b,{to:"/customer",className:e.line},"Customer")),l.a.createElement(s.a,null,l.a.createElement(r.b,{to:"/regularizationDeep",className:e.line},"Regularization Deep")),l.a.createElement(s.a,null,l.a.createElement(r.b,{to:"/imbalanced",className:e.line},"Imbalanced")),l.a.createElement(s.a,null,l.a.createElement(r.b,{to:"/imbalanced2",className:e.line},"Imbalanced2")),l.a.createElement(s.a,null,l.a.createElement(r.b,{to:"/convolutionals",className:e.line},"Convolutionals")),l.a.createElement(s.a,null,l.a.createElement(r.b,{to:"/data_augmentation",className:e.line},"data Augmentation")),l.a.createElement(s.a,null,l.a.createElement(r.b,{to:"/transfer",className:e.line},"Transfer")),l.a.createElement(s.a,null,l.a.createElement(r.b,{to:"/word_embedding",className:e.line},"Embedding")),l.a.createElement(s.a,null,l.a.createElement(r.b,{to:"/datatypests",className:e.line},"Datatypes")),l.a.createElement(s.a,null,l.a.createElement(r.b,{to:"/typeScript_2",className:e.line},"TS Function")),l.a.createElement(s.a,null,l.a.createElement(r.b,{to:"/typeScript_4",className:e.line},"Type Assertion"))),l.a.createElement("div",null))}},463:function(e,n,t){"use strict";t.r(n);var a=t(45),l=t(28),r=t(136),s=t(137),i=t(139),c=t(0),o=t.n(c),u=t(138),m=t.n(u),d=t(120),p=t(57),_=t(296),f=t(5),h=(t(141),t(150)),g=t(140),E={backgroundColor:"#F0F8FF",padding:"1px",fontSize:"16px"},b="\ndf = pd.read_csv(\"customer_churn.csv\")\n\ndf.Churn.value_counts()\n517400/ df.shape[0]\n\ndf.drop('customerID',axis='columns',inplace=True)\n\ndf.TotalCharges.values\npd.to_numeric(df.TotalCharges,errors='coerce').isnull()\ndf[pd.to_numeric(df.TotalCharges,errors='coerce').isnull()]\n\ndf.iloc[488].TotalCharges\ndf[df.TotalCharges!=' '].shape\ndf1 = df[df.TotalCharges!=' ']\n\ndf1.TotalCharges = pd.to_numeric(df1.TotalCharges)\ndf1.TotalCharges.values\ndf1[df1.Churn=='No']\n".trim(),y="\ntenure_churn_no = df1[df1.Churn=='No'].tenure\ntenure_churn_yes = df1[df1.Churn=='Yes'].tenure\n\nplt.xlabel(\"tenure\")\nplt.ylabel(\"Number Of Customers\")\nplt.title(\"Customer Churn Prediction Visualiztion\")\n\nblood_sugar_men = [113, 85, 90, 150, 149, 88, 93, 115, 135, 80, 77, 82, 129]\nblood_sugar_women = [67, 98, 89, 120, 133, 150, 84, 69, 89, 79, 120, 112, 100]\n\nplt.hist([tenure_churn_yes, tenure_churn_no], rwidth=0.95, color=['green','red'],label=['Churn=Yes','Churn=No'])\nplt.legend()\n\n\nmc_churn_no = df1[df1.Churn=='No'].MonthlyCharges      \nmc_churn_yes = df1[df1.Churn=='Yes'].MonthlyCharges      \n\nplt.xlabel(\"Monthly Charges\")\nplt.ylabel(\"Number Of Customers\")\nplt.title(\"Customer Churn Prediction Visualiztion\")\n\nblood_sugar_men = [113, 85, 90, 150, 149, 88, 93, 115, 135, 80, 77, 82, 129]\nblood_sugar_women = [67, 98, 89, 120, 133, 150, 84, 69, 89, 79, 120, 112, 100]\n\nplt.hist([mc_churn_yes, mc_churn_no], rwidth=0.95, color=['green','red'],label=['Churn=Yes','Churn=No'])\nplt.legend()\n\n\ndef print_unique_col_values(df):\n       for column in df:\n            if df[column].dtypes=='object':\n                print(f'{column}: {df[column].unique()}') \n                \nprint_unique_col_values(df1)\n\ndf1.replace('No internet service','No',inplace=True)\ndf1.replace('No phone service','No',inplace=True)\nprint_unique_col_values(df1)\n\nyes_no_columns = ['Partner','Dependents','PhoneService','MultipleLines','OnlineSecurity','OnlineBackup',\n                  'DeviceProtection','TechSupport','StreamingTV','StreamingMovies','PaperlessBilling','Churn']\nfor col in yes_no_columns:\n    df1[col].replace({'Yes': 1,'No': 0},inplace=True)\n    \n    \nfor col in df1:\n    print(f'{col}: {df1[col].unique()}') \n    \ndf1['gender'].replace({'Female':1,'Male':0},inplace=True)\ndf1.gender.unique()\n".trim(),v="\nfrom sklearn.preprocessing import MinMaxScaler\n\ndf2 = pd.get_dummies(data=df1, columns=['InternetService','Contract','PaymentMethod'])\n\ncols_to_scale = ['tenure','MonthlyCharges','TotalCharges']\n\nscaler = MinMaxScaler()\ndf2[cols_to_scale] = scaler.fit_transform(df2[cols_to_scale])\n\nfor col in df2:\n    print(f'{col}: {df2[col].unique()}')\n".trim(),N="\nX = df2.drop('Churn',axis='columns')\ny = testLabels = df2.Churn.astype(np.float32)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=15, stratify=y)\n\ny_train.value_counts()\n5163/1869\nlen(X_train.columns)\n".trim(),C="\nfrom tensorflow_addons import losses\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom sklearn.metrics import confusion_matrix , classification_report\n\ndef ANN(X_train, y_train, X_test, y_test, loss, weights):\n    model = keras.Sequential([\n        keras.layers.Dense(26, input_dim=26, activation='relu'),\n        keras.layers.Dense(15, activation='relu'),\n        keras.layers.Dense(1, activation='sigmoid')\n    ])\n\n    model.compile(optimizer='adam', loss=loss, metrics=['accuracy'])\n    \n    if weights == -1:\n        model.fit(X_train, y_train, epochs=100)\n    else:\n        model.fit(X_train, y_train, epochs=100, class_weight = weights)\n    \n    print(model.evaluate(X_test, y_test))\n    \n    y_preds = model.predict(X_test)\n    y_preds = np.round(y_preds)\n    \n    print(\"Classification Report:\", classification_report(y_test, y_preds))\n    \n    return y_preds\n    \ny_preds = ANN(X_train, y_train, X_test, y_test, 'binary_crossentropy', -1)\n".trim(),T="\n# Method 1: Undersampling\ncount_class_0, count_class_1 = df1.Churn.value_counts()\n\ndf_class_0 = df2[df2['Churn'] == 0]\ndf_class_1 = df2[df2['Churn'] == 1]\n\ndf_class_0_under = df_class_0.sample(count_class_1)\ndf_test_under = pd.concat([df_class_0_under, df_class_1], axis=0)\n\nprint('Random under-sampling:')\nprint(df_test_under.Churn.value_counts())\n\nX = df_test_under.drop('Churn',axis='columns')\ny = df_test_under['Churn']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=15, stratify=y)\ny_train.value_counts()\n\ny_preds = ANN(X_train, y_train, X_test, y_test, 'binary_crossentropy', -1)\n".trim(),w=function(e){function n(){return Object(a.a)(this,n),Object(r.a)(this,Object(s.a)(n).apply(this,arguments))}return Object(i.a)(n,e),Object(l.a)(n,[{key:"componentDidMount",value:function(){setTimeout((function(){return m.a.highlightAll()}),0)}},{key:"render",value:function(){var e=this.props.classes;return o.a.createElement(d.a,{container:!0},o.a.createElement(d.a,{item:!0,xs:2},o.a.createElement(p.a,{className:e.paper},o.a.createElement("h4",null,o.a.createElement(h.a,null)))),o.a.createElement(d.a,{item:!0,xs:10},o.a.createElement(p.a,{className:e.paper},o.a.createElement(_.a,null,o.a.createElement("h3",null,"Handling imbalanced data in customer churn prediction"),o.a.createElement("b",null,"Handling imbalanced dataset:"),o.a.createElement("ul",null,o.a.createElement("li",null,"1. Under sampling majority class."),o.a.createElement("li",null,"2. Over sampling minority class by duplication."),o.a.createElement("ul",null,o.a.createElement("li",null,"Generate new sample from current sample by simply duplicating them.")),o.a.createElement("br",null),o.a.createElement("li",null,"3. Over sampling minority class using SMOTE."),o.a.createElement("ul",null,o.a.createElement("li",null,"Generate synthetic example using KNN aglo."),o.a.createElement("li",null,o.a.createElement("b",null,"SMOTE: "),"Synthetic Minority Over-sampling Technique.")),o.a.createElement("br",null),o.a.createElement("li",null,"Ensemble Method."),o.a.createElement("li",null,"Focal loss."),o.a.createElement("ul",null,o.a.createElement("li",null,"Focal loss will penalize majority samples during loss calculation and give weight to minority class samples."))),o.a.createElement("br",null),o.a.createElement("i",null,"Customer churn prediction is to measure why customers are leaving a business. Looking at customer churn in telecom business. We will build a deep learning model to predict the churn and use precision,recall, f1-score to measure performance of our model. We will then handle imbalance in data using various techniques and improve f1-score."),o.a.createElement("br",null),o.a.createElement("div",{style:E},o.a.createElement(g.a,{code:b,language:"js",plugins:["line-numbers"]})),o.a.createElement("br",null),o.a.createElement("h3",null,"Data Visualization"),o.a.createElement("div",{style:E},o.a.createElement(g.a,{code:y,language:"js",plugins:["line-numbers"]})),o.a.createElement("br",null),o.a.createElement("h3",null,"One hot encoding for categorical columns"),o.a.createElement("div",{style:E},o.a.createElement(g.a,{code:v,language:"js",plugins:["line-numbers"]})),o.a.createElement("br",null),o.a.createElement("h3",null,"Train test split"),o.a.createElement("div",{style:E},o.a.createElement(g.a,{code:N,language:"js",plugins:["line-numbers"]})),o.a.createElement("br",null),o.a.createElement("h3",null,"Build a model (ANN) in tensorflow/ keras"),o.a.createElement("div",{style:E},o.a.createElement(g.a,{code:C,language:"js",plugins:["line-numbers"]})),o.a.createElement("br",null),o.a.createElement("h3",null,"Mitigating Skewdness of Data"),o.a.createElement("div",{style:E},o.a.createElement(g.a,{code:T,language:"js",plugins:["line-numbers"]})),o.a.createElement("br",null),o.a.createElement("i",null,"Printing Classification in the last, Scroll down till the last epoch to watch the classification report.")))))}}]),n}(c.Component);n.default=Object(f.a)((function(e){return{paper:{margin:e.spacing(1),padding:e.spacing(1)},smMargin:{margin:e.spacing(1)},actionDiv:{textAlign:"center"}}}))(w)}}]);
//# sourceMappingURL=103.361d0c65.chunk.js.map