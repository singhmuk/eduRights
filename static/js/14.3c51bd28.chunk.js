(this["webpackJsonpmern-stack-client"]=this["webpackJsonpmern-stack-client"]||[]).push([[14],{140:function(e,t,n){"use strict";n.d(t,"a",(function(){return p}));var a=n(45),l=n(28),r=n(136),i=n(137),s=n(139),c=n(0),o=n.n(c),u=n(138),m=n.n(u),p=(n(59),function(e){function t(e){var n;return Object(a.a)(this,t),(n=Object(r.a)(this,Object(i.a)(t).call(this,e))).highlight=function(){n.ref&&n.ref.current&&m.a.highlightElement(n.ref.current)},n.ref=o.a.createRef(),n}return Object(s.a)(t,e),Object(l.a)(t,[{key:"componentDidMount",value:function(){this.highlight()}},{key:"componentDidUpdate",value:function(){this.highlight()}},{key:"render",value:function(){var e=this.props,t=e.code,n=(e.plugins,e.language);return o.a.createElement("pre",{className:"code-prism"},o.a.createElement("code",{ref:this.ref,className:"language-".concat(n)},t.trim()))}}]),t}(o.a.Component))},141:function(e,t,n){},146:function(e,t,n){"use strict";n.d(t,"a",(function(){return u}));var a=n(0),l=n.n(a),r=n(26),i=n(297),s=n(295),c=n(114),o=Object(c.a)((function(e){return{root:{display:"flex"},paper:{marginRight:e.spacing(2)},line:{textDecoration:"none"}}}));function u(){var e=o();return l.a.createElement("div",{className:e.root},l.a.createElement(s.a,null,l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/infoMl",className:e.line},"InfoMl")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/gredient_decents",className:e.line},"Gredient Decents")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/training",className:e.line},"Traning")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/regularizations",className:e.line},"Regularizations")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/featuresEng",className:e.line},"FeaturesEng")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/adaboost",className:e.line},"Adaboots")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/greedSearch",className:e.line},"Greed Search")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/perceptron",className:e.line},"Perceptron")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/pcaPy",className:e.line},"PCA")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/leanearRegression",className:e.line},"Leanear Regression")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/logisticReg",className:e.line},"Logistic Regression")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/lda",className:e.line},"Lda")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/knn",className:e.line},"Knn")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/k_meanClustring",className:e.line},"K_Mean")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/naiveBar",className:e.line},"Naive Bayes")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/randomForest",className:e.line},"Random Forest")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/decisiontree",className:e.line},"Decision Tree")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/svmPy",className:e.line},"SVM")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/numpyPy",className:e.line},"Numpy")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/pandas",className:e.line},"Pandas")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/bagging",className:e.line},"Matplotlib")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/logisticRegrations",className:e.line},"Scikit Learn")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/regrations",className:e.line},"SciPy")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/libraries",className:e.line},"OpenCV")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/capture",className:e.line},"Capture")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/joinImages",className:e.line},"JoinImages")),l.a.createElement("br",null),"Deep Learning",l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/superwise",className:e.line},"Superwise"))),l.a.createElement("div",null))}},164:function(e,t,n){e.exports=n.p+"static/media/perceptrons.f6ad4ad3.png"},247:function(e,t,n){e.exports=n.p+"static/media/perceptrons2.5c2b5a03.png"},248:function(e,t,n){e.exports=n.p+"static/media/perceptrons3.82c24adc.png"},249:function(e,t,n){e.exports=n.p+"static/media/perceptrons4.5617a654.png"},520:function(e,t,n){"use strict";n.r(t);var a=n(45),l=n(28),r=n(136),i=n(137),s=n(139),c=n(0),o=n.n(c),u=n(138),m=n.n(u),p=n(120),d=n(57),E=n(296),f=n(5),b=(n(141),n(146)),h=n(140),g=n(164),_=n.n(g),y=n(247),w=n.n(y),x=n(248),N=n.n(x),v=n(249),k=n.n(v),X={backgroundColor:"#F0F8FF",padding:"1px",fontSize:"16px"},j={height:350,width:600},O="\nimport numpy as np\n\nclass Perceptron:\n    def __init__(self, learning_rate=0.01, n_iters=1000):\n        self.lr = learning_rate\n        self.n_iters = n_iters\n        self.activation_func = self._unit_step_func\n        self.weights = None\n        self.bias = None\n\n    def fit(self, X, y):\n        n_samples, n_features = X.shape\n\n        # init parameters\n        self.weights = np.zeros(n_features)\n        self.bias = 0\n\n        y_ = np.array([1 if i > 0 else 0 for i in y])\n\n        for _ in range(self.n_iters):\n\n            for idx, x_i in enumerate(X):\n\n                linear_output = np.dot(x_i, self.weights) + self.bias\n                y_predicted = self.activation_func(linear_output)\n\n                # Perceptron update rule\n                update = self.lr * (y_[idx] - y_predicted)\n\n                self.weights += update * x_i\n                self.bias += update\n\n    def predict(self, X):\n        linear_output = np.dot(X, self.weights) + self.bias\n        y_predicted = self.activation_func(linear_output)\n        return y_predicted\n\n    def _unit_step_func(self, x):\n        return np.where(x >= 0, 1, 0)\n".trim(),P="\nif 0.5x + 0.5y => 0, then 1\nif 0.5x + 0.5y < 0, then 0.\n".trim(),T='\nif __name__ == "__main__":\n    # Imports\n    import matplotlib.pyplot as plt\n    from sklearn.model_selection import train_test_split\n    from sklearn import datasets\n\n    def accuracy(y_true, y_pred):\n        accuracy = np.sum(y_true == y_pred) / len(y_true)\n        return accuracy\n\n    X, y = datasets.make_blobs(\n        n_samples=150, n_features=2, centers=2, cluster_std=1.05, random_state=2\n    )\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, random_state=123\n    )\n\n    p = Perceptron(learning_rate=0.01, n_iters=1000)\n    p.fit(X_train, y_train)\n    predictions = p.predict(X_test)\n\n    print("Perceptron classification accuracy", accuracy(y_test, predictions))\n\n    fig = plt.figure()\n    ax = fig.add_subplot(1, 1, 1)\n    plt.scatter(X_train[:, 0], X_train[:, 1], marker="o", c=y_train)\n\n    x0_1 = np.amin(X_train[:, 0])\n    x0_2 = np.amax(X_train[:, 0])\n\n    x1_1 = (-p.weights[0] * x0_1 - p.bias) / p.weights[1]\n    x1_2 = (-p.weights[0] * x0_2 - p.bias) / p.weights[1]\n\n    ax.plot([x0_1, x0_2], [x1_1, x1_2], "k")\n\n    ymin = np.amin(X_train[:, 1])\n    ymax = np.amax(X_train[:, 1])\n    ax.set_ylim([ymin - 3, ymax + 3])\n\n    plt.show()\n    '.trim(),A=function(e){function t(){return Object(a.a)(this,t),Object(r.a)(this,Object(i.a)(t).apply(this,arguments))}return Object(s.a)(t,e),Object(l.a)(t,[{key:"componentDidMount",value:function(){setTimeout((function(){return m.a.highlightAll()}),0)}},{key:"render",value:function(){var e=this.props.classes;return o.a.createElement(p.a,{container:!0},o.a.createElement(p.a,{item:!0,xs:2},o.a.createElement(d.a,{className:e.paper},o.a.createElement("h4",null,o.a.createElement(b.a,null)))),o.a.createElement(p.a,{item:!0,xs:10},o.a.createElement(d.a,{className:e.paper},o.a.createElement(E.a,null,o.a.createElement("h3",null,"Perceptron \u2013 Basics of Neural Networks"),"A single-layer perceptron is the basic unit of a neural network. A perceptron consists of input values, weights and a bias, a weighted sum and activation function.",o.a.createElement("br",null),o.a.createElement("br",null),o.a.createElement("i",null,"Perceptron consists of one/ more inputs, a processor, and only one o/p."),o.a.createElement("br",null),o.a.createElement("br",null),"A perceptron works by taking in some numerical i/p along with what is known as weights and a bias. It then multiplies these i/p with the respective weights(weighted sum). These products are then added together along with the bias. The activation function takes the weighted sum and the bias as i/p and returns a final o/p.",o.a.createElement("br",null),o.a.createElement("br",null),o.a.createElement("ul",null,o.a.createElement("li",null,"A perceptron consists of four parts: input values, weights and a bias, a weighted sum, and activation function."),o.a.createElement("li",null,o.a.createElement("b",null,"Function may look like: "),"y = xw + x2w2 +...+ xnwn"),o.a.createElement("ul",null,o.a.createElement("li",null,"bias  is alwase 1."),o.a.createElement("li",null,"This function is called the weighted sum because it is the sum of the weights and inputs. This looks like a good function, but what if we wanted the outputs to fall into a certain range 0 to 1."),o.a.createElement("li",null,"We can do this by using an activation function. An ",o.a.createElement("b",null,"activation function")," is a function that converts the i/p into a certain o/p based on a set of rules."),o.a.createElement("br",null),o.a.createElement("img",{src:_.a,alt:"Equations",className:"responsive",style:j}),o.a.createElement("br",null),o.a.createElement("br",null),"There are different kinds of activation functions that exist.",o.a.createElement("br",null),o.a.createElement("b",null,"1. Hyperbolic Tangent: "),"Used to o/p a number from -1 to 1.",o.a.createElement("br",null),o.a.createElement("b",null,"2. Logistic Function: "),"Used to o/p a number from 0 to 1.")),o.a.createElement("br",null),o.a.createElement("b",null,"Why are perceptron's used?"),o.a.createElement("br",null),"Perceptrons are the building blocks of neural networks. It is typically used for supervised learning of binary classifiers.",o.a.createElement("br",null),o.a.createElement("img",{src:w.a,alt:"Equations",className:"responsive",style:j}),o.a.createElement("br",null),"Suppose our goal was to separates this data so that there is a distinction between the blue dots and the red dots.",o.a.createElement("br",null),o.a.createElement("br",null),"A perceptron can create a decision boundary for a binary classification, where a decision boundary is regions of space on a graph that separates different data points.",o.a.createElement("br",null),o.a.createElement("br",null),"Let wx = -0.5, wy = 0.5 and b = 0",o.a.createElement("br",null),"Then the function for the perceptron.",o.a.createElement("br",null),"0.5x + 0.5y = 0",o.a.createElement("br",null),o.a.createElement("br",null),"and the graph is.",o.a.createElement("br",null),o.a.createElement("img",{src:N.a,alt:"Equations",className:"responsive",style:j}),o.a.createElement("br",null),"Let\u2019s suppose that the activation function, in this case, is a simple step function that outputs either 0 or 1. The perceptron function will then label the blue dots as 1 and the red dots as 0.",o.a.createElement("div",{style:X},o.a.createElement(h.a,{code:P,language:"js",plugins:["line-numbers"]})),o.a.createElement("br",null),"Therefore, the function 0.5x + 0.5y = 0 creates a decision boundary that separates the red and blue points.",o.a.createElement("br",null),o.a.createElement("img",{src:k.a,alt:"Equations",className:"responsive",style:j}),o.a.createElement("br",null),o.a.createElement("b",null,"Overall, we see that a perceptron can do basic classification using a decision boundary."),o.a.createElement("br",null),o.a.createElement("h3",null,"Example"),o.a.createElement("div",{style:X},o.a.createElement(h.a,{code:O,language:"js",plugins:["line-numbers"]})),o.a.createElement("br",null),o.a.createElement("h3",null,"Testing"),o.a.createElement("div",{style:X},o.a.createElement(h.a,{code:T,language:"js",plugins:["line-numbers"]}))))))}}]),t}(c.Component);t.default=Object(f.a)((function(e){return{paper:{margin:e.spacing(1),padding:e.spacing(1)},smMargin:{margin:e.spacing(1)},actionDiv:{textAlign:"center"}}}))(A)}}]);
//# sourceMappingURL=14.3c51bd28.chunk.js.map