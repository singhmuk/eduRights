(this["webpackJsonpmern-stack-client"]=this["webpackJsonpmern-stack-client"]||[]).push([[191],{140:function(e,a,n){"use strict";n.d(a,"a",(function(){return u}));var t=n(45),l=n(28),r=n(136),i=n(137),s=n(139),c=n(0),d=n.n(c),m=n(138),o=n.n(m),u=(n(59),function(e){function a(e){var n;return Object(t.a)(this,a),(n=Object(r.a)(this,Object(i.a)(a).call(this,e))).highlight=function(){n.ref&&n.ref.current&&o.a.highlightElement(n.ref.current)},n.ref=d.a.createRef(),n}return Object(s.a)(a,e),Object(l.a)(a,[{key:"componentDidMount",value:function(){this.highlight()}},{key:"componentDidUpdate",value:function(){this.highlight()}},{key:"render",value:function(){var e=this.props,a=e.code,n=(e.plugins,e.language);return d.a.createElement("pre",{className:"code-prism"},d.a.createElement("code",{ref:this.ref,className:"language-".concat(n)},a.trim()))}}]),a}(d.a.Component))},141:function(e,a,n){},146:function(e,a,n){"use strict";n.d(a,"a",(function(){return m}));var t=n(0),l=n.n(t),r=n(26),i=n(297),s=n(295),c=n(114),d=Object(c.a)((function(e){return{root:{display:"flex"},paper:{marginRight:e.spacing(2)},line:{textDecoration:"none"}}}));function m(){var e=d();return l.a.createElement("div",{className:e.root},l.a.createElement(s.a,null,l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/infoMl",className:e.line},"InfoMl")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/gredient_decents",className:e.line},"Gredient Decents")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/training",className:e.line},"Traning")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/regularizations",className:e.line},"Regularizations")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/featuresEng",className:e.line},"FeaturesEng")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/adaboost",className:e.line},"Adaboots")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/greedSearch",className:e.line},"Greed Search")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/perceptron",className:e.line},"Perceptron")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/pcaPy",className:e.line},"PCA")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/leanearRegression",className:e.line},"Leanear Regression")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/logisticReg",className:e.line},"Logistic Regression")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/lda",className:e.line},"Lda")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/knn",className:e.line},"Knn")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/k_meanClustring",className:e.line},"K_Mean")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/naiveBar",className:e.line},"Naive Bayes")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/randomForest",className:e.line},"Random Forest")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/decisiontree",className:e.line},"Decision Tree")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/svmPy",className:e.line},"SVM")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/numpyPy",className:e.line},"Numpy")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/pandas",className:e.line},"Pandas")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/bagging",className:e.line},"Matplotlib")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/logisticRegrations",className:e.line},"Scikit Learn")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/regrations",className:e.line},"SciPy")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/libraries",className:e.line},"OpenCV")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/capture",className:e.line},"Capture")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/joinImages",className:e.line},"JoinImages")),l.a.createElement("br",null),"Deep Learning",l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/superwise",className:e.line},"Superwise"))),l.a.createElement("div",null))}},510:function(e,a,n){"use strict";n.r(a);var t=n(45),l=n(28),r=n(136),i=n(137),s=n(139),c=n(0),d=n.n(c),m=n(138),o=n.n(m),u=n(120),p=n(57),g=n(296),f=n(5),E=(n(141),n(146)),h=n(140),b={backgroundColor:"#F0F8FF",padding:"1px",fontSize:"16px"},y="\na = [1, 7, 2]\n\nmyvar = pd.Series(a)\nprint(myvar)".trim(),v='\ndata = {\n  "calories": [420, 380, 390],\n  "duration": [50, 40, 45]\n}\n\ndf = pd.DataFrame(data)\ndf\n\n\ndf.loc[0]                                                                             #refer to the row index.\ndf = pd.DataFrame(data, index = ["day1", "day2", "day3"])                             #name your own indexes.\ndf.loc["day2"]                                                                        #refer to the named index:\n'.trim(),x="\ndf = pd.read_json('data.json')\n\nprint(df.to_string()) \n".trim(),w='\ndf.dropna()                                                             //Remove rows that contain empty cells.\ndf.fillna(130, inplace = True)                                          //Replace NULL values with the number 130.\ndf["Calories"].fillna(130, inplace = True)                              //Replace Only For Specified Columns.\n\nx = df["Calories"].mean()                                               //Find MEAN, and replace any empty values with it.\ndf["Calories"].fillna(x, inplace = True)\n'.trim(),_="\ndf['Date'] = pd.to_datetime(df['Date'])                                 //Convert to date.\ndf.dropna(subset=['Date'], inplace = True)                              //Remove rows with a NULL value in the \"Date\" column.\n".trim(),D='\nfor x in df.index:\n  if df.loc[x, "Duration"] > 120:\n    df.loc[x, "Duration"] = 120\n\n    \ndf.drop_duplicates(inplace = True)                                      //Remove all duplicates.\n'.trim(),k="\ndf.corr()                                                               //Relationship between the columns.\n".trim(),N="\ndf.plot()\nplt.show()                                                         \n\n\ndf.plot(kind = 'scatter', x = 'Duration', y = 'Calories')\n\ndf[\"Duration\"].plot(kind = 'hist')\n".trim(),j="\nimport pandas as pd\nX = music_data = pd.read_csv('music.csv')\nX \ny = music_data['genre']\n".trim(),F="\nimport pandas as pd \nfrom sklearn.tree import DecisionTreeClassifier\n\nmusic_data = pd.read_csv('music.csv')\nx = music_data.drop(columns=['genre'])\ny = music_data['genre']\n\nmodel = DecisionTreeClassifier()\nmodel.fit(x, y)\nmusic_data\n\npredictions = model.predict([21, 1], [22, 0])\npredictions\n".trim(),C='\ndf = pd.read_csv("pandas.csv")\npd.read_csv("pandas.csv", skiprows=1)\npd.read_csv(\'pandas.csv\', nrows=2)\npd.read_csv("pandas.csv", header=1)                                                       #skiprows and header are same\npd.read_csv("pandas.csv", na_values=["n.a.", "not available"])\npd.read_csv("pandas.csv", header=None, names = ["ticker","eps","revenue","people"])\npd.read_csv(\'pandas.csv\',header=0, parse_dates=[0], index_col=0, squeeze=True)\npd.read_csv(\'pandas.csv\',  na_values={\'eps\': [\'not available\'],\'revenue\': [-1],\'people\': [\'not available\',\'n.a.\']})\n    \n    \ndf.to_csv("new.csv", index=False)                                                             #Write to CSV\ndf.to_csv("new.csv", columns=["tickers","price"], index=False)\n\npd.read_excel("stock_data.xlsx","Sheet1")                                                     #Read Excel\ndf.to_excel("new.xlsx", sheet_name="stocks", index=False, startrow=2, startcol=1)             #Write to Excel\n\n\ndf.to_string()                                                                                #Print the entire DataFrame.\ndf=pd.options.display.max_rows                                                                #Maximum returned rows\ndf=pd.options.display.max_rows = 9999                       #Increase max. number of rows to display the entire DataFrame\n\n'.trim(),S="\ndf_stocks = pd.DataFrame({\n    'tickers': ['GOOGL', 'WMT', 'MSFT'],\n    'price': [845, 65, 64 ],\n    'pe': [30.37, 14.26, 30.97],\n    'eps': [27.82, 4.61, 2.12]\n})\n\ndf_weather =  pd.DataFrame({\n    'day': ['1/1/2017','1/2/2017','1/3/2017'],\n    'temperature': [32,35,28],\n    'event': ['Rain', 'Sunny', 'Snow']\n})\n\n\nwith pd.ExcelWriter('stocks_weather.xlsx') as writer:\n    df_stocks.to_excel(writer, sheet_name=\"stocks\")\n    df_weather.to_excel(writer, sheet_name=\"weather\")\n".trim(),T="\ndf.fillna(0)                                                                          #fillna\ndf.fillna(130, inplace = True)                                                        #Replace NULL values with the 130.\ndf[\"Calories\"].fillna(130, inplace = True)                                            #Replace Only For Specified Columns.\n\n\nnew_df = df.fillna(method=\"ffill\")                                                    #determine how to fill na values.\nnew_df = df.fillna(method=\"bfill\")\n\n\n#Use of axis\ndf.fillna(method=\"bfill\", axis=\"columns\")                                             # axis is either \"index\" or \"columns\"\ndf.fillna(method=\"ffill\",limit=1)                                                     #limit parameter\ndf.interpolate()                                                                      #interpolate\ndf.interpolate(method=\"time\")\n\ndf.dropna()                                                                           #dropna\ndf.drop_duplicates()\n\n\n#Inserting Missing Dates\ndt = pd.date_range(\"01-01-2017\",\"01-11-2017\")\nidx = pd.DatetimeIndex(dt)\ndf.reindex(idx)\n\n\ndf.replace(-99999, value=np.NaN)                                                      #Handling Missing Data-replace method\ndf.replace(to_replace=[-99999,-88888], value=0)                                       #Replacing list with single value\ndf.replace({'temperature': -99999,'windspeed': -99999,'event': '0'}, np.nan)          #Replacing per column\n          \nnew_df = df.replace({-99999: np.nan, 'no event': 'Sunny', })                          #Replacing by using mapping\ndf['area'][0] = 50                                                                    #Update data.\n\ndf=pd.Series([4.5, 7.2, -5.3, 3.6], index=['d', 'b', 'a', 'c'])                       #reindex\n\n".trim(),R="\ndf.replace({'temperature': '[A-Za-z]', 'windspeed': '[a-z]'},'', regex=True) \n\n\n3Replacing list with another list\n    df = pd.DataFrame({\n    'score': ['exceptional','average', 'good', 'poor', 'average', 'exceptional'],\n    'student': ['rob', 'maya', 'parthiv', 'tom', 'julian', 'erica']\n  })\n\n    df.replace(['poor', 'average', 'good', 'exceptional'], [1,2,3,4])\n".trim(),P="\n    g.get_group('mumbai')\n    g.max()\n    g.min()\n    g.mean()\n    g.describe()\n    g.size()\n    g.count()\n    g.plot()\n".trim(),M="\ndef grouper(df, idx, col):\n    if 80 <= df[col].loc[idx] <= 90:\n        return '80-90'\n    elif 50 <= df[col].loc[idx] <= 60:\n        return '50-60'\n    else:\n        return 'others'\n        \ng = df.groupby(lambda x: grouper(df, x, 'temperature'))\nfor key, d in g:\nprint(\"Group by Key: {}\n\".format(key))\nprint(d)\n".trim(),q='\nindia_weather = pd.DataFrame({\n  "city": ["mumbai","delhi","banglore"],\n  "temperature": [32,45,30],\n  "humidity": [80, 60, 78]\n})\n\ndf = pd.concat([india_weather, us_weather])\n\n\n#Concatenation Using Index.\ntemperature_df = pd.DataFrame({\n  "city": ["mumbai","delhi","banglore"],\n  "temperature": [32,45,30],\n}, index=[0,1,2])\n\npd.concat([temperature_df,windspeed_df],axis=1)\n\n\n#Concatenate dataframe with series\ns = pd.Series(["Humid","Dry","Rain"], name="event")\npd.concat([temperature_df,s],axis=1)\n'.trim(),I="\npd.concat([india_weather, us_weather], ignore_index=True)\n\n\n#pivot\ndf.pivot(index='city',columns='date')\ndf.pivot(index='city',columns='date',values=\"humidity\")\n\nf.pivot_table(index=\"city\",columns=\"date\", margins=True,aggfunc=np.sum)                                 #margins\ndf.pivot_table(index=pd.Grouper(freq='M',key='date'),columns='city')                                    #grouper\n\n\n#Melt\npd.melt(df, id_vars=[\"day\"], var_name='city', value_name='temperature')\n\n\n#Reshape dataframe using stack/unstack\ndf = pd.read_excel(\"stocks.xlsx\",header=[0,1])\ndf.stack()\ndf.stack(level=0)\ndf_stacked.unstack()\n\n\npd.read_excel(\"stocks_3_levels.xlsx\",header=[0,1,2])                                      #3 levels of column headers\ndf2.stack(level=1)\n".trim(),A="\npd.crosstab(df.Nationality,df.Handedness)\nMargins: pd.crosstab(df.Sex,df.Handedness, margins=True)\nNormalize: pd.crosstab(df.Sex, df.Handedness, normalize='index')\nAggfunc and Values: pd.crosstab(df.Sex, df.Handedness, values=df.Age, aggfunc=np.average)\n".trim(),O="\n#Partial Date Index\ndf['2017-06-30']\ndf['2017-06'].Close.mean() \n\n\n#Date Range\ndf['2017-01-08':'2017-01-03']\n\ndf['Close'].resample('M').mean().head()                                                       #Resampling\n\n#Finding missing dates from datetimeindex\ndaily_index = pd.date_range(start=\"6/1/2016\",end=\"6/30/2016\",freq='D')\ndaily_index.difference(df.index)\n\n\n#generating DatetimeIndex with periods argument\npd.date_range('1/1/2011', periods=72, freq='H')\n".trim(),L='\ndf1 = pd.DataFrame({\n  "city": ["new york","chicago","orlando"],\n  "temperature": [21,14,35],\n})\n\ndf2 = pd.DataFrame({\n  "city": ["chicago","new york","orlando"],\n  "humidity": [65,68,75],\n})\n\ndf3 = pd.merge(df1, df2, on="city")\n\ndf3=pd.merge(df1,df2,on="city",how="outer",indicator=True)\ndf3= pd.merge(df1,df2,on="city",how="outer", suffixes=(\'_first\',\'_second\'))\n'.trim(),z="\nimport pandas as pd\nimport sqlalchemy\n\nengine = sqlalchemy.create_engine('mysql+pymysql://root:@localhost:3306/application')\n\ndf = pd.read_sql_table('customers',engine)\n\ndf = pd.read_sql_table('customers', engine, columns=[\"name\"])       #Read only selected columns\n\n\n#Join two tables and read them in a dataframe using read_sql_query\ndf = pd.read_sql_query(\"select id,name from customers\",engine)      \n\n\nquery = '''\n SELECT customers.name, customers.phone_number, orders.name, orders.amount\n FROM customers INNER JOIN orders\n ON customers.id=orders.customer_id\n'''\npd.read_sql(query,engine)                           #read_sql is a wrapper around read_sql_query and read_sql_table\n\ndf = pd.read_csv(\"customers.csv\")                   #Write to mysql database using to_sql\n\ndf = pd.read_csv(\"customers.csv\")\n\ndf.rename(columns={\n    'Customer Name': 'name',\n    'Customer Phone': 'phone_number'\n}, inplace=True)\n\n\n\n#to_sql has different parameters such as chunksize which allows to write data in chunks. Useful when size is huge\ndf.to_sql(\n    name='customers', # database table name\n    con=engine,\n    if_exists='append',\n    index=False                                                             \n)                                   \n".trim(),U="\ndf.shape\ndf.values\ndf.head(10)\ndf.describe()\ndf.memory_usage()\ndf.memory_usage(deep=True)\ndf.loc[1:3]\ndf.drop_duplicates()\ndf.count()\ndf.tail() \ndf.info()\ndf.sort_index()\ndf.isna()                            #Returns a dataframe filled with boolean values with true indicating missing values.\ndf.isnull().sum()                    #Calculate the number of missing values in each column.\n".trim(),W="\nimport numpy as np\n\nseries = pd.Series([1,2,np.nan,4])\n\nseries_2=series.copy(deep=True)\nprint(series_2)\n".trim(),H="\n#Add rows\ndict = {'Name':['Martha', 'Tim', 'Rob', 'Georgia'],\n        'Maths':[87, 91, 97, 95],\n        'Science':[83, 99, 84, 76]\n       }\n  \ndf = pd.DataFrame(dict)\n  \ndf2 = {'Name': 'Amy', 'Maths': 89, 'Science': 93}\ndf = df.append(df2, ignore_index = True)\ndf\n\ndf.reset_index()\n\n\n\n#add columns\ndata = {'Name':['Martha', 'Tim', 'Rob', 'Georgia'],\n        'Maths':[87, 91, 97, 95],\n        'Science':[83, 99, 84, 76]\n       }\n\ndf = pd.DataFrame(data)\n\naddress = ['Delhi', 'Bangalore', 'Chennai', 'Patna']\n\ndf['Address'] = address\ndf\n\n\n\n#Add An Index\ndata = pd.read_csv(\"areas.csv\")\ndata.set_index(\"area\", inplace = True)                                                      #Setting area as index column\ndata.head()\n".trim(),G=function(e){function a(){return Object(t.a)(this,a),Object(r.a)(this,Object(i.a)(a).apply(this,arguments))}return Object(s.a)(a,e),Object(l.a)(a,[{key:"componentDidMount",value:function(){setTimeout((function(){return o.a.highlightAll()}),0)}},{key:"render",value:function(){var e=this.props.classes;return d.a.createElement(u.a,{container:!0},d.a.createElement(u.a,{item:!0,xs:2},d.a.createElement(p.a,{className:e.paper},d.a.createElement("h4",null,d.a.createElement(E.a,null)))),d.a.createElement(u.a,{item:!0,xs:10},d.a.createElement(p.a,{className:e.paper},d.a.createElement(g.a,null,d.a.createElement("h3",null,"Pandas (Data analysis)"),d.a.createElement("ul",null,d.a.createElement("li",null,"Provides functions to make working with structured or tabular data fast, easy, and expressive."),d.a.createElement("li",null,"Pandas allows us to analyze big data and make conclusions based on statistical theories."),d.a.createElement("li",null,"Primary objects is DataFrame and data.Series."),d.a.createElement("li",null,"Pandas find correlation between two/ more columns."),d.a.createElement("li",null,"Pandas is designed for working with tabular/ heterogeneous data."),d.a.createElement("li",null,"Pandas blends the high-performance, array-computing ideas of NumPy with the flexible data manipulation capabilities of spreadsheets and relational databases."),d.a.createElement("li",null,"Pandas has a special Categorical type for holding data that uses the integer-based categorical representation or encoding.")),d.a.createElement("br",null),d.a.createElement("br",null),d.a.createElement("ul",null,d.a.createElement("li",null,d.a.createElement("b",null,"Data Science/ Data Analytics: "),"Is a process of analyzing large set of data point to get answer on questions releted to that data set."),d.a.createElement("br",null),d.a.createElement("li",null,d.a.createElement("b",null,"Data Munging/ Data Wrangling: "),"It's a Process of cleaning messy data.")),d.a.createElement("br",null),d.a.createElement("h3",null,"Dataframe"),"Different ways of creating dataframe:",d.a.createElement("ul",null,d.a.createElement("li",null,"Using CSV"),d.a.createElement("li",null,"Using excel"),d.a.createElement("li",null,"From python dictionary"),d.a.createElement("li",null,"From list of tuples"),d.a.createElement("li",null,"From list of dictionaries")),d.a.createElement("br",null),d.a.createElement("h3",null,"What Are The Most Important Features Of The Pandas Library?"),d.a.createElement("ul",null,d.a.createElement("li",null,"Data Alignment"),d.a.createElement("li",null,"Merge and join"),d.a.createElement("li",null,"Memory Efficient"),d.a.createElement("li",null,"Time series"),d.a.createElement("li",null,"Reshaping")),d.a.createElement("br",null),d.a.createElement("h3",null,"Explain Categorical Data in Pandas?"),d.a.createElement("ul",null,d.a.createElement("li",null,"Categorical data refers to real-time data that can be repetitive for instance, data values under categories such as country, gender, codes will always be repetitive."),d.a.createElement("li",null,"Categorical values also take only a limited and fixed number of possible values. "),d.a.createElement("li",null,"Numerical operations cannot be performed on such data. All values of categorical data in pandas are either in categories or np.nan.")),d.a.createElement("br",null),d.a.createElement("b",null,"Import file."),d.a.createElement("br",null),d.a.createElement("div",{style:b},d.a.createElement(h.a,{code:C,language:"js",plugins:["line-numbers"]})),d.a.createElement("br",null),d.a.createElement("br",null),d.a.createElement("b",null,"Methods:"),d.a.createElement("div",{style:b},d.a.createElement(h.a,{code:U,language:"js",plugins:["line-numbers"]})),d.a.createElement("br",null),d.a.createElement("br",null),d.a.createElement("b",null,"Write two dataframes to two separate sheets in excel"),d.a.createElement("div",{style:b},d.a.createElement(h.a,{code:S,language:"js",plugins:["line-numbers"]})),d.a.createElement("br",null),d.a.createElement("h3",null,"Handle Missing Data"),d.a.createElement("div",{style:b},d.a.createElement(h.a,{code:T,language:"js",plugins:["line-numbers"]})),d.a.createElement("br",null),d.a.createElement("b",null,"Data Structures: "),"2",d.a.createElement("ul",null,d.a.createElement("li",null,d.a.createElement("b",null,"Series: "),"Is a 1D array-like object containing a sequence of values and an associated array of data labels, called its index."),d.a.createElement("br",null),d.a.createElement("li",null,d.a.createElement("b",null,"DataFrame: "),"A DataFrame represents a rectangular table of data and contains an ordered collection of columns, each of which can be a different value type (numeric, string, boolean, etc.). DataFrame has both a row and column index."),d.a.createElement("br",null),d.a.createElement("li",null,d.a.createElement("b",null,"Panel: "),"Is a 3-dimensional DS and includes items such as major_axis and minor_axis.")),d.a.createElement("br",null),d.a.createElement("h3",null,"Series"),d.a.createElement("div",{style:b},d.a.createElement(h.a,{code:y,language:"js",plugins:["line-numbers"]})),d.a.createElement("br",null),d.a.createElement("h3",null,"DataFrames"),d.a.createElement("div",{style:b},d.a.createElement(h.a,{code:v,language:"js",plugins:["line-numbers"]})),d.a.createElement("br",null),d.a.createElement("h3",null,"How can we create copy of series in Pandas?"),"copy() Make a deep copy, including a copy of the data and the indices. With deep=False neither the indices or the data are copied.",d.a.createElement("br",null),"Note that when deep=True data is copied, actual python objects will not be copied recursively, only the reference to the object.",d.a.createElement("br",null),d.a.createElement("div",{style:b},d.a.createElement(h.a,{code:W,language:"js",plugins:["line-numbers"]})),d.a.createElement("br",null),d.a.createElement("h3",null,"How Will You Add An Index, Row, Or Column To A Dataframe In Pandas?"),d.a.createElement("ul",null,d.a.createElement("li",null,d.a.createElement("b",null,".loc (): "),"Is label based."),d.a.createElement("li",null,d.a.createElement("b",null,".iloc (): "),"Integer based."),d.a.createElement("li",null,d.a.createElement("b",null,".ix(): "),"Both label and integer based."),d.a.createElement("br",null),d.a.createElement("li",null,"To add columns to the DataFrame, we can use .loc () or .iloc ().")),d.a.createElement("div",{style:b},d.a.createElement(h.a,{code:H,language:"js",plugins:["line-numbers"]})),d.a.createElement("br",null),d.a.createElement("h3",null,"Pandas Read JSON"),d.a.createElement("div",{style:b},d.a.createElement(h.a,{code:x,language:"js",plugins:["line-numbers"]})),d.a.createElement("br",null),d.a.createElement("h3",null,"Cleaning Empty Cells"),d.a.createElement("div",{style:b},d.a.createElement(h.a,{code:w,language:"js",plugins:["line-numbers"]})),d.a.createElement("br",null),d.a.createElement("h3",null,"Cleaning Data of Wrong Format"),d.a.createElement("div",{style:b},d.a.createElement(h.a,{code:_,language:"js",plugins:["line-numbers"]})),d.a.createElement("br",null),d.a.createElement("h3",null,"Pandas - Fixing Wrong Data"),d.a.createElement("div",{style:b},d.a.createElement(h.a,{code:D,language:"js",plugins:["line-numbers"]})),d.a.createElement("br",null),d.a.createElement("h3",null,"Data Correlations"),"The corr() method calculates the relationship between each column in our data set.",d.a.createElement("br",null),d.a.createElement("br",null),d.a.createElement("b",null,"The number varies from -1 to 1."),d.a.createElement("ul",null,d.a.createElement("li",null,"1 means that there is a 1 to 1 relationship (a perfect correlation), and for this data set, each time a value went up in the first column, the other one went up as well."),d.a.createElement("li",null,"0.9 is also a good relationship, and if you increase one value, the other will probably increase as well."),d.a.createElement("li",null,"-0.9 would be just as good relationship as 0.9, but if you increase one value, the other will probably go down."),d.a.createElement("li",null,"0.2 means NOT a good relationship, meaning that if one value goes up does not mean that the other will.")),d.a.createElement("div",{style:b},d.a.createElement(h.a,{code:k,language:"js",plugins:["line-numbers"]})),d.a.createElement("br",null),d.a.createElement("h3",null,"Plotting"),"Uses the plot() method to create diagrams.",d.a.createElement("br",null),d.a.createElement("br",null),"Specify that you want a scatter plot with the kind argument:",d.a.createElement("br",null),"kind = 'scatter'",d.a.createElement("br",null),"A scatter plot needs an x- and a y-axis.",d.a.createElement("br",null),'Will use "Duration" for the x-axis and "Calories" for the y-axis.',d.a.createElement("br",null),"Include the x and y arguments like this:",d.a.createElement("br",null),"x = 'Duration', y = 'Calories'",d.a.createElement("br",null),d.a.createElement("br",null),d.a.createElement("b",null,"Histogram"),d.a.createElement("br",null),"Use the kind argument to specify that you want a histogram:",d.a.createElement("br",null),d.a.createElement("br",null),"kind = 'hist'",d.a.createElement("br",null),"A histogram needs only one column.",d.a.createElement("br",null),"A histogram shows us the frequency of each interval, e.g. how many workouts lasted between 50 and 60 minutes?",d.a.createElement("br",null),'Will use the "Duration" column to create the histogram.',d.a.createElement("br",null),d.a.createElement("div",{style:b},d.a.createElement(h.a,{code:N,language:"js",plugins:["line-numbers"]})),d.a.createElement("h3",null,"Preparing the Data"),d.a.createElement("div",{style:b},d.a.createElement(h.a,{code:j,language:"js",plugins:["line-numbers"]})),d.a.createElement("h3",null,"Learning and Predicting"),d.a.createElement("div",{style:b},d.a.createElement(h.a,{code:F,language:"js",plugins:["line-numbers"]})),d.a.createElement("br",null),d.a.createElement("h3",null,"Regex"),"when windspeed is 6 mph, 7 mph etc. & temperature is 32 F, 28 F etc.",d.a.createElement("div",{style:b},d.a.createElement(h.a,{code:R,language:"js",plugins:["line-numbers"]})),d.a.createElement("br",null),d.a.createElement("br",null),d.a.createElement("b",null,"SELECT * from weather_data GROUP BY city"),d.a.createElement("br",null),d.a.createElement("div",{style:b},d.a.createElement(h.a,{code:P,language:"js",plugins:["line-numbers"]})),d.a.createElement("br",null),d.a.createElement("h3",null,"Group data using custom function"),"Let's say you want to group your data using custom function. Here the requirement is to create three groups.",d.a.createElement("ul",null,d.a.createElement("li",null,"1.Days when temperature was between 80 and 90."),d.a.createElement("li",null,"2.Days when it was between 50 and 60."),d.a.createElement("li",null,"3.Days when it was anything else.")),d.a.createElement("div",{style:b},d.a.createElement(h.a,{code:M,language:"js",plugins:["line-numbers"]})),d.a.createElement("br",null),d.a.createElement("h3",null,"Basic Concatenation"),d.a.createElement("div",{style:b},d.a.createElement(h.a,{code:q,language:"js",plugins:["line-numbers"]})),d.a.createElement("br",null),d.a.createElement("h3",null,"Ignore Index"),d.a.createElement("b",null,"Pivot: "),"Allows to Transform/ reshape data.",d.a.createElement("br",null),"Pivot table used tosummarize and aggregate data inside dataframe.",d.a.createElement("br",null),d.a.createElement("br",null),d.a.createElement("b",null,"Melt:"),"Used to transform/ reshape data.",d.a.createElement("br",null),d.a.createElement("div",{style:b},d.a.createElement(h.a,{code:I,language:"js",plugins:["line-numbers"]})),d.a.createElement("br",null),d.a.createElement("h3",null,"Crosstab"),d.a.createElement("div",{style:b},d.a.createElement(h.a,{code:A,language:"js",plugins:["line-numbers"]})),d.a.createElement("br",null),d.a.createElement("h3",null,"Time Series Analysis"),"Time Series is a set of data points indexed in time order.",d.a.createElement("br",null),d.a.createElement("br",null),d.a.createElement("b",null,"Benefits of DatetimeIndex:"),d.a.createElement("ul",null,d.a.createElement("li",null,"1.Partial Date Index: Select Specific Months Data."),d.a.createElement("li",null,"2.Select Date Range.")),d.a.createElement("br",null),d.a.createElement("br",null),d.a.createElement("b",null,"Benefits of having DatetimeIndex:"),d.a.createElement("ul",null,d.a.createElement("li",null,"Generating DatetimeIndex with periods argument.")),d.a.createElement("div",{style:b},d.a.createElement(h.a,{code:O,language:"js",plugins:["line-numbers"]})),d.a.createElement("br",null),d.a.createElement("h3",null,"Merge DataFrame"),d.a.createElement("div",{style:b},d.a.createElement(h.a,{code:L,language:"js",plugins:["line-numbers"]})),d.a.createElement("br",null),d.a.createElement("h3",null,"sqlalchemy"),d.a.createElement("div",{style:b},d.a.createElement(h.a,{code:z,language:"js",plugins:["line-numbers"]}))))))}}]),a}(c.Component);a.default=Object(f.a)((function(e){return{paper:{margin:e.spacing(1),padding:e.spacing(1)},smMargin:{margin:e.spacing(1)},actionDiv:{textAlign:"center"}}}))(G)}}]);
//# sourceMappingURL=191.44bda9b9.chunk.js.map