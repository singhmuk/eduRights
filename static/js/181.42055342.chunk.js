(this["webpackJsonpmern-stack-client"]=this["webpackJsonpmern-stack-client"]||[]).push([[181],{140:function(e,a,n){"use strict";n.d(a,"a",(function(){return d}));var t=n(45),l=n(28),r=n(136),i=n(137),s=n(139),c=n(0),o=n.n(c),m=n(138),u=n.n(m),d=(n(59),function(e){function a(e){var n;return Object(t.a)(this,a),(n=Object(r.a)(this,Object(i.a)(a).call(this,e))).highlight=function(){n.ref&&n.ref.current&&u.a.highlightElement(n.ref.current)},n.ref=o.a.createRef(),n}return Object(s.a)(a,e),Object(l.a)(a,[{key:"componentDidMount",value:function(){this.highlight()}},{key:"componentDidUpdate",value:function(){this.highlight()}},{key:"render",value:function(){var e=this.props,a=e.code,n=(e.plugins,e.language);return o.a.createElement("pre",{className:"code-prism"},o.a.createElement("code",{ref:this.ref,className:"language-".concat(n)},a.trim()))}}]),a}(o.a.Component))},141:function(e,a,n){},146:function(e,a,n){"use strict";n.d(a,"a",(function(){return m}));var t=n(0),l=n.n(t),r=n(26),i=n(297),s=n(295),c=n(114),o=Object(c.a)((function(e){return{root:{display:"flex"},paper:{marginRight:e.spacing(2)},line:{textDecoration:"none"}}}));function m(){var e=o();return l.a.createElement("div",{className:e.root},l.a.createElement(s.a,null,l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/infoMl",className:e.line},"InfoMl")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/gredient_decents",className:e.line},"Gredient Decents")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/training",className:e.line},"Traning")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/regularizations",className:e.line},"Regularizations")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/featuresEng",className:e.line},"FeaturesEng")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/adaboost",className:e.line},"Adaboots")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/greedSearch",className:e.line},"Greed Search")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/perceptron",className:e.line},"Perceptron")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/pcaPy",className:e.line},"PCA")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/leanearRegression",className:e.line},"Leanear Regression")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/logisticReg",className:e.line},"Logistic Regression")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/lda",className:e.line},"Lda")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/knn",className:e.line},"Knn")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/k_meanClustring",className:e.line},"K_Mean")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/naiveBar",className:e.line},"Naive Bayes")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/randomForest",className:e.line},"Random Forest")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/decisiontree",className:e.line},"Decision Tree")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/svmPy",className:e.line},"SVM")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/numpyPy",className:e.line},"Numpy")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/pandas",className:e.line},"Pandas")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/bagging",className:e.line},"Matplotlib")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/logisticRegrations",className:e.line},"Scikit Learn")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/regrations",className:e.line},"SciPy")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/libraries",className:e.line},"OpenCV")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/capture",className:e.line},"Capture")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/joinImages",className:e.line},"JoinImages")),l.a.createElement("br",null),"Deep Learning",l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/superwise",className:e.line},"Superwise"))),l.a.createElement("div",null))}},512:function(e,a,n){"use strict";n.r(a);var t=n(45),l=n(28),r=n(136),i=n(137),s=n(139),c=n(0),o=n.n(c),m=n(138),u=n.n(m),d=n(120),f=n(57),p=n(296),E=n(5),g=(n(141),n(146)),h=n(140),b={backgroundColor:"#F0F8FF",padding:"1px",fontSize:"16px"},_='\nimport numpy as np\n\n# Decision stump used as weak classifier\nclass DecisionStump:\n    def __init__(self):\n        self.polarity = 1\n        self.feature_idx = None\n        self.threshold = None\n        self.alpha = None\n\n    def predict(self, X):\n        n_samples = X.shape[0]\n        X_column = X[:, self.feature_idx]\n        predictions = np.ones(n_samples)\n        if self.polarity == 1:\n            predictions[X_column < self.threshold] = -1\n        else:\n            predictions[X_column > self.threshold] = -1\n\n        return predictions\n\n\nclass Adaboost:\n    def __init__(self, n_clf=5):\n        self.n_clf = n_clf\n        self.clfs = []\n\n    def fit(self, X, y):\n        n_samples, n_features = X.shape\n\n        w = np.full(n_samples, (1 / n_samples))                          # Initialize weights to 1/N\n\n        self.clfs = []\n        \n        for _ in range(self.n_clf):                                      # Iterate through classifiers\n            clf = DecisionStump()\n            min_error = float("inf")\n\n            for feature_i in range(n_features):                          # greedy search to find best threshold and feature\n                X_column = X[:, feature_i]\n                thresholds = np.unique(X_column)\n\n                for threshold in thresholds:\n                    p = 1                                                # predict with polarity 1\n                    predictions = np.ones(n_samples)\n                    predictions[X_column < threshold] = -1\n\n                    misclassified = w[y != predictions]                  # Error = sum of weights of misclassified samples\n                    error = sum(misclassified)\n\n                    if error > 0.5:\n                        error = 1 - error\n                        p = -1\n\n                    if error < min_error:                               # store the best configuration\n                        clf.polarity = p\n                        clf.threshold = threshold\n                        clf.feature_idx = feature_i\n                        min_error = error\n\n            # calculate alpha\n            EPS = 1e-10\n            clf.alpha = 0.5 * np.log((1.0 - min_error + EPS) / (min_error + EPS))\n\n            predictions = clf.predict(X)                                         # calculate predictions and update weights\n\n            w *= np.exp(-clf.alpha * y * predictions)\n            # Normalize to one\n            w /= np.sum(w)\n\n            self.clfs.append(clf)                                               # Save classifier\n\n    def predict(self, X):\n        clf_preds = [clf.alpha * clf.predict(X) for clf in self.clfs]\n        y_pred = np.sum(clf_preds, axis=0)\n        y_pred = np.sign(y_pred)\n\n        return y_pred\n'.trim(),y='\nif __name__ == "__main__":\n    from sklearn import datasets\n    from sklearn.model_selection import train_test_split\n\n    def accuracy(y_true, y_pred):\n        accuracy = np.sum(y_true == y_pred) / len(y_true)   \n        return accuracy\n\n    data = datasets.load_breast_cancer()\n    X, y = data.data, data.target\n\n    y[y == 0] = -1\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=5)\n\n    # Adaboost classification with 5 weak classifiers\n    clf = Adaboost(n_clf=5)\n    clf.fit(X_train, y_train)\n    y_pred = clf.predict(X_test)\n\n    acc = accuracy(y_test, y_pred)\n    print("Accuracy:", acc)\n    '.trim(),N="\nPerformance of stump = 1/2 * log(1-Total Error) / Total Error\n\nNew weights = old weight * e(+-Performanse)\n    where, + for Misclassification\n           - for Right classification\n".trim(),w=function(e){function a(){return Object(t.a)(this,a),Object(r.a)(this,Object(i.a)(a).apply(this,arguments))}return Object(s.a)(a,e),Object(l.a)(a,[{key:"componentDidMount",value:function(){setTimeout((function(){return u.a.highlightAll()}),0)}},{key:"render",value:function(){var e=this.props.classes;return o.a.createElement(d.a,{container:!0},o.a.createElement(d.a,{item:!0,xs:2},o.a.createElement(f.a,{className:e.paper},o.a.createElement("h4",null,o.a.createElement(g.a,null)))),o.a.createElement(d.a,{item:!0,xs:10},o.a.createElement(f.a,{className:e.paper},o.a.createElement(p.a,null,o.a.createElement("h3",null,"Boosting Types"),o.a.createElement("ul",null,o.a.createElement("li",null,o.a.createElement("b",null,"1. Adaboost: ")),o.a.createElement("li",null,o.a.createElement("b",null,"2. Gradient Boosting: "),"Instead of Weights updation, here gradient (residuals, loss) is passed in next model."),o.a.createElement("br",null),o.a.createElement("li",null,o.a.createElement("b",null,"3. Extream Gradient Boosting: ")),o.a.createElement("ul",null,o.a.createElement("li",null,"Much similar to GB."),o.a.createElement("li",null,"2nd order Derivatives of Loss function."),o.a.createElement("li",null,"High Performance."),o.a.createElement("li",null,"Fast training."),o.a.createElement("li",null,"Advanced L1 and L2 Loass Regularization."),o.a.createElement("li",null,"Parallel and Distributed computing (DMLC)."),o.a.createElement("li",null,"It handle missing values."),o.a.createElement("li",null,"Cache Optimisation"),o.a.createElement("li",null,"It has many hyperparameters. reg_alpha, reg_lambda."))),o.a.createElement("br",null),o.a.createElement("h3",null,"Adaboost (Adaptive boosting)"),o.a.createElement("ul",null,o.a.createElement("li",null,"Used for Classification and Regression."),o.a.createElement("li",null,"Sequencial boosting."),o.a.createElement("li",null,"AdaBoost is one of the first boosting algorithms to be adapted in solving practices. Adaboost helps you combine multiple \u201cweak classifiers\u201d into a single \u201cstrong classifier\u201d."),o.a.createElement("li",null,"The weak learners in AdaBoost are decision trees with a single split, called ",o.a.createElement("b",null,"decision stumps"),"."),o.a.createElement("li",null,"AdaBoost works by putting more weight on difficult to classify instances and less on those already handled well."),o.a.createElement("li",null,"Weight increase for misclassification and weight decreses for right classifications."),o.a.createElement("li",null,"Used to exploit dependency between models."),o.a.createElement("li",null,"Stagewise additive MultiModeling using Multiclass Exponential Loss Function."),o.a.createElement("li",null,"Can handle missing values and outliner."),o.a.createElement("li",null,"Can handles mixed predictors as well (Quantitive and Qualitative).")),o.a.createElement("br",null),o.a.createElement("br",null),o.a.createElement("b",null,"Steps for Adaboost Algoritham:"),o.a.createElement("ul",null,o.a.createElement("li",null,"1. Initialize the weights as 1/n to every n observations."),o.a.createElement("li",null,"2. Select the 1 Feature according to Lowest Gini/Highest information Gain and calculate the total error."),o.a.createElement("li",null,"3. Calculate the Performance of the stump."),o.a.createElement("li",null,"4. Calculate the new weights for each misclassification(increase) and right classification(decrease)."),o.a.createElement("li",null,"5. Normalize the new weights so that sum of weight is 1."),o.a.createElement("li",null,"6. Repeat from step 2 to till configured number of estimators reacfied the accuracy achieved.")),o.a.createElement("br",null),o.a.createElement("div",{style:b},o.a.createElement(h.a,{code:N,language:"js",plugins:["line-numbers"]})),o.a.createElement("br",null),o.a.createElement("div",{style:b},o.a.createElement(h.a,{code:_,language:"js",plugins:["line-numbers"]})),o.a.createElement("br",null),o.a.createElement("h3",null,"Testing"),o.a.createElement("div",{style:b},o.a.createElement(h.a,{code:y,language:"js",plugins:["line-numbers"]}))))))}}]),a}(c.Component);a.default=Object(E.a)((function(e){return{paper:{margin:e.spacing(1),padding:e.spacing(1)},smMargin:{margin:e.spacing(1)},actionDiv:{textAlign:"center"}}}))(w)}}]);
//# sourceMappingURL=181.42055342.chunk.js.map