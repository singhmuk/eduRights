(this["webpackJsonpmern-stack-client"]=this["webpackJsonpmern-stack-client"]||[]).push([[48],{140:function(e,a,t){"use strict";t.d(a,"a",(function(){return d}));var n=t(45),l=t(28),r=t(136),s=t(137),c=t(139),i=t(0),o=t.n(i),m=t(138),u=t.n(m),d=(t(59),function(e){function a(e){var t;return Object(n.a)(this,a),(t=Object(r.a)(this,Object(s.a)(a).call(this,e))).highlight=function(){t.ref&&t.ref.current&&u.a.highlightElement(t.ref.current)},t.ref=o.a.createRef(),t}return Object(c.a)(a,e),Object(l.a)(a,[{key:"componentDidMount",value:function(){this.highlight()}},{key:"componentDidUpdate",value:function(){this.highlight()}},{key:"render",value:function(){var e=this.props,a=e.code,t=(e.plugins,e.language);return o.a.createElement("pre",{className:"code-prism"},o.a.createElement("code",{ref:this.ref,className:"language-".concat(t)},a.trim()))}}]),a}(o.a.Component))},141:function(e,a,t){},150:function(e,a,t){"use strict";t.d(a,"a",(function(){return m}));var n=t(0),l=t.n(n),r=t(26),s=t(297),c=t(295),i=t(114),o=Object(i.a)((function(e){return{root:{display:"flex"},paper:{marginRight:e.spacing(2)},line:{textDecoration:"none"}}}));function m(){var e=o();return l.a.createElement("div",{className:e.root},l.a.createElement(c.a,null,l.a.createElement(s.a,null,l.a.createElement(r.b,{to:"/introAngular",className:e.line},"AI")),l.a.createElement(s.a,null,l.a.createElement(r.b,{to:"/tensorflow",className:e.line},"Tensorflow")),l.a.createElement(s.a,null,l.a.createElement(r.b,{to:"/tensors",className:e.line},"Tensorboards")),l.a.createElement(s.a,null,l.a.createElement(r.b,{to:"/angCompiler",className:e.line},"Compiler")),l.a.createElement(s.a,null,l.a.createElement(r.b,{to:"/neural",className:e.line},"NeuralKeras")),l.a.createElement(s.a,null,l.a.createElement(r.b,{to:"/activationFunctions",className:e.line},"activationFuncs")),l.a.createElement(s.a,null,l.a.createElement(r.b,{to:"/loss",className:e.line},"Loss")),l.a.createElement(s.a,null,l.a.createElement(r.b,{to:"/gradientNeural",className:e.line},"GradientNeural")),l.a.createElement(s.a,null,l.a.createElement(r.b,{to:"/stochastic",className:e.line},"Stochastic")),l.a.createElement(s.a,null,l.a.createElement(r.b,{to:"/benchmarking",className:e.line},"Benchmarking")),l.a.createElement(s.a,null,l.a.createElement(r.b,{to:"/customer",className:e.line},"Customer")),l.a.createElement(s.a,null,l.a.createElement(r.b,{to:"/regularizationDeep",className:e.line},"Regularization Deep")),l.a.createElement(s.a,null,l.a.createElement(r.b,{to:"/imbalanced",className:e.line},"Imbalanced")),l.a.createElement(s.a,null,l.a.createElement(r.b,{to:"/imbalanced2",className:e.line},"Imbalanced2")),l.a.createElement(s.a,null,l.a.createElement(r.b,{to:"/convolutionals",className:e.line},"Convolutionals")),l.a.createElement(s.a,null,l.a.createElement(r.b,{to:"/data_augmentation",className:e.line},"data Augmentation")),l.a.createElement(s.a,null,l.a.createElement(r.b,{to:"/transfer",className:e.line},"Transfer")),l.a.createElement(s.a,null,l.a.createElement(r.b,{to:"/word_embedding",className:e.line},"Embedding")),l.a.createElement(s.a,null,l.a.createElement(r.b,{to:"/datatypests",className:e.line},"Datatypes")),l.a.createElement(s.a,null,l.a.createElement(r.b,{to:"/typeScript_2",className:e.line},"TS Function")),l.a.createElement(s.a,null,l.a.createElement(r.b,{to:"/typeScript_4",className:e.line},"Type Assertion"))),l.a.createElement("div",null))}},207:function(e,a,t){e.exports=t.p+"static/media/hp.2e354048.jpg"},457:function(e,a,t){"use strict";t.r(a);var n=t(45),l=t(28),r=t(136),s=t(137),c=t(139),i=t(0),o=t.n(i),m=t(138),u=t.n(m),d=t(120),p=t(57),g=t(296),b=t(5),_=(t(141),t(150)),h=t(140),E=t(207),f=t.n(E),w={backgroundColor:"#F0F8FF",padding:"1px",fontSize:"16px"},y={height:200,width:500},v="\nimport pandas as pd\nimport numpy as np\nfrom sklearn import preprocessing\nfrom matplotlib import pyplot as plt\n%matplotlib inline\n\ndf = pd.read_csv(\"homeprices_banglore.csv\")\n\nsx = preprocessing.MinMaxScaler()\nsy = preprocessing.MinMaxScaler()\n\nscaled_X = sx.fit_transform(df.drop('price',axis='columns'))\nscaled_y = sy.fit_transform(df['price'].values.reshape(df.shape[0],1))\n\nscaled_y.reshape(20,)\n".trim(),N='\ndef batch_gradient_descent(X, y_true, epochs, learning_rate = 0.01):\n    number_of_features = X.shape[1]\n    w = np.ones(shape=(number_of_features)) \n    b = 0\n    total_samples = X.shape[0]                                                  # number of rows in X\n    cost_list = []\n    epoch_list = []\n    \n    for i in range(epochs):        \n        y_predicted = np.dot(w, X.T) + b\n\n        w_grad = -(2/total_samples)*(X.T.dot(y_true-y_predicted))\n        b_grad = -(2/total_samples)*np.sum(y_true-y_predicted)\n        \n        w = w - learning_rate * w_grad\n        b = b - learning_rate * b_grad\n        \n        cost = np.mean(np.square(y_true-y_predicted))                          # MSE (Mean Squared Error)\n        \n        if i%10==0:\n            cost_list.append(cost)\n            epoch_list.append(i)\n    return w, b, cost, cost_list, epoch_list\n\nw, b, cost, cost_list, epoch_list = batch_gradient_descent(scaled_X,scaled_y.reshape(scaled_y.shape[0],),500)\nw, b, cost\n\nplt.xlabel("epoch")\nplt.ylabel("cost")\nplt.plot(epoch_list,cost_list)\n\n'.trim(),x="\ndef predict(area,bedrooms,w,b):\n    scaled_X = sx.transform([[area, bedrooms]])[0]\n    scaled_price = w[0] * scaled_X[0] + w[1] * scaled_X[1] + b\n    return sy.inverse_transform([[scaled_price]])[0][0]\n\npredict(2600,4,w,b)\npredict(1000,2,w,b)\npredict(1500,3,w,b)\n".trim(),j='\nimport random\nrandom.randint(0,6)                              # randit gives random number between two numbers specified in the argument.\n\ndef stochastic_gradient_descent(X, y_true, epochs, learning_rate = 0.01):\n    number_of_features = X.shape[1]\n    w = np.ones(shape=(number_of_features)) \n    b = 0\n    total_samples = X.shape[0]\n    \n    cost_list = []\n    epoch_list = []\n    \n    for i in range(epochs):    \n        random_index = random.randint(0,total_samples-1) # random index from total samples\n        sample_x = X[random_index]\n        sample_y = y_true[random_index]\n        y_predicted = np.dot(w, sample_x.T) + b\n    \n        w_grad = -(2/total_samples)*(sample_x.T.dot(sample_y-y_predicted))\n        b_grad = -(2/total_samples)*(sample_y-y_predicted)\n        \n        w = w - learning_rate * w_grad\n        b = b - learning_rate * b_grad\n        cost = np.square(sample_y-y_predicted)\n        \n        if i%100==0: # at every 100th iteration record the cost and epoch value\n            cost_list.append(cost)\n            epoch_list.append(i)\n    return w, b, cost, cost_list, epoch_list\n\nw_sgd, b_sgd, cost_sgd, cost_list_sgd, epoch_list_sgd = SGD(scaled_X,scaled_y.reshape(scaled_y.shape[0],),10000)\nw_sgd, b_sgd, cost_sgd\n\nw , b \nplt.xlabel("epoch")\nplt.ylabel("cost")\nplt.plot(epoch_list_sgd,cost_list_sgd)\n\npredict(2600,4,w_sgd, b_sgd) \npredict(1000,2,w_sgd, b_sgd)\npredict(1500,3,w_sgd, b_sgd)\n'.trim(),X=function(e){function a(){return Object(n.a)(this,a),Object(r.a)(this,Object(s.a)(a).apply(this,arguments))}return Object(c.a)(a,e),Object(l.a)(a,[{key:"componentDidMount",value:function(){setTimeout((function(){return u.a.highlightAll()}),0)}},{key:"render",value:function(){var e=this.props.classes;return o.a.createElement(d.a,{container:!0},o.a.createElement(d.a,{item:!0,xs:2},o.a.createElement(p.a,{className:e.paper},o.a.createElement("h4",null,o.a.createElement(_.a,null)))),o.a.createElement(d.a,{item:!0,xs:10},o.a.createElement(p.a,{className:e.paper},o.a.createElement(g.a,null,o.a.createElement("h3",null,"Implementation of stochastic and batch grandient descent in python."),o.a.createElement("i",null,"We will use home prices data set to implement batch and stochastic gradient descent in python. Batch gradient descent uses ",o.a.createElement("b",null,"all")," training samples in forward pass to calculate cumulitive error and than we adjust weights using derivaties. In stochastic GD, we ",o.a.createElement("b",null,"randomly pick one")," training sample, perform forward pass, compute the error and immidiately adjust weights."),o.a.createElement("br",null),o.a.createElement("br",null),o.a.createElement("ul",null,o.a.createElement("li",null,o.a.createElement("b",null,"Preprocessing/ Scaling: "),"Since our columns are on different sacle it is important to perform scaling on them.")),o.a.createElement("br",null),o.a.createElement("div",{style:w},o.a.createElement(h.a,{code:v,language:"js",plugins:["line-numbers"]})),o.a.createElement("br",null),o.a.createElement("i",null,"We should convert target column (price) into one dimensional array. It has become 2D due to scaling that we did above but now we should change to 1D"),o.a.createElement("br",null),o.a.createElement("img",{src:f.a,alt:"Theata",className:"responsive2",style:y}),o.a.createElement("h3",null,"Now implement mini batch gradient descent. "),o.a.createElement("ul",null,o.a.createElement("li",null,"numpy array with 1 row and columns equal to number of features. In our case number_of_features = 2 (area, bedroom).")),o.a.createElement("div",{style:w},o.a.createElement(h.a,{code:N,language:"js",plugins:["line-numbers"]})),o.a.createElement("br",null),o.a.createElement("h3",null,"Lets do some predictions now. "),o.a.createElement("ul",null,o.a.createElement("li",null,"Here w1 = w[0] , w2 = w[1], w3 = w[2] and bias is b."),o.a.createElement("li",null,"Equation for price is w1*area + w2*bedrooms + w3*age + bias."),o.a.createElement("li",null,"scaled_X[0] is area."),o.a.createElement("li",null,"scaled_X[1] is bedrooms."),o.a.createElement("li",null,"scaled_X[2] is age."),o.a.createElement("li",null,"Once we get price prediction we need to to rescal it back to original value also since it returns 2D array, to get single value we need to do value[0][0].")),o.a.createElement("div",{style:w},o.a.createElement(h.a,{code:x,language:"js",plugins:["line-numbers"]})),o.a.createElement("br",null),o.a.createElement("h3",null,"Stochastic Gradient Descent Implementation"),o.a.createElement("i",null,"Stochastic GD will use randomly picked single training sample to calculate error and using this error we backpropage to adjust weights."),o.a.createElement("br",null),o.a.createElement("div",{style:w},o.a.createElement(h.a,{code:j,language:"js",plugins:["line-numbers"]})),o.a.createElement("br",null),o.a.createElement("i",null,"Compare this with weights and bias that we got using gradient descent. They both of quite similar.")))))}}]),a}(i.Component);a.default=Object(b.a)((function(e){return{paper:{margin:e.spacing(1),padding:e.spacing(1)},smMargin:{margin:e.spacing(1)},actionDiv:{textAlign:"center"}}}))(X)}}]);
//# sourceMappingURL=48.88abeced.chunk.js.map