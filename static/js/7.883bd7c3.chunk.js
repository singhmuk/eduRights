(this["webpackJsonpmern-stack-client"]=this["webpackJsonpmern-stack-client"]||[]).push([[7],{140:function(e,a,t){"use strict";t.d(a,"a",(function(){return h}));var n=t(45),l=t(28),r=t(136),i=t(137),s=t(139),c=t(0),o=t.n(c),m=t(138),u=t.n(m),h=(t(59),function(e){function a(e){var t;return Object(n.a)(this,a),(t=Object(r.a)(this,Object(i.a)(a).call(this,e))).highlight=function(){t.ref&&t.ref.current&&u.a.highlightElement(t.ref.current)},t.ref=o.a.createRef(),t}return Object(s.a)(a,e),Object(l.a)(a,[{key:"componentDidMount",value:function(){this.highlight()}},{key:"componentDidUpdate",value:function(){this.highlight()}},{key:"render",value:function(){var e=this.props,a=e.code,t=(e.plugins,e.language);return o.a.createElement("pre",{className:"code-prism"},o.a.createElement("code",{ref:this.ref,className:"language-".concat(t)},a.trim()))}}]),a}(o.a.Component))},141:function(e,a,t){},146:function(e,a,t){"use strict";t.d(a,"a",(function(){return m}));var n=t(0),l=t.n(n),r=t(26),i=t(297),s=t(295),c=t(114),o=Object(c.a)((function(e){return{root:{display:"flex"},paper:{marginRight:e.spacing(2)},line:{textDecoration:"none"}}}));function m(){var e=o();return l.a.createElement("div",{className:e.root},l.a.createElement(s.a,null,l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/infoMl",className:e.line},"InfoMl")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/gredient_decents",className:e.line},"Gredient Decents")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/training",className:e.line},"Traning")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/regularizations",className:e.line},"Regularizations")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/featuresEng",className:e.line},"FeaturesEng")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/adaboost",className:e.line},"Adaboots")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/greedSearch",className:e.line},"Greed Search")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/perceptron",className:e.line},"Perceptron")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/pcaPy",className:e.line},"PCA")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/leanearRegression",className:e.line},"Leanear Regression")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/logisticReg",className:e.line},"Logistic Regression")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/lda",className:e.line},"Lda")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/knn",className:e.line},"Knn")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/k_meanClustring",className:e.line},"K_Mean")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/naiveBar",className:e.line},"Naive Bayes")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/randomForest",className:e.line},"Random Forest")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/decisiontree",className:e.line},"Decision Tree")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/svmPy",className:e.line},"SVM")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/numpyPy",className:e.line},"Numpy")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/pandas",className:e.line},"Pandas")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/bagging",className:e.line},"Matplotlib")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/logisticRegrations",className:e.line},"Scikit Learn")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/regrations",className:e.line},"SciPy")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/libraries",className:e.line},"OpenCV")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/capture",className:e.line},"Capture")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/joinImages",className:e.line},"JoinImages")),l.a.createElement("br",null),"Deep Learning",l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/superwise",className:e.line},"Superwise"))),l.a.createElement("div",null))}},250:function(e,a,t){e.exports=t.p+"static/media/svms.31f561b2.PNG"},251:function(e,a,t){e.exports=t.p+"static/media/svm2.0b14febb.PNG"},252:function(e,a,t){e.exports=t.p+"static/media/svms3.567681a3.PNG"},253:function(e,a,t){e.exports=t.p+"static/media/svm4.8f6be448.PNG"},254:function(e,a,t){e.exports=t.p+"static/media/svm5.4412fc04.PNG"},255:function(e,a,t){e.exports=t.p+"static/media/svm6.f5fc341b.PNG"},256:function(e,a,t){e.exports=t.p+"static/media/svm7.7b52a1bf.PNG"},523:function(e,a,t){"use strict";t.r(a);var n=t(45),l=t(28),r=t(136),i=t(137),s=t(139),c=t(0),o=t.n(c),m=t(138),u=t.n(m),h=t(120),p=t(57),d=t(296),b=t(5),g=(t(141),t(146)),E=t(140),f=t(250),y=t.n(f),w=t(251),v=t.n(w),S=t(252),_=t.n(S),N=t(253),x=t.n(N),M=t(254),k=t.n(M),V=t(255),C=t.n(V),P=t(256),j=t.n(P),I={backgroundColor:"#F0F8FF",padding:"1px",fontSize:"16px"},X={height:350,width:600},A="\nimport pandas as pd\nfrom sklearn.datasets import load_iris\n\niris = load_iris()\ndf = pd.DataFrame(iris.data,columns=iris.feature_names)\n\ndf['target'] = iris.target\n\ndf[df.target==1].head()\ndf[df.target==2].head()\n\ndf['flower_name'] =df.target.apply(lambda x: iris.target_names[x])\ndf[45:55]\n\ndf2 = df[:50]\ndf2\n".trim(),F="\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import train_test_split \n\nplt.xlabel('Sepal Length')\nplt.ylabel('Sepal Width')\nplt.scatter(df0['sepal length (cm)'], df0['sepal width (cm)'],color=\"green\",marker='+')\nplt.scatter(df1['sepal length (cm)'], df1['sepal width (cm)'],color=\"blue\",marker='.')\n\nplt.xlabel('Petal Length')                                            #Petal length vs Pepal Width (Setosa vs Versicolor)\nplt.ylabel('Petal Width')\nplt.scatter(df0['petal length (cm)'], df0['petal width (cm)'],color=\"green\",marker='+')\nplt.scatter(df1['petal length (cm)'], df1['petal width (cm)'],color=\"blue\",marker='.')\n\nX = df.drop(['target','flower_name'], axis='columns')\ny = df.target\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\nlen(X_train)\n\nmodel = SVC()\nmodel.fit(X_train, y_train)\n\nmodel.score(X_test, y_test)\nmodel.predict([[4.8,3.0,1.5,0.3]])\n".trim(),T="\nmodel_C = SVC(C=1)\nmodel_C.fit(X_train, y_train)\nmodel_C.score(X_test, y_test)\n\nmodel_C = SVC(C=10)\nmodel_C.fit(X_train, y_train)\nmodel_C.score(X_test, y_test)\n".trim(),z="\nmodel_g = SVC(gamma=10)\nmodel_g.fit(X_train, y_train)\nmodel_g.score(X_test, y_test)\n".trim(),R="\nmodel_linear_kernal = SVC(kernel='linear')\nmodel_linear_kernal.fit(X_train, y_train)\n\nmodel_linear_kernal.score(X_test, y_test)\n".trim(),B=function(e){function a(){return Object(n.a)(this,a),Object(r.a)(this,Object(i.a)(a).apply(this,arguments))}return Object(s.a)(a,e),Object(l.a)(a,[{key:"componentDidMount",value:function(){setTimeout((function(){return u.a.highlightAll()}),0)}},{key:"render",value:function(){var e=this.props.classes;return o.a.createElement(h.a,{container:!0},o.a.createElement(h.a,{item:!0,xs:2},o.a.createElement(p.a,{className:e.paper},o.a.createElement("h4",null,o.a.createElement(g.a,null)))),o.a.createElement(h.a,{item:!0,xs:10},o.a.createElement(p.a,{className:e.paper},o.a.createElement(d.a,null,o.a.createElement("h3",null,"Support Vector Machine(SVM) (supervised machine learning algorithms)"),"SVMs are powerful yet flexible ML algorithms which are used both for classification and regression. But generally, they are used in classification problems.",o.a.createElement("br",null),"Lately, they are extremely popular because of their ability to handle multiple continuous and categorical variables.",o.a.createElement("br",null),o.a.createElement("br",null),o.a.createElement("b",null,"Working of SVM: "),"An SVM model is basically a representation of different classes in a hyperplane in multidimensional space. The hyperplane will be generated in an iterative manner by SVM so that the error can be minimized.",o.a.createElement("br",null),"The goal of SVM is to divide the datasets into classes to find a maximum marginal hyperplane (MMH).",o.a.createElement("br",null),o.a.createElement("br",null),o.a.createElement("i",null,o.a.createElement("b",null,"The followings are important concepts in SVM \u2212 ")),o.a.createElement("ul",null,o.a.createElement("li",null,o.a.createElement("b",null,"Support Vectors: "),"Datapoints that are closest to the hyperplane is called support vectors. Separating line will be defined with the help of these data points."),o.a.createElement("li",null,o.a.createElement("b",null,"Hyperplane: "),"It is a decision plane or space which is divided between a set of objects having different classes."),o.a.createElement("li",null,o.a.createElement("b",null,"Margin: "),"It may be defined as the gap between two lines on the closet data points of different classes. It can be calculated as the perpendicular distance from the line to the support vectors. Large margin is considered as a good margin and small margin is considered as a bad margin. The main goal of SVM is to divide the datasets into classes to find a maximum marginal hyperplane (MMH) and it can be done in the following two steps:"),o.a.createElement("ul",null,o.a.createElement("li",null,"1. First, SVM will generate hyperplanes iteratively that segregates the classes in best way. Then, it will choose the hyperplane that separates the classes correctly."),o.a.createElement("li",null,"SVM Kernels SVM algorithm is implemented with kernel that transforms an input data space into the required form. Kernel converts non-separable problems into separable problems by adding more dimensions to it. It makes SVM more powerful, flexible and accurate. The following are some of the types of kernels used by SVM. Linear Kernel It can be used as a dot product between any two observations. The formula of linear kernel is \u2212",o.a.createElement("br",null),o.a.createElement("i",null,o.a.createElement("b",null,"K(x,xi)=sum(x\u2217xi)")),o.a.createElement("br",null),"From the above formula, we can see that the product between two vectors \ud835\udc65 & \ud835\udc65\ud835\udc56 is the sum of the multiplication of each pair of input values.",o.a.createElement("br",null),o.a.createElement("br",null),"Polynomial Kernel It is more generalized form of linear kernel and distinguish curved or nonlinear input space. Following is the formula for polynomial kernel \u2212",o.a.createElement("br",null),o.a.createElement("i",null,o.a.createElement("b",null,"k(X,Xi)=1+sum(X\u2217Xi)^d")),o.a.createElement("br",null),o.a.createElement("br",null),"Here d is the degree of polynomial, which we need to specify manually in the learning algorithm.",o.a.createElement("br",null),o.a.createElement("br",null),o.a.createElement("b",null,"Radial Basis Function (RBF) Kernel: ")," RBF kernel, mostly used in SVM classification, maps input space in indefinite dimensional space. Following formula explains it mathematically \u2212",o.a.createElement("br",null),o.a.createElement("i",null,o.a.createElement("b",null,"K(x,xi)=exp(\u2212gamma\u2217sum(x\u2212xi^2)) "))),o.a.createElement("br",null),"Here, gamma ranges from 0 to 1. We need to manually specify it in the learning algorithm. A good default value of gamma is 0.1. As we implemented SVM for linearly separable data, we can implement it in Python for the data that is not linearly separable. It can be done by using kernels.",o.a.createElement("br",null),o.a.createElement("br",null),o.a.createElement("b",null,"Pros: "),"SVM classifiers offers great accuracy and work well with high dimensional space. SVM classifiers basically use a subset of training points hence in result uses very less memory.",o.a.createElement("br",null),o.a.createElement("br",null),o.a.createElement("b",null,"Cons: ")," of SVM classifiers They have high training time hence in practice not suitable for large datasets. Another disadvantage is that SVM classifiers do not work well with overlapping classes.")),o.a.createElement("br",null),o.a.createElement("h3",null,"Support Vector Machine (Supervised)"),"Used for both classification or regression challenges.",o.a.createElement("br",null),"In the SVM algorithm, we plot each data item as a point in n-dimensional space (where n is a number of features we have) with the value of each feature being the value of a particular coordinate. Then, we perform classification by finding the hyper-plane that differentiates the two classes very well.",o.a.createElement("br",null),o.a.createElement("img",{src:y.a,alt:"Equations",className:"responsive",style:X}),o.a.createElement("br",null),"We got accustomed to the process of segregating the two classes with a hyper-plane. Now, \u201cHow can we identify the right hyper-plane?\u201d.",o.a.createElement("br",null),o.a.createElement("br",null),o.a.createElement("ul",null,o.a.createElement("li",null,o.a.createElement("b",null,"Identify the right hyper-plane (Scenario-1): "),"We have three hyper-planes (A, B, and C). Now, identify the right hyper-plane to classify stars and circles."),o.a.createElement("br",null),o.a.createElement("img",{src:v.a,alt:"Equations",className:"responsive",style:X}),o.a.createElement("br",null),o.a.createElement("i",null,"Remember a thumb rule to identify the right hyper-plane: \u201cSelect the hyper-plane which segregates the two classes better\u201d. Here, hyper-plane B."),o.a.createElement("br",null),o.a.createElement("br",null),o.a.createElement("li",null,o.a.createElement("b",null,"Identify the right hyper-plane (Scenario-2): ")),o.a.createElement("br",null),o.a.createElement("img",{src:_.a,alt:"Equations",className:"responsive",style:X}),o.a.createElement("br",null),o.a.createElement("i",null,"Here, maximizing the distances between nearest data point (either class) and hyper-plane will help us to decide the right hyper-plane. This distance is called as Margin."),o.a.createElement("br",null),o.a.createElement("br",null),o.a.createElement("i",null,"Above, you can see that the margin for hyper-plane C is high as compared to both A and B. Hence, we name the right hyper-plane as C. Another lightning reason for selecting the hyper-plane with higher margin is robustness. If we select a hyper-plane having low margin then there is high chance of miss-classification."),o.a.createElement("br",null),o.a.createElement("br",null),o.a.createElement("li",null,o.a.createElement("b",null,"Identify the right hyper-plane (Scenario-3): ")),o.a.createElement("br",null),o.a.createElement("img",{src:x.a,alt:"Equations",className:"responsive",style:X}),o.a.createElement("br",null),o.a.createElement("i",null,"Some of you may have selected the hyper-plane B as it has higher margin compared to A. But, here is the catch, SVM selects the hyper-plane which classifies the classes accurately prior to maximizing margin. Here, hyper-plane B has a classification error and A has classified all correctly. Therefore, the right hyper-plane is A."),o.a.createElement("br",null),o.a.createElement("br",null),o.a.createElement("li",null,o.a.createElement("b",null,"Can we classify two classes (Scenario-4): "),"We unable to segregate the two classes using a straight line, as one of the stars lies in the territory of other(circle) class as an outlier. "),o.a.createElement("br",null),o.a.createElement("img",{src:k.a,alt:"Equations",className:"responsive",style:X}),o.a.createElement("br",null),o.a.createElement("br",null),o.a.createElement("li",null,o.a.createElement("b",null,"Find the hyper-plane to segregate to classes (Scenario-5): ")),o.a.createElement("br",null),o.a.createElement("img",{src:C.a,alt:"Equations",className:"responsive",style:X}),"SVM can solve this problem. Easily! It solves this problem by introducing additional feature. Here, we will add a new feature z=x^2+y^2. Now, let\u2019s plot the data points on axis x and z.",o.a.createElement("br",null),o.a.createElement("img",{src:j.a,alt:"Equations",className:"responsive",style:X}),o.a.createElement("ul",null,o.a.createElement("li",null,"All values for z would be positive always because z is the squared sum of both x and y."),o.a.createElement("li",null,"In the original plot, red circles appear close to the origin of x and y axes, leading to lower value of z and star relatively away from the origin result to higher value of z."),o.a.createElement("br",null),"In the SVM classifier, it is easy to have a linear hyper-plane between these two classes. But, another burning question which arises is, should we need to add this feature manually to have a hyper-plane. No, the SVM  algorithm has a technique called the kernel trick. The SVM kernel is a function that takes low dimensional input space and transforms it to a higher dimensional space i.e. it converts not separable problem to separable problem. It is mostly useful in non-linear separation problem. Simply put, it does some extremely complex data transformations, then finds out the process to separate the data based on the labels or outputs you\u2019ve defined.")),o.a.createElement("br",null),o.a.createElement("h3",null,"Example"),o.a.createElement("div",{style:I},o.a.createElement(E.a,{code:A,language:"js",plugins:["line-numbers"]})),o.a.createElement("br",null),o.a.createElement("h3",null,"Sepal length vs Sepal Width (Setosa vs Versicolor)"),o.a.createElement("div",{style:I},o.a.createElement(E.a,{code:F,language:"js",plugins:["line-numbers"]})),o.a.createElement("br",null),o.a.createElement("h3",null,"Tune parameters"),o.a.createElement("b",null,"1. Regularization (C)"),o.a.createElement("br",null),o.a.createElement("div",{style:I},o.a.createElement(E.a,{code:T,language:"js",plugins:["line-numbers"]})),o.a.createElement("br",null),o.a.createElement("b",null,"2. Gamma"),o.a.createElement("br",null),o.a.createElement("div",{style:I},o.a.createElement(E.a,{code:z,language:"js",plugins:["line-numbers"]})),o.a.createElement("br",null),o.a.createElement("b",null,"3. Kernel"),o.a.createElement("br",null),o.a.createElement("div",{style:I},o.a.createElement(E.a,{code:R,language:"js",plugins:["line-numbers"]}))))))}}]),a}(c.Component);a.default=Object(b.a)((function(e){return{paper:{margin:e.spacing(1),padding:e.spacing(1)},smMargin:{margin:e.spacing(1)},actionDiv:{textAlign:"center"}}}))(B)}}]);
//# sourceMappingURL=7.883bd7c3.chunk.js.map