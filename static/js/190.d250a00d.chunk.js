(this["webpackJsonpmern-stack-client"]=this["webpackJsonpmern-stack-client"]||[]).push([[190],{140:function(e,a,n){"use strict";n.d(a,"a",(function(){return d}));var t=n(45),l=n(28),r=n(136),i=n(137),s=n(139),o=n(0),c=n.n(o),m=n(138),u=n.n(m),d=(n(59),function(e){function a(e){var n;return Object(t.a)(this,a),(n=Object(r.a)(this,Object(i.a)(a).call(this,e))).highlight=function(){n.ref&&n.ref.current&&u.a.highlightElement(n.ref.current)},n.ref=c.a.createRef(),n}return Object(s.a)(a,e),Object(l.a)(a,[{key:"componentDidMount",value:function(){this.highlight()}},{key:"componentDidUpdate",value:function(){this.highlight()}},{key:"render",value:function(){var e=this.props,a=e.code,n=(e.plugins,e.language);return c.a.createElement("pre",{className:"code-prism"},c.a.createElement("code",{ref:this.ref,className:"language-".concat(n)},a.trim()))}}]),a}(c.a.Component))},141:function(e,a,n){},146:function(e,a,n){"use strict";n.d(a,"a",(function(){return m}));var t=n(0),l=n.n(t),r=n(26),i=n(297),s=n(295),o=n(114),c=Object(o.a)((function(e){return{root:{display:"flex"},paper:{marginRight:e.spacing(2)},line:{textDecoration:"none"}}}));function m(){var e=c();return l.a.createElement("div",{className:e.root},l.a.createElement(s.a,null,l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/infoMl",className:e.line},"InfoMl")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/gredient_decents",className:e.line},"Gredient Decents")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/training",className:e.line},"Traning")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/regularizations",className:e.line},"Regularizations")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/featuresEng",className:e.line},"FeaturesEng")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/adaboost",className:e.line},"Adaboots")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/greedSearch",className:e.line},"Greed Search")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/perceptron",className:e.line},"Perceptron")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/pcaPy",className:e.line},"PCA")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/leanearRegression",className:e.line},"Leanear Regression")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/logisticReg",className:e.line},"Logistic Regression")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/lda",className:e.line},"Lda")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/knn",className:e.line},"Knn")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/k_meanClustring",className:e.line},"K_Mean")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/naiveBar",className:e.line},"Naive Bayes")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/randomForest",className:e.line},"Random Forest")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/decisiontree",className:e.line},"Decision Tree")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/svmPy",className:e.line},"SVM")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/numpyPy",className:e.line},"Numpy")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/pandas",className:e.line},"Pandas")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/bagging",className:e.line},"Matplotlib")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/logisticRegrations",className:e.line},"Scikit Learn")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/regrations",className:e.line},"SciPy")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/libraries",className:e.line},"OpenCV")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/capture",className:e.line},"Capture")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/joinImages",className:e.line},"JoinImages")),l.a.createElement("br",null),"Deep Learning",l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/superwise",className:e.line},"Superwise"))),l.a.createElement("div",null))}},517:function(e,a,n){"use strict";n.r(a);var t=n(45),l=n(28),r=n(136),i=n(137),s=n(139),o=n(0),c=n.n(o),m=n(138),u=n.n(m),d=n(120),p=n(57),g=n(296),E=n(5),f=(n(141),n(146)),b=n(140),h={backgroundColor:"#F0F8FF",padding:"1px",fontSize:"16px"},_='\nfrom sklearn.datasets import load_iris\n\niris = load_iris()\nX = iris.data\ny = iris.target\n\nfeature_names = iris.feature_names\ntarget_names = iris.target_names\n\nprint("Feature names:", feature_names)\nprint("Target names:", target_names)\nprint("First 10 rows of X", X[:10])\n'.trim(),k="\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 1)\n\nX_train.shape\nX_test.shape\n\ny_train.shape\ny_test.shape\n".trim(),v='\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import metrics\n\nclassifier_knn = KNeighborsClassifier(n_neighbors = 3)\nclassifier_knn.fit(X_train, y_train)\ny_pred = classifier_knn.predict(X_test)\n\n# Finding accuracy by comparing actual response values(y_test)with predicted response value(y_pred)\nprint("Accuracy:", metrics.accuracy_score(y_test, y_pred))\n\n# Providing sample data and the model will make prediction out of that data\nsample = [[5, 5, 3, 2], [2, 4, 3, 5]]\npreds = classifier_knn.predict(sample)\n'.trim(),y="\nfrom sklearn import linear_model\nreg = linear_model.LinearRegression()  \n\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, Y_train)                       \n\nfrom sklearn.preprocessing import StandardScaler  \nsc_X = StandardScaler()\n\nfrom sklearn.linear_model import LogisticRegression\nmodel = LogisticRegression()\n\nfrom sklearn.svm import SVC\nsvm = SVC(gamma='auto') \n\nfrom sklearn.svm import SVC\nmodel = SVC()\nmodel.fit(X_train, y_train)\n\nfrom sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(n_estimators=40)                                                      \n\nfrom sklearn.model_selection import KFold\nkf = KFold(n_splits=3)\n\nfrom sklearn.neighbors import KNeighborsClassifier \nknn = KNeighborsClassifier(n_neighbors=10)\n\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nscaler.fit(df[['Income($)']])\n\nfrom sklearn.pipeline import Pipeline\nclf = Pipeline([('vectorizer', CountVectorizer()),('nb', MultinomialNB())])\nclf.fit(X_train, y_train)\n\nfrom sklearn.preprocessing import LabelEncoder\nle_company = LabelEncoder()\n\nfrom sklearn import tree\nmodel = tree.DecisionTreeClassifier()\nmodel.fit(inputs_n, target)\n\nfrom sklearn.decomposition import PCA\npca = PCA(0.95)\nX_pca = pca.fit_transform(X)\n\nfrom sklearn import preprocessing\nencoder = preprocessing.LabelEncoder()\nencoder.fit(input_labels)\n\nfrom sklearn.linear_model import Ridge                                                  \nridge_reg= Ridge(alpha=50, max_iter=100, tol=0.1) \nridge_reg.fit(train_X, train_y)\n\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.tree import DecisionTreeClassifier\nscores = cross_val_score(DecisionTreeClassifier(), X, y, cv=5)\n\nfrom sklearn.model_selection import GridSearchCV\nclf = GridSearchCV(svm.SVC(gamma='auto'), {'C': [1,10,20], 'kernel': ['rbf','linear']}, cv=5, return_train_score=False)\nclf.fit(iris.data, iris.target)\n\nfrom sklearn.model_selection import RandomizedSearchCV\nrs = RandomizedSearchCV(svm.SVC(gamma='auto'), {'C': [1,10,20],'kernel': ['rbf','linear']}, \n    cv=5, return_train_score=False, n_iter=2)\nrs.fit(iris.data, iris.target)\n\nfrom sklearn.ensemble import BaggingClassifier\n\nbag_model = BaggingClassifier(\n    base_estimator=DecisionTreeClassifier(), \n    n_estimators=100, \n    max_samples=0.8, \n    oob_score=True,\n    random_state=0\n)\nbag_model.fit(X_train, y_train)\n".trim(),N=function(e){function a(){return Object(t.a)(this,a),Object(r.a)(this,Object(i.a)(a).apply(this,arguments))}return Object(s.a)(a,e),Object(l.a)(a,[{key:"componentDidMount",value:function(){setTimeout((function(){return u.a.highlightAll()}),0)}},{key:"render",value:function(){var e=this.props.classes;return c.a.createElement(d.a,{container:!0},c.a.createElement(d.a,{item:!0,xs:2},c.a.createElement(p.a,{className:e.paper},c.a.createElement("h4",null,c.a.createElement(f.a,null)))),c.a.createElement(d.a,{item:!0,xs:10},c.a.createElement(p.a,{className:e.paper},c.a.createElement(g.a,null,c.a.createElement("h3",null,"Scikit Learn (common algoritham) - Modelling Process"),"Rather than focusing on loading, manipulating and summarising data, Scikit-learn library is focused on modeling the data. Some of the most popular groups of models provided by Sklearn are as follows \u2212",c.a.createElement("br",null),c.a.createElement("br",null),"We can do following with scikit Learn.",c.a.createElement("ul",null,c.a.createElement("li",null,c.a.createElement("b",null,"Classification: "),"SVM, nearest neighbors, random forest, logistic regression, etc."),c.a.createElement("li",null,c.a.createElement("b",null,"Regression: "),"Lasso, ridge regression, etc."),c.a.createElement("li",null,c.a.createElement("b",null,"Clustering: "),"k-means, spectral clustering, etc."),c.a.createElement("li",null,c.a.createElement("b",null,"Dimensionality reduction: "),"PCA, feature selection, matrix factorization, etc."),c.a.createElement("li",null,c.a.createElement("b",null,"Model selection: "),"Grid search, cross-validation, metrics."),c.a.createElement("li",null,c.a.createElement("b",null,"Preprocessing: "),".Feature extraction, normalization")),c.a.createElement("br",null),c.a.createElement("b",null,"Dataset Loading:"),"A collection of data is called dataset. It is having the following two components.",c.a.createElement("br",null),"Dataset having the following two components.",c.a.createElement("ul",null,c.a.createElement("li",null,c.a.createElement("b",null,"Features: "),"The variables of data are called its features."),c.a.createElement("ul",null,c.a.createElement("li",null,c.a.createElement("b",null,"Feature matrix: "),"It is the collection of features, in case there are more than one."),c.a.createElement("li",null,c.a.createElement("b",null,"Feature Names: "),"It is the list of all the names of the features.")),c.a.createElement("br",null),c.a.createElement("li",null,c.a.createElement("b",null,"Response: "),"It is the output variable that basically depends upon the feature variables."),c.a.createElement("ul",null,c.a.createElement("li",null,c.a.createElement("b",null,"Response Vector: "),"It is used to represent response column. Generally, we have just one response column."),c.a.createElement("li",null,c.a.createElement("b",null,"Target Names: "),"It represent the possible values taken by a response vector."))),c.a.createElement("div",{style:h},c.a.createElement(b.a,{code:_,language:"js",plugins:["line-numbers"]})),c.a.createElement("br",null),c.a.createElement("br",null),c.a.createElement("b",null,"Some popular groups of models provided by scikit-learn include:"),c.a.createElement("ul",null,c.a.createElement("li",null,c.a.createElement("b",null,"Clustering: "),"For grouping unlabeled data such as KMeans."),c.a.createElement("li",null,c.a.createElement("b",null,"Cross Validation: "),"For estimating the performance of supervised models on unseen data."),c.a.createElement("li",null,c.a.createElement("b",null,"Datasets: "),"For test datasets and for generating datasets with specific properties for investigating model behavior."),c.a.createElement("li",null,c.a.createElement("b",null,"Dimensionality Reduction: "),"For reducing the number of attributes in data for summarization, visualization and feature selection such as Principal component analysis."),c.a.createElement("li",null,c.a.createElement("b",null,"Ensemble methods: "),"For combining the predictions of multiple supervised models."),c.a.createElement("li",null,c.a.createElement("b",null,"Feature extraction: "),"For defining attributes in image and text data."),c.a.createElement("li",null,c.a.createElement("b",null,"Feature selection: "),"For identifying meaningful attributes from which to create supervised models."),c.a.createElement("li",null,c.a.createElement("b",null,"Parameter Tuning: "),"For getting the most out of supervised models."),c.a.createElement("li",null,c.a.createElement("b",null,"Manifold Learning: "),"For summarizing and depicting complex multi-dimensional data.")),c.a.createElement("br",null),c.a.createElement("h3",null,"Splitting the dataset"),"To check the accuracy of our model, we can split the dataset into two pieces-a training set and a testing set.",c.a.createElement("div",{style:h},c.a.createElement(b.a,{code:k,language:"js",plugins:["line-numbers"]})),c.a.createElement("br",null),c.a.createElement("h3",null,"Train the Model"),"Next, we can use our dataset to train some prediction-model. ML algorithms have a consistent interface for fitting, predicting accuracy, recall etc.",c.a.createElement("div",{style:h},c.a.createElement(b.a,{code:v,language:"js",plugins:["line-numbers"]})),c.a.createElement("br",null),c.a.createElement("h3",null,"sklearn Models"),c.a.createElement("div",{style:h},c.a.createElement(b.a,{code:y,language:"js",plugins:["line-numbers"]}))))))}}]),a}(o.Component);a.default=Object(E.a)((function(e){return{paper:{margin:e.spacing(1),padding:e.spacing(1)},smMargin:{margin:e.spacing(1)},actionDiv:{textAlign:"center"}}}))(N)}}]);
//# sourceMappingURL=190.d250a00d.chunk.js.map