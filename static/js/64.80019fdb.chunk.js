(this["webpackJsonpmern-stack-client"]=this["webpackJsonpmern-stack-client"]||[]).push([[64],{140:function(e,a,n){"use strict";n.d(a,"a",(function(){return d}));var t=n(45),l=n(28),r=n(136),i=n(137),s=n(139),c=n(0),o=n.n(c),m=n(138),u=n.n(m),d=(n(59),function(e){function a(e){var n;return Object(t.a)(this,a),(n=Object(r.a)(this,Object(i.a)(a).call(this,e))).highlight=function(){n.ref&&n.ref.current&&u.a.highlightElement(n.ref.current)},n.ref=o.a.createRef(),n}return Object(s.a)(a,e),Object(l.a)(a,[{key:"componentDidMount",value:function(){this.highlight()}},{key:"componentDidUpdate",value:function(){this.highlight()}},{key:"render",value:function(){var e=this.props,a=e.code,n=(e.plugins,e.language);return o.a.createElement("pre",{className:"code-prism"},o.a.createElement("code",{ref:this.ref,className:"language-".concat(n)},a.trim()))}}]),a}(o.a.Component))},141:function(e,a,n){},146:function(e,a,n){"use strict";n.d(a,"a",(function(){return m}));var t=n(0),l=n.n(t),r=n(26),i=n(297),s=n(295),c=n(114),o=Object(c.a)((function(e){return{root:{display:"flex"},paper:{marginRight:e.spacing(2)},line:{textDecoration:"none"}}}));function m(){var e=o();return l.a.createElement("div",{className:e.root},l.a.createElement(s.a,null,l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/infoMl",className:e.line},"InfoMl")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/gredient_decents",className:e.line},"Gredient Decents")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/training",className:e.line},"Traning")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/regularizations",className:e.line},"Regularizations")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/featuresEng",className:e.line},"FeaturesEng")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/adaboost",className:e.line},"Adaboots")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/greedSearch",className:e.line},"Greed Search")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/perceptron",className:e.line},"Perceptron")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/pcaPy",className:e.line},"PCA")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/leanearRegression",className:e.line},"Leanear Regression")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/logisticReg",className:e.line},"Logistic Regression")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/lda",className:e.line},"Lda")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/knn",className:e.line},"Knn")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/k_meanClustring",className:e.line},"K_Mean")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/naiveBar",className:e.line},"Naive Bayes")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/randomForest",className:e.line},"Random Forest")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/decisiontree",className:e.line},"Decision Tree")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/svmPy",className:e.line},"SVM")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/numpyPy",className:e.line},"Numpy")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/pandas",className:e.line},"Pandas")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/bagging",className:e.line},"Matplotlib")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/logisticRegrations",className:e.line},"Scikit Learn")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/regrations",className:e.line},"SciPy")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/libraries",className:e.line},"OpenCV")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/capture",className:e.line},"Capture")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/joinImages",className:e.line},"JoinImages")),l.a.createElement("br",null),"Deep Learning",l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/superwise",className:e.line},"Superwise"))),l.a.createElement("div",null))}},258:function(e,a,n){e.exports=n.p+"static/media/oneHotEncodung.f2dd5e01.png"},526:function(e,a,n){"use strict";n.r(a);var t=n(45),l=n(28),r=n(136),i=n(137),s=n(139),c=n(0),o=n.n(c),m=n(138),u=n.n(m),d=n(120),p=n(57),b=n(296),E=n(5),g=(n(141),n(146)),f=n(140),h=n(258),v=n.n(h),y={backgroundColor:"#F0F8FF",padding:"1px",fontSize:"16px"},_={height:350,width:600},w="\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\ndf = pd.read_csv(\"carprices.csv\")\n\nnewPlt = plt.scatter(df['Mileage'],df['Sell Price($)'])                              \nnewPlt = plt.scatter(df['Age(yrs)'],df['Sell Price($)'])\n\nX = df[['Mileage','Age(yrs)']]\nY = df['Sell Price($)']  \n\ndf.shape\ndf.head()\ndf.isna().sum() \ndf.describe()\nnewPlt\n".trim(),k="\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3) \nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=10)             #random_state argument\n \nX_train\nX_test\n".trim(),j="\nfrom sklearn.linear_model import LinearRegression\n\nmodel = LinearRegression()\nmodel.fit(X_train, Y_train)\n\nmodel.coef_\nmodel.intercept_\n\nmodel.predict(X_test)\nmodel.predict([[69000,6]])\nmodel.score(X_test, Y_test)\n".trim(),N="\nimport pickle\n\nwith open('model_pickle','wb') as file:\n    pickle.dump(model,file)\n    \nwith open('model_pickle','rb') as file:                                                  # Load save modal.\n    mp = pickle.load(file)\n".trim(),O="\nfrom sklearn.externals import joblib                                                     \n\njoblib.dump(model, 'model_joblib')\n\nmj = joblib.load('model_joblib')                                                         # Load save modal.\n".trim(),L='\nimport csv\nimport numpy as np\nimport pandas as pd\n\n# Download data from https://archive.ics.uci.edu/ml/datasets/spambase\nFILE_NAME = "spambase.data"\n\nwith open(FILE_NAME, "r") as f:                                           # 1) load with csv file\n    data = list(csv.reader(f, delimiter=","))\n    \ndata = np.array(data, dtype=np.float32)\ndata.shape\ndata.dtype\n\n\n# skiprows=1\ndata = np.loadtxt(FILE_NAME, delimiter=",", dtype=np.float32)             # 2) load with np.loadtxt()\n\n\n# skip_header=0, missing_values="---", filling_values=0.0                 # 3) load with np.genfromtxt()\ndata = np.genfromtxt(FILE_NAME, delimiter=",", dtype=np.float32)\n\n\nn_samples, n_features = data.shape                                        # split into X and y\nn_features -= 1\n\nX = data[:, 0:n_features]\ny = data[:, n_features]\n\nX[0, 0:5]\n'.trim(),X="\nimport pandas as pd\n\ndf = pd.read_csv(\"homeprices.csv\")\n\ndummies = pd.get_dummies(df.town)                                                 # Using Pandas to create dummy varriables.\nmerged = pd.concat([df,dummies],axis='columns')\n\nfinal = merged.drop(['town'], axis='columns')\nfinal\n".trim(),M="\nfinal = final.drop(['west windsor'], axis='columns')\n\nX = final.drop('price', axis='columns')\ny = final.price\n\nfrom sklearn.linear_model import LinearRegression\n\nmodel = LinearRegression()\nmodel.fit(X,y)\nmodel.predict(X)                                                                   # 2600 sqr ft home in new jersey\n".trim(),x="\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\n\ndfle = df\nle = LabelEncoder()\ndfle.town = le.fit_transform(dfle.town)\n\nX = dfle[['town','area']].values\nY = dfle.price.values\n\n#Use OHE to create dummy variables for each of the town.\nct = ColumnTransformer([('town', OneHotEncoder(), [0])], remainder = 'passthrough')\n\nX = ct.fit_transform(X)\nX = X[:,1:]\n\nmodel.fit(X,y)\n\nmodel.predict([[0,1,3400]])\nmodel.predict([[1,0,2800]])\n".trim(),P="\nimport numpy as np\nfrom sklearn import preprocessing\n\ninput_labels = ['red','black','red','green','black','yellow','white']           #input labels.\n\nencoder = preprocessing.LabelEncoder()                                          #create the label encoder and train it\nencoder.fit(input_labels)\n\n\n#Check the performance by encoding the random ordered list\n\ntest_labels = ['green','red','black']\nencoded_values = encoder.transform(test_labels)\nprint(\"Labels =\", test_labels)\nprint(\"Encoded values =\", list(encoded_values))\n\nencoded_values = [3,0,4,1]\ndecoded_list = encoder.inverse_transform(encoded_values)\nprint(\"Encoded values =\", encoded_values)\nprint(\"Decoded labels =\", list(decoded_list))\n".trim(),T=function(e){function a(){return Object(t.a)(this,a),Object(r.a)(this,Object(i.a)(a).apply(this,arguments))}return Object(s.a)(a,e),Object(l.a)(a,[{key:"componentDidMount",value:function(){setTimeout((function(){return u.a.highlightAll()}),0)}},{key:"render",value:function(){var e=this.props.classes;return o.a.createElement(d.a,{container:!0},o.a.createElement(d.a,{item:!0,xs:2},o.a.createElement(p.a,{className:e.paper},o.a.createElement("h4",null,o.a.createElement(g.a,null)))),o.a.createElement(d.a,{item:!0,xs:10},o.a.createElement(p.a,{className:e.paper},o.a.createElement(b.a,null,o.a.createElement("h3",null,"1. Training And Testing Available Data"),"We have a dataset containing prices of used BMW cars. We are going to analyze this dataset and build a prediction function that can predict a price by taking mileage and age of the car as input. We will use sklearn ",o.a.createElement("b",null,"train_test_split")," method to split training and testing dataset.",o.a.createElement("br",null),o.a.createElement("br",null),o.a.createElement("div",{style:y},o.a.createElement(f.a,{code:w,language:"js",plugins:["line-numbers"]})),o.a.createElement("br",null),o.a.createElement("br",null),o.a.createElement("b",null,"we are going to split available data in two sets."),o.a.createElement("ol",null,o.a.createElement("li",null,o.a.createElement("b",null,"Training: "),"We will train our model on this dataset."),o.a.createElement("li",null,o.a.createElement("b",null,"Testing: "),"We will use this subset to make actual predictions using trained model.")),o.a.createElement("br",null),o.a.createElement("div",{style:y},o.a.createElement(f.a,{code:k,language:"js",plugins:["line-numbers"]})),o.a.createElement("br",null),o.a.createElement("h3",null,"2. Run linear regression model"),o.a.createElement("div",{style:y},o.a.createElement(f.a,{code:j,language:"js",plugins:["line-numbers"]})),o.a.createElement("br",null),o.a.createElement("h3",null,"3. Save Model"),"There are two ways we can save a model in scikit learn.",o.a.createElement("ul",null,o.a.createElement("li",null,o.a.createElement("b",null,"1.Pickle string: "),"Algorithm for serializing and de-serializing a Python object structure. "),o.a.createElement("ul",null,o.a.createElement("li",null,o.a.createElement("b",null,"pickle.dump: "),"Use to serialize an object hierarchy."),o.a.createElement("li",null,o.a.createElement("b",null,"pickle.load : "),"Use to deserialize a data stream.")),o.a.createElement("br",null),o.a.createElement("li",null,o.a.createElement("b",null,"2.Pickled model as a file using joblib: "),"It is more efficient on objects that carry large numpy arrays. These functions also accept file-like object instead of filenames."),o.a.createElement("ul",null,o.a.createElement("li",null,o.a.createElement("b",null,"joblib.dump: "),"To serialize an object hierarchy "),o.a.createElement("li",null,o.a.createElement("b",null,"joblib.load: "),"To deserialize a data stream"))),o.a.createElement("br",null),o.a.createElement("br",null),o.a.createElement("b",null,"Save Trained Modal using Python Pickle"),o.a.createElement("br",null),o.a.createElement("br",null),o.a.createElement("div",{style:y},o.a.createElement(f.a,{code:N,language:"js",plugins:["line-numbers"]})),o.a.createElement("br",null),o.a.createElement("br",null),o.a.createElement("b",null,"Save Trained Modal using joblib"),o.a.createElement("br",null),o.a.createElement("br",null),o.a.createElement("div",{style:y},o.a.createElement(f.a,{code:O,language:"js",plugins:["line-numbers"]})),o.a.createElement("br",null),o.a.createElement("h3",null,"4. Diffrent way to load data"),o.a.createElement("div",{style:y},o.a.createElement(f.a,{code:L,language:"js",plugins:["line-numbers"]})),o.a.createElement("br",null),o.a.createElement("h3",null,"5. What is One Hot Encoding?"),o.a.createElement("ul",null,o.a.createElement("li",null,"OHE is a process of converting categorical data variables so they can be provided to ML algorithms to improve predictions."),o.a.createElement("br",null),o.a.createElement("li",null,"Categorical data refers to variables that are made up of label values, for example, a \u201ccolor\u201d variable could have the values \u201cred\u201c, \u201cblue, and \u201cgreen\u201d. Think of values like different categories that sometimes have a natural ordering to them."),o.a.createElement("br",null),o.a.createElement("li",null,"Some ML algorithms can work directly with categorical data depending on implementation, such as a decision tree, but most require any i/p or o/p variables to be a numeric in value. This means that any categorical data must be mapped to integers."),o.a.createElement("br",null),o.a.createElement("li",null,"OHE is one method of converting data to prepare it for an algorithm and get a better prediction. With one-hot, we convert each categorical value into a new categorical column and assign a binary value of 1 or 0 to those columns. Each integer value is represented as a binary vector. All the values are zero, and the index is marked with a 1.")),o.a.createElement("br",null),o.a.createElement("img",{src:v.a,alt:"Equations",className:"responsive",style:_}),o.a.createElement("br",null),o.a.createElement("b",null,"Why use OHE?"),o.a.createElement("ul",null,o.a.createElement("li",null,"OHE is useful for data that has no relationship to each other."),o.a.createElement("li",null,"ML algorithms read a higher number as better/ more important than a lower number."),o.a.createElement("li",null,"OHE makes our training data more useful and expressive, and it can be rescaled easily. By using numeric values, we more easily determine a probability for our values.")),o.a.createElement("br",null),o.a.createElement("b",null,"How to convert categorical data to numerical data"),o.a.createElement("br",null),"Manually converting our data to numerical values includes two basic steps:",o.a.createElement("ul",null,o.a.createElement("li",null,o.a.createElement("b",null,"Integer encoding:"),"We need to assign each category value with an integer, value. If we had the values red, yellow, and blue, we could assign them 1, 2, and 3 respectively."),o.a.createElement("li",null,"One hot encoding")),o.a.createElement("br",null),o.a.createElement("h3",null,"6. Categorical Variables and One Hot Encoding"),o.a.createElement("div",{style:y},o.a.createElement(f.a,{code:X,language:"js",plugins:["line-numbers"]})),o.a.createElement("br",null),o.a.createElement("h3",null,"7. Dummy Varriable Trap"),"When you can derive one variable from other variables, they are known to be multi-colinear.",o.a.createElement("br",null),"Here if you know values of california and georgia then you can easily infer value of new jersey state, i.e. california=0 and georgia=0. There for these state variables are called to be multi-colinear. In this situation linear regression won't work as expected. Hence you need to drop one column.",o.a.createElement("br",null),o.a.createElement("br",null),o.a.createElement("b",null,"N: "),"sklearn library takes care of dummy variable trap hence even if you don't drop one of the state columns it is going to work.",o.a.createElement("br",null),o.a.createElement("br",null),o.a.createElement("div",{style:y},o.a.createElement(f.a,{code:M,language:"js",plugins:["line-numbers"]})),o.a.createElement("br",null),o.a.createElement("h3",null,"8. Using sklearn OneHotEncoder"),"First step is to use label encoder to convert town names into numbers.",o.a.createElement("div",{style:y},o.a.createElement(f.a,{code:x,language:"js",plugins:["line-numbers"]})),o.a.createElement("br",null),o.a.createElement("h3",null,"9. What is Label Encoding?"),"Most of the sklearn functions expect that the data with number labels rather than word labels. Hence, we need to convert such labels into number labels. This process is called label encoding. We can perform label encoding of data with the help of ",o.a.createElement("b",null,"LabelEncoder()")," function of scikit-learn Python library.",o.a.createElement("br",null),o.a.createElement("br",null),o.a.createElement("div",{style:y},o.a.createElement(f.a,{code:P,language:"js",plugins:["line-numbers"]}))))))}}]),a}(c.Component);a.default=Object(E.a)((function(e){return{paper:{margin:e.spacing(1),padding:e.spacing(1)},smMargin:{margin:e.spacing(1)},actionDiv:{textAlign:"center"}}}))(T)}}]);
//# sourceMappingURL=64.80019fdb.chunk.js.map