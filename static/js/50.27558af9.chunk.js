(this["webpackJsonpmern-stack-client"]=this["webpackJsonpmern-stack-client"]||[]).push([[50],{140:function(e,a,n){"use strict";n.d(a,"a",(function(){return d}));var t=n(45),l=n(28),r=n(136),i=n(137),s=n(139),o=n(0),c=n.n(o),m=n(138),u=n.n(m),d=(n(59),function(e){function a(e){var n;return Object(t.a)(this,a),(n=Object(r.a)(this,Object(i.a)(a).call(this,e))).highlight=function(){n.ref&&n.ref.current&&u.a.highlightElement(n.ref.current)},n.ref=c.a.createRef(),n}return Object(s.a)(a,e),Object(l.a)(a,[{key:"componentDidMount",value:function(){this.highlight()}},{key:"componentDidUpdate",value:function(){this.highlight()}},{key:"render",value:function(){var e=this.props,a=e.code,n=(e.plugins,e.language);return c.a.createElement("pre",{className:"code-prism"},c.a.createElement("code",{ref:this.ref,className:"language-".concat(n)},a.trim()))}}]),a}(c.a.Component))},141:function(e,a,n){},150:function(e,a,n){"use strict";n.d(a,"a",(function(){return m}));var t=n(0),l=n.n(t),r=n(26),i=n(297),s=n(295),o=n(114),c=Object(o.a)((function(e){return{root:{display:"flex"},paper:{marginRight:e.spacing(2)},line:{textDecoration:"none"}}}));function m(){var e=c();return l.a.createElement("div",{className:e.root},l.a.createElement(s.a,null,l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/introAngular",className:e.line},"AI")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/tensorflow",className:e.line},"Tensorflow")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/tensors",className:e.line},"Tensorboards")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/angCompiler",className:e.line},"Compiler")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/neural",className:e.line},"NeuralKeras")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/activationFunctions",className:e.line},"activationFuncs")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/loss",className:e.line},"Loss")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/gradientNeural",className:e.line},"GradientNeural")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/stochastic",className:e.line},"Stochastic")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/benchmarking",className:e.line},"Benchmarking")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/customer",className:e.line},"Customer")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/regularizationDeep",className:e.line},"Regularization Deep")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/imbalanced",className:e.line},"Imbalanced")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/imbalanced2",className:e.line},"Imbalanced2")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/convolutionals",className:e.line},"Convolutionals")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/data_augmentation",className:e.line},"data Augmentation")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/transfer",className:e.line},"Transfer")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/word_embedding",className:e.line},"Embedding")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/datatypests",className:e.line},"Datatypes")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/typeScript_2",className:e.line},"TS Function")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/typeScript_4",className:e.line},"Type Assertion"))),l.a.createElement("div",null))}},211:function(e,a,n){e.exports=n.p+"static/media/wordembedding.baf9235a.png"},467:function(e,a,n){"use strict";n.r(a);var t=n(45),l=n(28),r=n(136),i=n(137),s=n(139),o=n(0),c=n.n(o),m=n(138),u=n.n(m),d=n(120),p=n(57),g=n(296),h=n(5),b=(n(141),n(150)),E=n(140),f=n(211),v=n.n(f),w={backgroundColor:"#F0F8FF",padding:"1px",fontSize:"16px"},y={height:200,width:500},N="\nimport numpy as np\nfrom tensorflow.keras.preprocessing.text import one_hot\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import Flatten\nfrom tensorflow.keras.layers import Embedding\n\nreviews = ['nice food',\n        'amazing restaurant',\n        'too good',\n        'just loved it!',\n        'will go again',\n        'horrible food',\n        'never go there',\n        'poor service',\n        'poor quality',\n        'needs improvement']\n\nsentiment = np.array([1,1,1,1,1,0,0,0,0,0])\none_hot(\"amazing restaurant\",30)\n\nvocab_size = 30\nencoded_reviews = [one_hot(d, vocab_size) for d in reviews]\nprint(encoded_reviews)\n\nmax_length = 4\npadded_reviews = pad_sequences(encoded_reviews, maxlen=max_length, padding='post')\nprint(padded_reviews)\n\nembeded_vector_size = 5\n\nmodel = Sequential()\nmodel.add(Embedding(vocab_size, embeded_vector_size, input_length=max_length,name=\"embedding\"))\nmodel.add(Flatten())\nmodel.add(Dense(1, activation='sigmoid'))\n\nX = padded_reviews\ny = sentiment\n\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\nprint(model.summary())\n\nmodel.fit(X, y, epochs=50, verbose=0)\nweights = model.get_layer('embedding').get_weights()[0]\n\nweights[13]\nweights[4]\nweights[16]\n".trim(),_=function(e){function a(){return Object(t.a)(this,a),Object(r.a)(this,Object(i.a)(a).apply(this,arguments))}return Object(s.a)(a,e),Object(l.a)(a,[{key:"componentDidMount",value:function(){setTimeout((function(){return u.a.highlightAll()}),0)}},{key:"render",value:function(){var e=this.props.classes;return c.a.createElement(d.a,{container:!0},c.a.createElement(d.a,{item:!0,xs:2},c.a.createElement(p.a,{className:e.paper},c.a.createElement("h4",null,c.a.createElement(b.a,null)))),c.a.createElement(d.a,{item:!0,xs:10},c.a.createElement(p.a,{className:e.paper},c.a.createElement(g.a,null,c.a.createElement("h3",null,"Word Embedding"),c.a.createElement("ul",null,c.a.createElement("li",null,"Computers break everything down to numbers. What happens when a software inside a computer (like a ML algorithm) has to operate/ process a word? Simple, this word needs to be given to the computer as the only thing it can understand: as numbers."),c.a.createElement("li",null,"In NLP, the most simple way to do this is by creating a vocabulary with a huge amount of words (100.000), and assigning a number to each word in the vocabulary."),c.a.createElement("li",null,"The first word in our vocabulary (\u2018apple\u2019 maybe) will be number 0. The second word (\u2018banana\u2019) will be number 1, and so on up to number 99.998, the previous to last word (\u2018king\u2019) and 999.999 being assigned to the last word (\u2018queen\u2019)."),c.a.createElement("li",null,"Then we represent every word as a vector of length 100.000, where every single item is a zero except one of them, corresponding to the index of the number that the word is associated with."),c.a.createElement("br",null),c.a.createElement("img",{src:v.a,alt:"Theata",className:"responsive2",style:y}),c.a.createElement("br",null),c.a.createElement("li",null,"This is called one-hot encoding for words."),c.a.createElement("li",null,"The main thing is that word embeddings are vectors that represent words, so that similar meaning words have similar vectors."),c.a.createElement("li",null,"The two most used Word embedding algorithms are Word2Vec and GloVe.")),c.a.createElement("br",null),c.a.createElement("div",{style:w},c.a.createElement(E.a,{code:N,language:"js",plugins:["line-numbers"]})),c.a.createElement("br",null),c.a.createElement("h3",null),c.a.createElement("div",{style:w},c.a.createElement(E.a,{code:N,language:"js",plugins:["line-numbers"]})),c.a.createElement("br",null),c.a.createElement("h3",null),c.a.createElement("div",{style:w},c.a.createElement(E.a,{code:N,language:"js",plugins:["line-numbers"]})),c.a.createElement("br",null),c.a.createElement("h3",null),c.a.createElement("div",{style:w},c.a.createElement(E.a,{code:N,language:"js",plugins:["line-numbers"]})),c.a.createElement("br",null),c.a.createElement("h3",null),c.a.createElement("div",{style:w},c.a.createElement(E.a,{code:N,language:"js",plugins:["line-numbers"]}))))))}}]),a}(o.Component);a.default=Object(h.a)((function(e){return{paper:{margin:e.spacing(1),padding:e.spacing(1)},smMargin:{margin:e.spacing(1)},actionDiv:{textAlign:"center"}}}))(_)}}]);
//# sourceMappingURL=50.27558af9.chunk.js.map