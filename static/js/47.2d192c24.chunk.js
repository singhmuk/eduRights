(this["webpackJsonpmern-stack-client"]=this["webpackJsonpmern-stack-client"]||[]).push([[47],{140:function(e,n,t){"use strict";t.d(n,"a",(function(){return d}));var a=t(45),l=t(28),r=t(136),i=t(137),s=t(139),c=t(0),o=t.n(c),m=t(138),u=t.n(m),d=(t(59),function(e){function n(e){var t;return Object(a.a)(this,n),(t=Object(r.a)(this,Object(i.a)(n).call(this,e))).highlight=function(){t.ref&&t.ref.current&&u.a.highlightElement(t.ref.current)},t.ref=o.a.createRef(),t}return Object(s.a)(n,e),Object(l.a)(n,[{key:"componentDidMount",value:function(){this.highlight()}},{key:"componentDidUpdate",value:function(){this.highlight()}},{key:"render",value:function(){var e=this.props,n=e.code,t=(e.plugins,e.language);return o.a.createElement("pre",{className:"code-prism"},o.a.createElement("code",{ref:this.ref,className:"language-".concat(t)},n.trim()))}}]),n}(o.a.Component))},141:function(e,n,t){},150:function(e,n,t){"use strict";t.d(n,"a",(function(){return m}));var a=t(0),l=t.n(a),r=t(26),i=t(297),s=t(295),c=t(114),o=Object(c.a)((function(e){return{root:{display:"flex"},paper:{marginRight:e.spacing(2)},line:{textDecoration:"none"}}}));function m(){var e=o();return l.a.createElement("div",{className:e.root},l.a.createElement(s.a,null,l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/introAngular",className:e.line},"AI")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/tensorflow",className:e.line},"Tensorflow")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/tensors",className:e.line},"Tensorboards")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/angCompiler",className:e.line},"Compiler")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/neural",className:e.line},"NeuralKeras")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/activationFunctions",className:e.line},"activationFuncs")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/loss",className:e.line},"Loss")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/gradientNeural",className:e.line},"GradientNeural")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/stochastic",className:e.line},"Stochastic")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/benchmarking",className:e.line},"Benchmarking")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/customer",className:e.line},"Customer")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/regularizationDeep",className:e.line},"Regularization Deep")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/imbalanced",className:e.line},"Imbalanced")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/imbalanced2",className:e.line},"Imbalanced2")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/convolutionals",className:e.line},"Convolutionals")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/data_augmentation",className:e.line},"data Augmentation")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/transfer",className:e.line},"Transfer")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/word_embedding",className:e.line},"Embedding")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/datatypests",className:e.line},"Datatypes")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/typeScript_2",className:e.line},"TS Function")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/typeScript_4",className:e.line},"Type Assertion"))),l.a.createElement("div",null))}},206:function(e,n,t){e.exports=t.p+"static/media/nn.73ebaea6.png"},456:function(e,n,t){"use strict";t.r(n);var a=t(45),l=t(28),r=t(136),i=t(137),s=t(139),c=t(0),o=t.n(c),m=t(138),u=t.n(m),d=t(120),p=t(57),g=t(296),b=t(5),E=(t(141),t(150)),_=t(140),f=t(206),h=t.n(f),y={backgroundColor:"#F0F8FF",padding:"1px",fontSize:"16px"},w={height:200,width:500},v="\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom sklearn.model_selection import train_test_split\nfrom matplotlib import pyplot as plt\n%matplotlib inline\n\ndf = pd.read_csv(\"insurance_data.csv\")\n\nX_train, X_test, y_train, y_test = train_test_split(df[['age','affordibility']],df.bought_insurance,test_size=0.2, \n    random_state=25)\n\n\n#Preprocessing: Scale the data so both age and affordibility are in same scaling range.\nX_train_scaled = X_train.copy()\nX_train_scaled['age'] = X_train_scaled['age'] / 100\n\nX_test_scaled = X_test.copy()\nX_test_scaled['age'] = X_test_scaled['age'] / 100\n".trim(),N="\nmodel = keras.Sequential([\n  keras.layers.Dense(1, input_shape=(2,), activation='sigmoid', kernel_initializer='ones', bias_initializer='zeros')\n])\n\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\nmodel.fit(X_train_scaled, y_train, epochs=5000)\n\nmodel.evaluate(X_test_scaled,y_test)                                            #Evaluate the model on test set.\nmodel.predict(X_test_scaled)\n\ny_test\n".trim(),k="\nimport math\n\ncoef, intercept = model.get_weights()\n\ndef sigmoid(x):\n    return 1 / (1 + math.exp(-x))\nsigmoid(18)\n\nX_test\n".trim(),j="\ndef prediction_function(age, affordibility):\n    weighted_sum = coef[0]*age + coef[1]*affordibility + intercept\n    return sigmoid(weighted_sum)\n\nprediction_function(.47, 1)\nprediction_function(.18, 1)\n".trim(),X="\ndef sigmoid_numpy(X):\n   return 1/(1+np.exp(-X))\n\nsigmoid_numpy(np.array([12,0,1]))\n\ndef log_loss(y_true, y_predicted):\n    epsilon = 1e-15\n    y_predicted_new = [max(i,epsilon) for i in y_predicted]\n    y_predicted_new = [min(i,1-epsilon) for i in y_predicted_new]\n    y_predicted_new = np.array(y_predicted_new)\n    return -np.mean(y_true*np.log(y_predicted_new)+(1-y_true)*np.log(1-y_predicted_new))\n".trim(),x="\ndef gradient_descent(age, affordability, y_true, epochs, loss_thresold):\n    w1 = w2 = 1\n    bias = 0\n    rate = 0.5\n    n = len(age)\n    for i in range(epochs):\n        weighted_sum = w1 * age + w2 * affordability + bias\n        y_predicted = sigmoid_numpy(weighted_sum)\n        loss = log_loss(y_true, y_predicted)\n\n        w1d = (1/n)*np.dot(np.transpose(age),(y_predicted-y_true)) \n        w2d = (1/n)*np.dot(np.transpose(affordability),(y_predicted-y_true)) \n\n        bias_d = np.mean(y_predicted-y_true)\n        w1 = w1 - rate * w1d\n        w2 = w2 - rate * w2d\n        bias = bias - rate * bias_d\n\n        print (f'Epoch:{i}, w1:{w1}, w2:{w2}, bias:{bias}, loss:{loss}')\n\n        if loss<=loss_thresold:\n            break\n\n    return w1, w2, bias\n    \ngradient_descent(X_train_scaled['age'],X_train_scaled['affordibility'],y_train,1000, 0.4631)\n\ncoef, intercept\n".trim(),O=function(e){function n(){return Object(a.a)(this,n),Object(r.a)(this,Object(i.a)(n).apply(this,arguments))}return Object(s.a)(n,e),Object(l.a)(n,[{key:"componentDidMount",value:function(){setTimeout((function(){return u.a.highlightAll()}),0)}},{key:"render",value:function(){var e=this.props.classes;return o.a.createElement(d.a,{container:!0},o.a.createElement(d.a,{item:!0,xs:2},o.a.createElement(p.a,{className:e.paper},o.a.createElement("h4",null,o.a.createElement(E.a,null)))),o.a.createElement(d.a,{item:!0,xs:10},o.a.createElement(p.a,{className:e.paper},o.a.createElement(g.a,null,o.a.createElement("h3",null,"Implement Gradient Descent For Neural Network (or Logistic Regression)"),"An optimization algorithm used to train machine learning models by minimizing errors between predicted and actual results.",o.a.createElement("br",null),o.a.createElement("br",null),o.a.createElement("b",null,"Predicting if a person would buy life insurnace based on his age using logistic regression"),o.a.createElement("br",null),o.a.createElement("br",null),o.a.createElement("div",{style:y},o.a.createElement(_.a,{code:v,language:"js",plugins:["line-numbers"]})),o.a.createElement("br",null),o.a.createElement("br",null),o.a.createElement("b",null,"Model Building: First build a model in keras/tensorflow and see what weights and bias values it comes up with. We will than try to reproduce same weights and bias in our plain python implementation of gradient descent. Below is the architecture of our simple neural network"),o.a.createElement("br",null),o.a.createElement("br",null),o.a.createElement("img",{src:h.a,alt:"Theata",className:"responsive2",style:w}),o.a.createElement("br",null),o.a.createElement("br",null),o.a.createElement("div",{style:y},o.a.createElement(_.a,{code:N,language:"js",plugins:["line-numbers"]})),o.a.createElement("br",null),o.a.createElement("h3",null,"Now get the value of weights and bias from the model"),o.a.createElement("div",{style:y},o.a.createElement(_.a,{code:k,language:"js",plugins:["line-numbers"]})),o.a.createElement("br",null),o.a.createElement("h3",null,"Instead of model.predict, write our own prediction function that uses w1,w2 and bias."),o.a.createElement("div",{style:y},o.a.createElement(_.a,{code:j,language:"js",plugins:["line-numbers"]})),o.a.createElement("br",null),o.a.createElement("br",null),o.a.createElement("b",null,"Now we start implementing gradient descent in plain python. Again the goal is to come up with same w1, w2 and bias that keras model calculated. We want to show how keras/tensorflow would have computed these values internally using gradient descent"),o.a.createElement("br",null),o.a.createElement("br",null),o.a.createElement("i",null,"First write couple of helper routines such as sigmoid and log_loss."),o.a.createElement("br",null),o.a.createElement("div",{style:y},o.a.createElement(_.a,{code:X,language:"js",plugins:["line-numbers"]})),o.a.createElement("br",null),o.a.createElement("br",null),o.a.createElement("b",null,"All right now comes the time to implement our final gradient descent function"),o.a.createElement("div",{style:y},o.a.createElement(_.a,{code:x,language:"js",plugins:["line-numbers"]})),o.a.createElement("br",null),o.a.createElement("i",null,"This shows that in the end we were able to come up with same value of w1,w2 and bias using a plain python implementation of gradient descent function.")))))}}]),n}(c.Component);n.default=Object(b.a)((function(e){return{paper:{margin:e.spacing(1),padding:e.spacing(1)},smMargin:{margin:e.spacing(1)},actionDiv:{textAlign:"center"}}}))(O)}}]);
//# sourceMappingURL=47.2d192c24.chunk.js.map