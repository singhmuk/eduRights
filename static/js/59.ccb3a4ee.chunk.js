(this["webpackJsonpmern-stack-client"]=this["webpackJsonpmern-stack-client"]||[]).push([[59],{140:function(e,a,t){"use strict";t.d(a,"a",(function(){return d}));var n=t(45),l=t(28),r=t(136),i=t(137),s=t(139),c=t(0),o=t.n(c),m=t(138),u=t.n(m),d=(t(59),function(e){function a(e){var t;return Object(n.a)(this,a),(t=Object(r.a)(this,Object(i.a)(a).call(this,e))).highlight=function(){t.ref&&t.ref.current&&u.a.highlightElement(t.ref.current)},t.ref=o.a.createRef(),t}return Object(s.a)(a,e),Object(l.a)(a,[{key:"componentDidMount",value:function(){this.highlight()}},{key:"componentDidUpdate",value:function(){this.highlight()}},{key:"render",value:function(){var e=this.props,a=e.code,t=(e.plugins,e.language);return o.a.createElement("pre",{className:"code-prism"},o.a.createElement("code",{ref:this.ref,className:"language-".concat(t)},a.trim()))}}]),a}(o.a.Component))},141:function(e,a,t){},146:function(e,a,t){"use strict";t.d(a,"a",(function(){return m}));var n=t(0),l=t.n(n),r=t(26),i=t(297),s=t(295),c=t(114),o=Object(c.a)((function(e){return{root:{display:"flex"},paper:{marginRight:e.spacing(2)},line:{textDecoration:"none"}}}));function m(){var e=o();return l.a.createElement("div",{className:e.root},l.a.createElement(s.a,null,l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/infoMl",className:e.line},"InfoMl")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/gredient_decents",className:e.line},"Gredient Decents")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/training",className:e.line},"Traning")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/regularizations",className:e.line},"Regularizations")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/featuresEng",className:e.line},"FeaturesEng")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/adaboost",className:e.line},"Adaboots")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/greedSearch",className:e.line},"Greed Search")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/perceptron",className:e.line},"Perceptron")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/pcaPy",className:e.line},"PCA")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/leanearRegression",className:e.line},"Leanear Regression")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/logisticReg",className:e.line},"Logistic Regression")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/lda",className:e.line},"Lda")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/knn",className:e.line},"Knn")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/k_meanClustring",className:e.line},"K_Mean")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/naiveBar",className:e.line},"Naive Bayes")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/randomForest",className:e.line},"Random Forest")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/decisiontree",className:e.line},"Decision Tree")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/svmPy",className:e.line},"SVM")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/numpyPy",className:e.line},"Numpy")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/pandas",className:e.line},"Pandas")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/bagging",className:e.line},"Matplotlib")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/logisticRegrations",className:e.line},"Scikit Learn")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/regrations",className:e.line},"SciPy")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/libraries",className:e.line},"OpenCV")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/capture",className:e.line},"Capture")),l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/joinImages",className:e.line},"JoinImages")),l.a.createElement("br",null),"Deep Learning",l.a.createElement(i.a,null,l.a.createElement(r.b,{to:"/superwise",className:e.line},"Superwise"))),l.a.createElement("div",null))}},232:function(e,a,t){e.exports=t.p+"static/media/dt.765746c2.png"},513:function(e,a,t){"use strict";t.r(a);var n=t(45),l=t(28),r=t(136),i=t(137),s=t(139),c=t(0),o=t.n(c),m=t(138),u=t.n(m),d=t(120),E=t(57),p=t(296),b=t(5),f=(t(141),t(146)),h=t(140),g=t(232),y=t.n(g),v={backgroundColor:"#F0F8FF",padding:"1px",fontSize:"16px"},w={height:350,width:600},N="\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn import tree\n\ndf = pd.read_csv(\"salaries.csv\")\ninputs = df.drop('salary_more_then_100k',axis='columns')\ntarget = df['salary_more_then_100k']\n\nle_company = LabelEncoder()\nle_job = LabelEncoder()\nle_degree = LabelEncoder()\n\ninputs['company_n'] = le_company.fit_transform(inputs['company'])\ninputs['job_n'] = le_job.fit_transform(inputs['job'])\ninputs['degree_n'] = le_degree.fit_transform(inputs['degree'])\ninputs\n\ninputs_n = inputs.drop(['company','job','degree'],axis='columns')\ninputs_n\ntarget\n\nmodel = tree.DecisionTreeClassifier()\nmodel.fit(inputs_n, target)\nmodel.score(inputs_n,target)\n\nmodel.predict([[2,1,0]])                                #Is salary of Google, Computer Engineer, Bachelors degree > 100 k\nmodel.predict([[2,1,1]])                                #Is salary of Google, Computer Engineer, Masters degree > 100 k\n".trim(),k='\ndef gini(rows):\n    """Calculate the Gini Impurity for a list of rows."""\n    counts = class_counts(rows)\n    impurity = 1\n    \n    for lbl in counts:\n        prob_of_lbl = counts[lbl] / float(len(rows))\n        impurity -= prob_of_lbl**2\n    return impurity'.trim(),_="\ntraining_data = [\n  ['Green', 3, 'Apple'],\n  ['Yellow', 3, 'Apple'],\n  ['Red', 1, 'Grape'],\n  ['Red', 1, 'Grape'],\n  ['Yellow', 3, 'Lemon'],\n  ]\n# Header = [\"Color\", \"diameter\", \"Label\"]\n\nmy_tree = build_tree(training_data)\nprint_tree(my_tree)\n".trim(),j=function(e){function a(){return Object(n.a)(this,a),Object(r.a)(this,Object(i.a)(a).apply(this,arguments))}return Object(s.a)(a,e),Object(l.a)(a,[{key:"componentDidMount",value:function(){setTimeout((function(){return u.a.highlightAll()}),0)}},{key:"render",value:function(){var e=this.props.classes;return o.a.createElement(d.a,{container:!0},o.a.createElement(d.a,{item:!0,xs:2},o.a.createElement(E.a,{className:e.paper},o.a.createElement("h4",null,o.a.createElement(f.a,null)))),o.a.createElement(d.a,{item:!0,xs:10},o.a.createElement(E.a,{className:e.paper},o.a.createElement(p.a,null,o.a.createElement("h3",null,"Decision Tree (supervised algorithms)"),"Decision tree analysis is a predictive modelling tool that can be applied across many areas. Decision trees can be constructed by an algorithmic approach that can split the dataset in different ways based on different conditions.",o.a.createElement("br",null),"It can be used for both classification and regression tasks.",o.a.createElement("br",null),o.a.createElement("br",null),"The two main entities of a tree are decision nodes, where the data is split and leaves, where we got outcome.",o.a.createElement("br",null),o.a.createElement("br",null),"We have the following two types of decision trees.",o.a.createElement("ul",null,o.a.createElement("li",null,o.a.createElement("b",null,"1. Classification decision trees: "),"In this kind of decision trees, the decision variable is categorical. The above decision tree is an example of classification decision tree."),o.a.createElement("li",null,o.a.createElement("b",null,"Regression decision trees: "),"In this kind of decision trees, the decision variable is continuous.")),o.a.createElement("br",null),"Implementing Decision Tree Algorithm: Gini Index It is the name of the cost function that is used to evaluate the binary splits in the dataset and works with the categorial target variable \u201cSuccess\u201d or \u201cFailure\u201d. Higher the value of Gini index, higher the homogeneity. A perfect Gini index value is 0 and worst is 0.5 (for 2 class problem). Split Creation A split is basically including an attribute in the dataset and a value. We can create a split in dataset with the help of following three parts \u2212 Part 1: Calculating Gini Score.",o.a.createElement("br",null),o.a.createElement("h3",null,"Decision Tree (supervised learning)"),"A decision tree is a flowchart-like structure in which each internal node represents a test on a feature, each leaf node represents a class label (decision taken after computing all features) and branches represent conjunctions of features that lead to those class labels. The paths from root to leaf represent classification rules.",o.a.createElement("br",null),o.a.createElement("br",null),"decision making with labels (Rain(Yes), No Rain(No)).",o.a.createElement("br",null),o.a.createElement("br",null),o.a.createElement("img",{src:y.a,alt:"Equations",className:"responsive",style:w}),o.a.createElement("br",null),o.a.createElement("br",null),"Tree models where the target variable can take a discrete set of values are called classification trees. Decision trees where the target variable can take continuous values (typically real numbers) are called regression trees.",o.a.createElement("br",null),o.a.createElement("br",null),o.a.createElement("b",null,"Gini Impurity:"),"Understand the meaning of Pure and Impure.",o.a.createElement("br",null),o.a.createElement("ul",null,o.a.createElement("li",null,o.a.createElement("b",null,"Pure: "),"Means, in a selected sample of dataset all data belongs to same class (PURE)."),o.a.createElement("li",null,o.a.createElement("b",null,"Impure: "),"Means, data is mixture of different classes.")),o.a.createElement("br",null),o.a.createElement("b",null,"Definition of Gini Impurity:"),o.a.createElement("br",null),"Gini Impurity is a measurement of the likelihood of an incorrect classification of a new instance of a random variable, if that new instance were randomly classified according to the distribution of class labels from the data set.",o.a.createElement("br",null),o.a.createElement("br",null),"If our dataset is Pure then likelihood of incorrect classification is 0. If our sample is mixture of different classes then likelihood of incorrect classification will be high.",o.a.createElement("br",null),o.a.createElement("br",null),o.a.createElement("b",null,"Calculating Gini Impurity: "),o.a.createElement("br",null),o.a.createElement("div",{style:v},o.a.createElement(h.a,{code:k,language:"js",plugins:["line-numbers"]})),o.a.createElement("br",null),o.a.createElement("br",null),o.a.createElement("b",null,"Steps for Making decision tree:"),o.a.createElement("ul",null,o.a.createElement("li",null,"Get list of rows (dataset) which are taken into consideration for making decision tree (recursively at each nodes)."),o.a.createElement("li",null,"Calculate uncertanity of our dataset or Gini impurity or how much our data is mixed up etc."),o.a.createElement("li",null,"Generate list of all question which needs to be asked at that node."),o.a.createElement("li",null,"Partition rows into True rows and False rows based on each question asked."),o.a.createElement("li",null,"Calculate information gain based on gini impurity and partition of data from previous step."),o.a.createElement("li",null,"Update highest information gain based on each question asked."),o.a.createElement("li",null,"Update best question based on information gain (higher information gain)."),o.a.createElement("li",null,"Divide the node on best question. Repeat again from step 1 again until we get pure node (leaf nodes).")),o.a.createElement("br",null),o.a.createElement("b",null,"Let\u2019s build decision tree based on training data."),o.a.createElement("ul",null,o.a.createElement("li",null,"The last column is the label."),o.a.createElement("li",null,"The first two columns are features.")),o.a.createElement("div",{style:v},o.a.createElement(h.a,{code:_,language:"js",plugins:["line-numbers"]})),o.a.createElement("br",null),o.a.createElement("b",null,"Advantage :"),o.a.createElement("ul",null,o.a.createElement("li",null,"Easy to use and understand."),o.a.createElement("li",null,"Can handle both categorical and numerical data."),o.a.createElement("li",null,"Resistant to outliers, hence require little data preprocessing.")),o.a.createElement("br",null),o.a.createElement("b",null,"Disadvantage :"),o.a.createElement("ul",null,o.a.createElement("li",null,"Prone to overfitting."),o.a.createElement("li",null,"Require some kind of measurement as to how well they are doing."),o.a.createElement("li",null,"Need to be careful with parameter tuning."),o.a.createElement("li",null,"Can create biased learned trees if some classes dominate.")),o.a.createElement("br",null),o.a.createElement("b",null,"Why Dropout help with overfitting."),o.a.createElement("ul",null,o.a.createElement("li",null,"It can't rely on one i/p as it might be dropped out at random."),o.a.createElement("li",null,"Neurons will not learn redundant details i/p.")),o.a.createElement("br",null),o.a.createElement("h3",null,"Decision Tree Classification"),o.a.createElement("div",{style:v},o.a.createElement(h.a,{code:N,language:"js",plugins:["line-numbers"]}))))))}}]),a}(c.Component);a.default=Object(b.a)((function(e){return{paper:{margin:e.spacing(1),padding:e.spacing(1)},smMargin:{margin:e.spacing(1)},actionDiv:{textAlign:"center"}}}))(j)}}]);
//# sourceMappingURL=59.ccb3a4ee.chunk.js.map